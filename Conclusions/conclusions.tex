\chapter{Conclusions}\label{chap:Conclusions}
This project aimed to improve the type error debugging experience in Hazel. Two novel features, \textit{type slicing} and \textit{cast slicing}, were mathematically formalised and implemented successfully. Additionally, upon evaluation, variations on these ideas were devised and implemented. 

These type slicing implementations provided more complete \textit{explanations} for why expressions were given their static type, and therefore, why type errors were detected. The feature was much more \textit{expressive} than originally aimed for (applying to \textit{all} expression rather than just errors), and it's variants applied to \textit{more situations}\footnote{Including: calculating static code regions, extracting inconsistent sub-parts of slices.} than set out in my core goals.

Cast slicing successfully provided a link between \textit{dynamic errors} (cast errors), and the static context (source code) which sourced the parts of the cast. These slices were found to be very small, giving very specific and concise information to the programmer.

Additionally, a \textit{type error witness search procedure}  improve the explanation of both static and dynamic type errors in the Hazel language. Together, these features provide richer, more intuitive diagnostics by offering both abstract and concrete perspectives on type errors. Evaluation results show that the features are largely effective and performant, complementing each other and the existing error highlighting, to form a cohesive debugging experience. 


\section{Further Directions}
This section presents some \textit{extensions} to improve the features as justified by the evaluation (\cref{sec:CriticalAnalysis} \& \ref{sec:HolisticEvaluation}). Also, further interesting \textit{applications} of the features are considered.

\subsection{Extension to Full Hazel Language}
Some of the Hazel language was not covered, or was partially covered. Type functions (System-F style polymorphism) was only partially supported. Type functions have especially difficult typing rules, which may lead to more subtle errors, where this project would be \textit{particularly useful}. 

\subsection{UI Improvements, User Studies}
The current UI is relatively simple. \Cref{sec:UIImprovements} suggested various improvements to the usability of all three features. 

To assess how well these features actually work, and compare the usefulness of the different slicing methods, user studies would be beneficial.

Seidel et al. \cite{SearchProc} implemented a similar \textit{type error witness search procedure}, evidencing it's utility by a user study. They additionally made use of various techniques to improve usability including: jump-compressed execution traces\footnote{Hiding the execution of functions, except for when the error occurs.}, graph visualisation, and interactive steppers.

\subsection{Cast Slicing}
Cast slicing purely propagated type slice information throughout evaluation, and tracked the a dependency relation on casts. While this is useful for debugging type errors, as demonstrated, it provides no slicing based on \textit{execution properties} of the program. Extensions could include slicing methods which, for example, provide a minimal program which evaluates to the \textit{same} cast around the \textit{same} value. This would be similar to traditional \textit{dynamic slicing} methods \cite{DynProgSlice, FunctionalProgExplain} which take parts of the program relevant to producing a \textit{given} result.


\subsection{Proofs}

Giving proofs of the type slicing theory was an unmet \textit{extension} goal. However, \textit{much thought}\footnote{The largest time-investment of the entire project.} was given to ensuring that the theory is still sensible, and correct, as is demonstrated by the reasoning provided in the implementation (\cref{sec:TypeSlicingTheory}).

Additionally, proofs and formal semantics for cast slicing and the search procedure would be useful to formalise what exactly cast slices, and witness actually \textit{are}.

\subsection{Property Testing}
Indeterminate evaluation allows for exhaustive generation of inputs to functions, by applying a hole to the function. The searching logic is sufficiently abstract to allow arbitrary property testing of resulting values, indeterminate expressions, and even intermediate expressions.

The current algorithm generates smaller inputs first,\footnote{By default. This is also customisable, as in SmallCheck.} similarly to SmallCheck \cite{SmallCheck}; the approach being justified by the \textit{small scope hypothesis} (\cref{sec:SmallScopeHypothesis}). 

Other property testing models could be implemented, for example QuickCheck \cite{QuickCheck} performs random generation, along with input reduction to find simpler inputs which cause the same error.

Testing of intermediate expressions is \textit{not possible}\footnote{Without refactoring the tested code.} in either SmallCheck or QuickCheck. Indeterminate evaluation would easily allow testing a property like: \textit{Does every execution trace have a length of 5?}.

\subsection{Non-determinism, Connections to Logic Programming, and Program Synthesis}
\label{sec:LogicProgramming}
\textbf{Discuss this...}

Allowing holes to evaluate non-deterministically could be harnessed directly to write non-deterministic algorithms themselves in Hazel. The results to search for could be defined in Hazel code, and injected into the indeterminate evaluation algorithm.

This way of treating holes is reminiscent of \textit{free logic variables} in logic programming, of which functional logic programming languages \cite{FunctionalLogicProgramming} like \textit{Curry} \cite{CurryLang}. Adding unification \cite{UnificationSurvey} to turn these into full-blown logic variables would allow full logic programming in Hazel. A needed-narrowing evaluation strategy \cite{NeededNarrowing} would be an interesting, and more efficient, way to handle and extend indeterminate evaluation in this situation.

Logic programming has historically been used for \textit{general-purpose program synthesis} \cite{LogicProgramSynthesis}. An incomplete Hazel program could then be used as a specification for it's incomplete parts, where executing the program would synthesise the incomplete parts. However, this approach has been found to be inefficient due to the need to solve \textit{every} possible program, and prone to underspecified problems, among other drawbacks \cite{LogicProgramSynthesisDrawbacks}. 

More modern approaches to program synthesis often use logic programming as specification, but use more specialised and scalable methods making use of machine learning to synthesise the programs \cite{NeuralGuidedLogicProgramSynthesis, ProgramSynthesisSketching}. Therefore, an incomplete Hazel (logic) program sketch could be used as a specification to synthesise the incomplete portions automatically, using more modern methods.

\subsection{Symbolic Execution}
The search procedure was not always able to find witnesses, this was suspected to be due to low code coverage (\cref{sec:ImprovingCodeCoverage}). Improving code coverage for testing has been extensively researched, often via symbolic execution \cite{SymbolicExecutionSurvey}. Often, constraints along execution paths are modelled via \textit{satisfiability modulo theories} (SMTs) \cite{SMTs}, for which there exist many efficient solvers \cite{SMTSolver}.

The evaluation of holes can already be consider a form of \textit{symbolic execution}. However, for indeterminate evaluation, the only constraints considered are the type of the hole. \Cref{sec:ExtendedPatternMatching} considered constraining instantiation based on structures of pattern within match statements. Full symbolic execution would also consider more complex constraints on the \textit{values} of base types.

\subsubsection{Polymorphism}
The set of possible types is infinite. Generating witnesses for polymorphic values is therefore a much expanded state-space. Symbolic theories and solvers for polymorphic operations, would help in this situation \cite{PolymorphicSMTs}.

\subsection{Let Polymorphism \& Global Inference}
Errors in the presence of global inference are often more subtle (\cite{SubtleOCamlErrors}), as there are fewer annotations which concretely assert the types for expressions. In this situation, type slicing, cast slicing, and the witness search procedure would be particularly useful.

Global inference is difficult to combine with complex type systems, for example polymorphism where global inference for Hazel's System-F style polymorphism is undecidable \cite{SystemFUndecidable}). However, ML-family languages often implement restricted \textit{let-polymorphism} via principal type schemes \cite{PrincipalTypeSchemes}.

The intersection of gradually typing and principal type schemes has been explored by Garcia and Cimini \cite{GradualTI} Miyazaki et al. \cite{DTI}, the latter of which would integrate most smoothly with Hazel and indeterminate evaluation. 

Hazel currently has a branch exploring global inference \textbf{(branch name here)}, although without polymorphism as of yet.

\subsubsection{Constraint Slicing}
The search procedure and cast slicing would be relatively easily extended to a let-polymorphic Hazel in the style of Miyazaki et al. However, type slicing would now have to consider non-local constraints involved and the code which sourced them, differing significantly from the proposed theory for bidirectional typing. Somewhat similar ideas have been explored in \textit{constraint-based type error slicing} by Haack and Wells \cite{HaackErrSlice}.

\section{Reflections}
TODO