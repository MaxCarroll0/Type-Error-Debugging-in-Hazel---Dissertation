\chapter{Evaluation}\label{chap:Evaluation}
This section evaluates how successfully and effectively the implemented features achieve the goals stated in the introduction.

\section{Success}
This evaluation is more ambitious than that presented in the project proposal goals. As demonstrated below, the type witness search procedure and cast slicing features exceeds all core goals\footnote{That is, reasonable coverage and performance.} presented in the project proposal, while type slicing had not been conceived, but naturally complements cast slicing. Some extension goals were reached: almost of Hazel was supported\footnote{Except the type slicing of two constructs: type functions and labelled tuples (which was a new feature merged by the Hazel dev team in February).}. The search procedure was not mathematically formalised, but the slicing mechanisms were.

\section{Goals}\label{sec:EvaluationGoals}
This project devised and implemented three features: \textit{type slicing, cast slicing,} and a \textit{static type error witness search procedure}. Each of which had a clear intention for it's use:

\paragraph{Type Slicing:} Expected to give a greater static context to expressions. Explaining why an expression was given a specific type.

\paragraph{Cast Slicing:} Expected to provide static context to runtime type casts and propagate this throughout evaluation. Explaining where a cast originated and why it was inserted.

\paragraph{Search Procedure:} Finds dynamic type errors (cast errors) automatically, linked back to source code by their execution trace and \textit{cast slice}. Therefore, a static type error can be associated automatically with a concrete dynamic type error \textit{witness} to better explain.

\section{Methodology}\label{sec:EvaluationMethodology}
I evaluate the features and their various implementations (where applicable) along \textit{four} axes. With quantitative measures were evaluated over a corpus of ill-typed and dynamically-typed Hazel programs (\cref{sec:EvaluationCorpus}):

\subsubsection{Quantitative Analysis}
\paragraph{Performance: } \textit{Are the features performant enough for use in interactive debugging? Which implementations perform best?}

The time and space usage of the search procedure implementations were micro-benchmarked for each ill-typed program in the corpus. Up to 100 runs were taken per program with estimated time, major and minor heap allocations were estimated using an ordinary linear regression (OLS) to $r^2 > 0.95$\footnote{TODO: get values} via the \textit{bechamel} library \cite{Bechamel}.

\paragraph{Effectiveness: } \textit{Do the features effectively solve the problems? Are the results easily interpretable by a user?}

The \textit{coverage},  what proportion of programs admit a witness, for each search procedure implementation was measured. The search procedure does not always terminate, a 30s time limit was chosen. The coverage was expected to be \textit{reasonable}, chosen at 75\%.

Additionally, the \textit{size} of witnesses, evaluation traces, type slices, and cast slices were measured. The intention being that a smaller size implies that there is less information for a user to parse, and hence easier to interpret.\footnote{Not necessarily \textit{always} true, but a reasonable assumption.}

\subsubsection{Qualitative Analysis}
\paragraph{Critical: } \textit{What \textit{classes} of programs are missed by the search procedure? What are the implications of the \textit{quantitative} results? What improvements were, or could be made in response to this?}

This section provides \textit{critical} arguments on {usefulness} or {effectiveness}, which are \textit{evidenced} by quantitative data. Differing implementations and \textit{subsequent improvements}\footnote{Which were then implemented and analysed.} are compared. Additionally, further unimplemented improvements are proposed.

\paragraph{Holistic: } \textit{Do the features work well together to provide a helpful debugging experience? Is the user interface intuitive?}

Various program examples are given, demonstrating how all three features can be used together to debug a type error. Improvements to the UI are discussed.\footnote{Improved UI being a low-priority extension.} 



\section{Hypotheses}
Various hypotheses for properties of the results are expected. The evidence and implications of these are discussed in the \textit{critical evaluation}.

\paragraph{Search method space requirements: } The space requirements for DFS and Bounded DFS are expected to be lower than that of BFS and interleaved DFS.

\paragraph{Type Slices are larger than Cast Slices: } Casts are de-constructed during elaboration and evaluation, so cast slices are expected to be smaller than the original type slices, and therefore more directly explain why errors occur. 

\paragraph{The Small Scope Hypothesis: }
\label{sec:SmallScopeHypothesis} This hypothesis \cite{SmallScopeHypothesisOrigination} states that a high proportion of errors can be found by generating only \textit{small} inputs. Evidence that this hypothesis holds has been provided for Java data-structures \cite{SmallScopeHypothesis} and answer-set programs \cite{SmallScopeHypothesisAnswerSet}. Does it also hold for finding dynamic type errors from small \textit{hole instantiations}?

\paragraph{Smaller instantiations correlate with smaller traces: } As functional programs are often written recursively, destructuring compound data types on each step. If this and the small scope hypothesis hold, then most errors could be found with \textit{small execution traces}.

\section{Program Corpus Collection}\label{sec:CorpusCollection}

A corpus of small and mostly ill-typed programs was produced, containing both dynamic (unannotated) programs and annotated programs (containing statically caught errors). We have made this corpus available on GitHub \cite{HazelCorpus}.

\subsection{Methodology}
There are no extensive existing corpora of Hazel programs, nor ill-typed Hazel programs. Therefore, we opted to transpile parts of an existing OCaml corpus collected by Seidel and Jhala \cite{OCamlCorpus}. Which is freely available under a Creative Commons CC0 licence. 

I am grateful for my supervisor who created a best-effort OCaml to Hazel transpiler \cite{HazelOfOCaml}. This translates the OCaml examples into both a dynamic example, and a (possibly partially) statically typed version according to what type the OCaml type checker expects expression to be.\footnote{The annotations may be consistent, as we are translating ill-typed code.}

This corpus contains both OCaml unification and constructor errors. When translated to Hazel, these may manifest as differing errors. The only errors that the search procedure is expected to detect are those which contain \textit{inconsistent expectations} errors. Hence, the search procedure is ran on the corpus of annotated programs filtering those without this class of errors. Additionally, the search procedure requires the erroneous functions to have holes applied to start the search, these are inserted automatically by the evaluation code after type checking the programs.


\subsection{Statistics}
The program corpus contains \textbf{698} programs of which \textbf{203} were applicable to performing the search procedure on. Averages and standard deviations in size and trace size\footnote{When using normal, deterministic, evaluation.} shown in \cref{fig:CorpusStats}.
\begin{figure}
\centering
\begin{tabular}{c|ccccc}
& \textbf{Count} & \multicolumn{2}{c}{\textbf{Prog. Size}}& \multicolumn{2}{c}{\textbf{Trace Length}}\\
&  & Avg. & Std. dev. & Avg. & Std. dev.\\
\hline
\textbf{Unannotated} &404 &117 &81 &9&9\\
\textbf{Annotated} &294 &117 &76 &9&9\\
\textbf{Searched} &203 &120 &77 &10 &10\\
\hline
(Total)  &698 &117 &79 &9 &9\\
\end{tabular}
\caption{Hazel Program Corpus}
\label{fig:CorpusStats}
\end{figure}

\section{Performance Analysis}\label{sec:PerformanceAnalysis}

\subsection{Slicing}
The type and cast slicing mechanisms don't increase the time complexity of the type checker nor evaluator. Hence, they are still as performant as the original, and can still be used interactively in the web browser up to medium sized programs.

\subsection{Search Procedure}
Only the annotated ill-typed corpus containing inconsistency errors are used in evaluating the search procedure. After all, any well-typed program cannot have a dynamic type error.

As the search procedure may be non-terminating, the results are found under a 30s time limit. Each search implementation gives terminates on a different set of programs with potentially different witnesses and traces. The succeeding \textit{effectiveness} analysis considers the search procedures results under these constraints.

However, by micro-benchmarking only the programs which do not time-out, the time and space used searching for each witness can be estimated. The averages over all successful programs for each implementation are given in \cref{fig:SearchPerformance}. These results suggest that, as expected, BFS and interleaved DFS (IDFS) use more memory in total\footnote{Although, IDFS efficiently keeps most memory allocated short lived, being in the \textit{minor} heap.} than bounded DFS (BDFS) and DFS, while DFS is the fastest\footnote{Having the least overhead.}. 

This is not a fair test as the sets of programs averaged over are different, for example, BFS only succeeds on the smaller programs. Hence, the ratios between each implementation as compared to DFS on only the programs which \textit{both} succeed on are given in \cref{fig:SearchPerformanceRatios}. Under this, the results are even more pronounced.
\begin{figure}
  \centering
  \begin{tabular}{lc|cccc}
  & Averages & \multicolumn{4}{c}{\textbf{Implementations}}\\
   & unit & \texttt{DFS} & \texttt{BDFS} & \texttt{IDFS} & \texttt{BFS}\\
   \hline
   \textbf{Time} & ms &  7.6 & 73 & 140 & 120\\
   \textbf{Major Heap} & mB & 3.7 & 32 & 5.9 & 25\\
   \textbf{Minor Heap} & mB & 66 & 680 & 1900 & 1300
  \end{tabular}
  
\caption{Benchmarks: Search Implementations}
\label{fig:SearchPerformance}
\end{figure}
\begin{figure}
  \centering
  \begin{tabular}{l|ccc}
  Ratios & \multicolumn{3}{c}{\textbf{Implementations}}\\
    vs. \texttt{DFS}& \texttt{BDFS} & \texttt{IDFS} & \texttt{BFS} \\
   \hline
   \textbf{Time} &  8.3 & 52 & 230\\
   \textbf{Major Heap} & 9.0 & 3.2 & 270\\
   \textbf{Minor Heap} & 9.7 & 83 & 390
  \end{tabular}
  
\caption{Benchmarks: Performance ratios to DFS over common programs}
\label{fig:SearchPerformanceRatios}
\end{figure}


\section{Effectiveness Analysis}\label{sec:EffectivenessAnalysis}


\subsection{Slicing}
\textit{Type slice} sizes were calculated by the type checker over the entire corpus, where the size is the number of constructs highlighted. While \textit{cast slice} sizes were calculated over in the elaborated expressions after type checking.

\Cref{fig:TypeSlicingEffectiveness} shows that both type and cast slices are generally small. In particular, the proportion of the context\footnote{Calculated by close approximation by the \textit{program size}. As each program in the corpus is just one definition. Calculating the context itself is non-trivial.} highlighted is very low, generally less than 5\% for dynamic code and 10\% for annotated code. Therefore, they concisely explain the types. However, some slices are still large, as seen by the relatively large standard deviations.

Additionally, for errors, there will be multiple inconsistent slices involved. \Cref{sec:SlicingAnalysis} describes how these slices can be summarised to only report the inconsistent parts. We find that these \textit{minimised} error slices are significantly (3x) smaller than directly \textit{combining} the slices.

\begin{figure}[h]
  \centering
  \begin{tabular}{lc|c|ccc|ccc}
  \multicolumn{2}{c}{\textbf{Averages}} & \multicolumn{7}{c}{\textbf{Subdivisions}}\\
  & & & \multicolumn{3}{c|}{\textbf{Combined Error Slices}} & \multicolumn{3}{c}{\textbf{Minimised Error Slices}}\\ 
   & unit & ok & expects & branches & all & expects & branches & all\\
   \hline
   \textbf{Type Slice} & size &  8.2 & 13 & 22 & 15&  5.7 & 3.2 & 5 \\
   Std. dev. &  				 &  11 & 10 & 24 & 15&    4.31 & 4.1 & 4.4 \\
   \textbf{Proportion}& \%    & 5 & 8 & 14 & 9&        3  & 2 & 3 \\
   Std. dev. &  				 &  7 & 8 & 14 & 10&      3 & 3 & 3 \\
   \multicolumn{9}{c}{\textit{(Unannotated)}}\\
   \textbf{Type Slice} & size &  7.5 & 21 & 133* & 22.6 &8.2&  2.0* & 8.2  \\
   Std. dev. 			&    &  13 & 22 & 42* & 25.2& 12.6&  0.0* & 12.6  \\
   \textbf{Proportion}& \% 	 & 4 & 14 & 0.48* & 15&  6&    1* & 5  \\
   Std. dev. &  				 &  9 & 18 & 0.07* & 18&   9&    0* & 12  \\
   \multicolumn{9}{c}{\textit{(Annotated)}}\\
   \multicolumn{9}{c}{* only 2 annotated programs had inconsistent branches}
  \end{tabular}
  \caption{Effectiveness: Type Slices}
\label{fig:TypeSlicingEffectiveness}
\end{figure}
Casts have a pair of slices, \textit{`from'} a type and \textit{`to'} a type, both of which are smaller on average (\cref{fig:CastSlicingEffectiveness}) than type slices. This is because casts are split between ground types, i.e. to a `dynamic function' type, resulting parts of the type information being spread out over a number of casts; therefore, casts are can more precisely point to which part of an expressions type caused them.

Unnanotated code has larger \textit{`from'} slices as it relies more on synthesis for typing code, and vice versa for annotated code.



\begin{figure}[h]
  \centering
  \begin{tabular}{lc|cc|cc}
  \multicolumn{2}{c}{\textbf{Averages}} & \multicolumn{4}{c}{\textbf{Subdivisions}}\\
  & & \multicolumn{2}{c|}{\textbf{Ok}} & \multicolumn{2}{c}{\textbf{Errors}}\\ 
   & unit & to & from & to & from\\
   \hline
   \textbf{Cast Slice} & size &  5.5 & 1.2 & 5.9 & 1.5 \\
   Std. dev. &  				 &  8.1 & 3.7 & 7.1 & 2.0\\
   \textbf{Proportion}& \%    & 1 & 0.2 & 1 & 0.2\\
   Std. dev. &  				 &  1 & 0.5 & 1 & 0.4\\
   \multicolumn{6}{c}{\textit{(Unannotated)}}\\
   \textbf{Type Slice} & size &  4.8 & 6.3 & 6.9 & 4.4  \\
   Std. dev. 			&    &  11 & 13 & 9.1 & 9.3\\
   \textbf{Proportion}& \% 	 & 1 & 2 & 2 & 1\\
   Std. dev. &  				 &  1 & 1 & 1 & 1\\
   \multicolumn{6}{c}{\textit{(Annotated)}}
  \end{tabular}
  \caption{Effectiveness: Cast Slices}
\label{fig:CastSlicingEffectiveness}
\end{figure}


\subsection{Search Procedure}

\subsubsection{Witness Coverage}
\begin{figure}[h]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/DFS_coverage}
\caption{DFS}
\end{subfigure}
%
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/BDFS_coverage}
\caption{Bounded DFS}
\end{subfigure}

\vspace{1cm}

\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=1\textwidth]{Media/Figures/IDFS_coverage}
\caption{Interleaved DFS}
\end{subfigure}
%
\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=\textwidth]{Media/Figures/BFS_coverage}
\caption{BFS}
\end{subfigure}

\caption{Search Procedure Coverage}
\label{fig:PieChart}
\end{figure}

The search procedure terminates either with a witness or proving no witness exists. A majority of programs terminated when using BDFS, DFS and IDFS, with BDFS meeting the 75\% target directly (\cref{fig:PieChart}). IDFS and BFS perform relatively poorly likely due to excessive memory usage (\cref{fig:SearchPerformance}). 

However, not all programs actually have an execution path leading to a dynamic type error. For example, errors within dead code cannot be found. I manually classified each failed program to check if a witness does exist, but was not found, or no witness exists. Of which, 89\% was found to be dead code and 5\% due to Hazel bugs\footnote{Being also present on the main branch, not just in my code.}, the bugs being excluded from the results. This gives only 2\% cases where BDFS failed to find an \textit{existing} witness; DFS and IDFS also meet the goal in failing in less than 25\% of cases.

\Cref{sec:SearchCategories} goes into further detail on categorising the programs which time out, and how this could be avoided.

\subsubsection{Witness \& Trace Size}
As predicted by the small-scope hypothesis, most programs admitted \textit{small} witnesses. And the depth first searches produce longer and more varied trace lengths, which allowed them, in combination with their lower memory overhead, to attain higher coverage (many witnesses were at a deeper depth than IDFS or BFS were able to reach).

However, there was no correlation (Pearson correlation \cite{PearsonCorrelation}) between witness sizes and trace sizes, even when normalised by the original deterministic evaluation trace lengths. This is likely because most errors are in the base cases, so few large witnesses are even found, with the noise from trace lengths to different programs' base cases dominating. 

\begin{figure}[h]\centering
\begin{tabular}{l|cccc}
& DFS & BDFS & BFS & IDFS\\
\hline
\textbf{Witness Size} Avg. & 1.1& 1.9&1.4&  2\\
Std. dev. & 1.2& 2.3&1.4&  2.3\\
\textbf{Trace size} Avg. & 33& 32& 11& 17\\
Std. dev. & 35& 33&2.4& 5
\end{tabular}
\caption{Witness \& Trace Sizes}
\label{fig:WitnessSize}
\end{figure}


\section{Critical Analysis}\label{sec:CriticalAnalysis}
This section discusses the implications of the previous results and delves deeper into the reasoning behind them. As a response to this analysis, many improvements have been devised, some of which have been implemented.
\subsection{Slicing}\label{sec:SlicingAnalysis}
\textit{Contribution slicing} produces large slices, highlight all branches, with much of the information is not particularly \textit{useful}, with much of it explaining why the use of \textit{subsumption} in subterms was valid, which is never directly related to type errors. As such,  I opted to use and evaluate synthesis and analysis slices for the task of explaining static errors more concisely. If a user were unsure as to why a sub-term is consistent with it's expected type, they can still select it to retrieve the slices.

The type slicing theory (\cref{sec:TypeSlicingTheory}) required that highlighted parts must form valid expressions or contexts.\footnote{Allows useful mathematical properties to be formalised.} But, some of the constructs highlighted are purely structural, not influencing the type, which could be omitted in practice. For example, bindings when the bound variable is dynamic or unused when typing the particular expression considered. This was a primary motivation in implementing slices via \textit{unstructured slices} (\cref{sec:UnstructuredCodeSlice}). I call these `ad-hoc' slices.

\Cref{fig:LetSliceOmitted} binds a integer \code{x}, but this variable is only used in one branch, where the type information for the \code{if} is already known from the other branch, so not included in the slice (highlighted in green). So, even though we selected\footnote{See the red cursor \& let expression filled in a darker colour.} the whole program, the let expression is omitted from the slice. Further, a contribution slice would be even more verbose here, highlighting \textit{everything}.\footnote{Both branches must be highlighted, and the conditional is only well-typed if \code{x} checks against an integer.}

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=0.85\textwidth]{Media/Figures/Unused_let}
\caption{Ad-hoc Slice: Let expression omitted}
\end{subfigure}$\qquad$
\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=0.85\textwidth]{Media/Figures/Unused_let_ordinary}
\caption{Ordinary Slice}
\end{subfigure}

\caption{An Ad-hoc Slice vs. Ordinary Slice}
\label{fig:LetSliceOmitted}
\end{figure}


\subsubsection{Error Slices}
When looking specifically at a \textit{type error}, there will be an inconsistency between the term's analysis and synthesis slices. Or, for synthesising branch statements, may also occur from multiple inconsistent branches (whom are \textit{all} synthesising). Meaning understanding the error requires looking at each of these.

However, some portions of the type may be consistent, not causing to the error. A form of type joining which extracts only the \textit{inconsistent parts} was therefore implemented. This became the default UI when clicking on a type error, see how branches synthesising or expecting inconsistent types \code{(Int, Int)} vs \code{(Int, String)} only highlights the right element in \cref{fig:ErrorSlice}.

\begin{figure}[h]
\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=1\textwidth]{Media/Figures/partially_inconsistent_branches}
\caption{Partially Inconsistent Branches}
\end{subfigure}$\qquad$
\begin{subfigure}{0.45\textwidth}
\centering

\includegraphics[width=1\textwidth]{Media/Figures/partially_inconsistent_expectations}
\caption{Partially Inconsistent Expectations}
\end{subfigure}

\caption{Error Slices}
\label{fig:ErrorSlice}
\end{figure}

Additionally, when the inconsistent parts are compound types, they must differ at their \textit{outermost} constructor (i.e. a \code{List} vs an \code{Int}). This can be considered the \textit{primary} cause of the error, inconsistencies deeper within the type were not the ones causing the error. These minimised slices (\cref{fig:MinimisedSlice}) were significantly smaller than naively combining the full slices (by 3x, see \cref{fig:SlicingEffectiveness}); there would be an even larger ratio for more complex programs.\footnote{The corpus has few \textit{partially} inconsistent errors.}


\begin{figure}[h]
\centering
\begin{subfigure}{0.3\textwidth}
\centering

\includegraphics[width=1\textwidth]{Media/Figures/partially_inconsistent_compound}
\caption{Minimised Error Slice: Inner \code{Int} slice within list is omitted}
\end{subfigure}$\qquad\qquad$
\begin{subfigure}{0.3\textwidth}
\centering

\includegraphics[width=1\textwidth]{Media/Figures/partially_inconsistent_compound_ordinary}
\caption{Ordinary Error Slice}
\end{subfigure}

\caption{Minimised Error Slices}
\label{fig:MinimisedSlice}
\end{figure}

\subsection{Structure Editing}\label{sec:StructureEditing}
Hazel uses a structure editor with an update calculus \cite{HazelStructureCalculus}. Statics are recalculated upon edits and even \textit{cursor movements}. While type checking with slices is still very fast, it is not so suitable for such a rapid use case in larger programs.

Parsing and structural edit actions are made efficient\footnote{Constant time.} by use of a zipper data structure \cite{HuetZipper, OneHoleContext}. It might be possible to extend this zipper idea into the abstract syntax tree and it's statics (typing information), allowing local changes (and type changes) to propagate in the AST zipper. However, some local changes can propagate type changes \textit{non-locally} (e.g. inserting a new binding) which would require extensive recalculation of the typed AST. These non-local updates would be relatively rare. This is very complex and would require an entire rewrite of the Hazel statics, but would be beneficial for Hazel in general, not just type slicing.
  
\subsection{Static-Dynamic Error Correspondence}
\label{sec:ErrorCorrespondence}
\Cref{sec:StaticCastError} noted that a static error will elaborates a cast failure, which can be associated with a dynamic error whose cast error is dependent on (decomposed from) this original failed cast. This works well for \textit{inconsistent expectations} static type errors.

However, \textit{inconsistent branches} errors, static errors caused by synthesised branches being inconsistent, do not directly cause cast errors. Such errors will only be detected if both branches are used within a static context during the same evaluation run. The search procedure might find such cases if they exist, but cannot so easily associate it back to the static error. However, elaborating these terms does insert casts: on each branch to the dynamic type; these could be associated together with the error and tracked accordingly. 
  
\subsection{Categorising Programs Lacking Type Error Witnesses}
47 programs which timed out under the BDFS search procedure were manually inspected and classified as either:
\begin{itemize}
\item Witness Exists: BDFS failed to find an existing witness.
\item Dead Code: The error was in unreachable code by any type consistent instantiations. These then subclassified into:
\begin{itemize}
\item Pattern Cast Failure: An error was within a pattern matching branch. This additionally makes the branch unreachable. These cast failures would be found by an extended pattern directed instantiation algorithm (\cref{sec:ExtendedPatternMatching}).\footnote{Where inconsistent patterns would be attempted, and subsequently reduced to concrete cast failures in expressions.}
\item Unbound Constructor: A pattern matching branch matching an unbound constructor. These unbound errors are also detectable with extended pattern directed instantiation.
\item Wildcards: Error within trivially dead code bound to the inaccessible wildcard pattern: \code{let _ = ... in ...}.
\item Non-Trivial: Dead code which is less easily detectable. There was only one example which was due to infinite recursion for all inputs.
\end{itemize}
\item Hazel Bugs: Evaluation errors encountered stemming from \textit{unboxing bugs} in the main branch of Hazel. These were excluded from the statistics.
\end{itemize}
\Cref{fig:FailureDistribution} shows this distribution and three (paraphrased) examples are given in \cref{fig:FailureExamples}. The full classification is in \texttt{failure-classification.txt}.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Media/Figures/Failures}
\caption{Distribution of Failed Program Classes}
\label{fig:FailureDistribution}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{1\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/witness_exists}

Depth-first bias caused the procedure to try mostly permutations of \code{Sine(...)} and \code{Cosine(...)}. The error was on the \code{Average(...)} branch, not found within the time limit.
\caption{Witness Exists: prog2270.typed.hazel}
\end{subfigure}

\begin{subfigure}{1\textwidth}
\includegraphics[width=0.8\textwidth]{Media/Figures/dead_code_pattern_instantiation}
A tuple pattern is used when an \code{expr} is expected. Instantiation only tries value of type \code{expr}. Further, another error exists inside this inaccessible branch.
\caption{Dead Code -- Wildcard: prog0080.typed.hazel}
\end{subfigure}

\begin{subfigure}{0.7\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/dead_code_wildcard}
Product arity inconsistency is present in inaccessible code bound to the wildcard pattern.
\caption{Dead Code -- Pattern Cast Failure: prog0339.typed.hazel}
\end{subfigure}
\caption{(Paraphrased) Failure Examples}
\label{fig:FailureExamples}
\end{figure}

\label{sec:SearchCategories}
\subsubsection{Non-Termination, Unfairness, and Search Order}
DFS is fast, but has significant insurmountable issues with non-termination, resulting in it having worse coverage than BDFS (\cref{fig:PieChart}). Any time evaluation goes into an infinite loop, no more instantiations can be explored. Similarly, DFS will never backtrack to try previous instantiations, for example instantiating a pair of integers \code{(}\dyn\code{, }\dyn\code{)} would only try \code{(0,}\dyn\code{)}, \code{(0, 0)}, \code{(0, 1)}, \code{(0, -1)}, \code{(0, 2)} etc. but never \code{(1, 0)} for example (\cref{fig:DFS}). This was a common issue, occurring in 10\% of the search corpus. Bounded DFS, BFS, and interleaved DFS, were implemented in response to this. With BDFS performing best due to preference of evaluation over instantiation (and it's lesser memory footprint: \cref{fig:SearchPerformance}).

\begin{figure}[H]
\centering
\includegraphics[width=1.25\textwidth, trim={15cm 10cm 10cm 10cm},clip]{Media/Figures/DFS}
\caption{Non-exhaustive Instantiations in DFS}
\label{fig:DFS}
\end{figure}
BFS and IDFS would do significantly better if the search algorithm was tweaked to less frequently wrap evaluation steps deeper in the search tree, effectively reducing the \textit{cost} of evaluation. For example, it could wrap evaluation in one extra search depth only once every 10 steps, allowing 10 steps of evaluation to be explored for every instantiation. This is a delicate balancing act.

\subsubsection{Dead Code \& Nested Errors}

Dead code caused most time outs for the search procedure using BDFS. Errors within dead code cannot have a witness as they are not dynamically reachable. Therefore, dead code analysis will reduce timeouts.

Additionally, code can become dead due to errors. For example, 12 (32\%) of the dead code failed searches also had a nested error within a branch which was unreachable due to an error\footnote{Unbound constructor or inconsistent expectations.} within the branch pattern. Even if we find witnesses for these branch errors, the nested errors will still be unreachable.


\subsubsection{Dynamically Safe Code}
Some code with static errors is inherently safe dynamically, the typical example being:\\ \code{if true then 0 else "str"}. These have \textit{no} dynamic witness, and the search procedure can often prove this by running out of possible instantiations: BDFS proved no witness 12\% of the time. 

However, in general, the procedure may repeatedly instantiate infinitely many witnesses in dynamically safe code, and thereby not terminating. Therefore, search procedure timing out does not necessarily mean that a witness has not been found or that a witness does not exist.

\subsubsection{Cast Laziness}\label{sec:EvalCastLaziness}
Hazel treats casts lazily, only pushing casts on compound data to their elements upon usage. The instantiation procedure is therefore unable to instantiate parts of compound data until it is de-structured: for example, extracting the first element of a tuple. Further, Hazel does not detect cast errors between non-ground compound types (e.g. \code{[Int]} and \code{[String]}). Therefore, type inconsistencies between such types will not be found by the search procedure. These errors are less common, in the search corpus there were \textit{no} programs exhibiting this.

Fixing this requires reworking the Hazel semantics to be based around eager casts. Dynamic type systems with this cast semantics have been explored \cite{EagerCasts} and applied also to gradual type systems \cite{GradualEagerCasts}.\footnote{These papers refer to eager casts as \textit{coercions}.} Alternatively, instantiation could be performed less lazily, using more sophisticated methods to determine a holes type (accumulating information over \textit{multiple} casts). This would significantly expand the search space, and still require some form of eager cast failure checking.

The eager cast semantics would be able to detect more dynamic errors at an earlier point in execution. For example, \cref{fig:LazyCastError} shows an expression which evaluates to casts between inconsistent types, yet is not caught as a dynamic error yet. The error is only found when extracting the second element.
\begin{figure}[h]
\centering
\includegraphics[width=0.55\textwidth]{Media/Figures/cast_laziness_no_error}
\caption{Inconsistent Lazy Casts: No Cast Failure}
\label{fig:LazyCastError}
\end{figure}

\subsubsection{Combinatorial Explosion}
When multiple holes are involved when searching, the search space increases exponentially.

Combinatorial explosion clearly has a particularly big affect on IDFS and BFS, who prioritise trying different instantiations over evaluating. The space usage then becomes a big bottleneck on speed, causing the witnesses (which have been instantiated by now) to not actually evaluate far enough to detect the error.

Further, some errors will only occur for very specific inputs, for example only for \code{(23, 31)} and \code{(31, 23)} in \cref{fig:SpecificInstantiations}. Directing instantiation to maximise code coverage earlier would result in these errors to be found attempting fewer instantiations.
\begin{figure}\centering
\includegraphics[width=0.6\textwidth]{Media/Figures/very_specific_error}
\caption{Witness Requiring Very Specific Instantiations}
\label{fig:SpecificInstantiations}
\end{figure}

\subsection{Improving Code Coverage}
\label{sec:EvalHoleInstantiation}
Of the discussed classes of programs lacking a type error witness (\cref{sec:SearchCategories}), only 3 actually had concrete witnesses that were not found within the time limit. These occurred as the instantiations generated did not happen to take the branches involving the error.

These errors often require rather specific, interdependent, inputs to be missed by the current search procedure. Therefore, it is likely that programmers are \textit{also} more likely to not notice errors (in dynamic code), or not understand the errors (in static code).\footnote{After all, spotting the error might require understanding interdependent inputs.}

Intelligently directing hole instantiations to better cover the code would help here. Symbolic execution and program test generation is a well-researched area with numerous dedicated constraint solvers \cite{CITE MANY HERE} and translations to theories for SMT solvers \cite{CITE}.

One form of this, purely structural pattern-directed instantiation, discussed in \cref{sec:ExtendedPatternMatching}, would have discovered 2 of the 3 missed witnesses by BDFS. In order to extend this structural pattern-directed instantiation to worth also with base types (integers), requires more efficient ways to representing constraints. To extend this to include floats, inequalities, list lengths, etc. requires full-blown symbolic execution \cite{SymbolicExecutionSurvey}. 

However, even without this, the BDFS search procedure still retains a very high coverage over the search procedure (97\%) when excluding dead code.

\section{Holistic Evaluation}
\label{sec:HolisticEvaluation}

This section considers a number of examples of ill-typed Hazel programs, \textit{holistically} and \textit{qualitatively} evaluating how a user might use the three features and the existing bidirectional type error localisation \cite{MarkedLocalisation} to debug the errors. 


\subsection{Interaction with Existing Hazel Type Error Localisation}
Hazel has three types of errors which can be addressed to varying extent by this project:\footnote{Other errors not addressed being being: syntax errors, unbound names, duplicate \& malformed labels, redundant pattern matches etc.} \textit{inconsistent expectations} (where a term's analytic and synthetic types are inconsistent), \textit{inconsistent branches }(where branches are synthetic and inconsistent, this also applies to values in a list literal), and \textit{inexhaustive match} warnings. 

\Cref{sec:ErrorCorrespondence} showed how dynamic witnesses can be associated to both classes of inconsistency errors, with inconsistent branch errors being more difficult to detect in general. Generating examples for inexhaustive match statements was not a core aim of this project, but indeterminate evaluation can do this automatically (see the \texttt{inexhaustive-matches} branch \textbf{TODO}). Faster, static generation of counter-examples to match statements is already a (mostly)\footnote{Data abstraction can cause issues: for example Views \cite{Views} and Active Patterns {\cite{ActivePatterns}}. Hazel is implementing modules, but yet to consider pattern matching on abstract data.} solved problem in static languages \cite{PatternMatchingWarnings}, and is under development for Hazel.\footnote{Indeterminate evaluation is very inefficient compared to pattern matrices. These matrices would also be useful in directing hole instantiation (\cref{sec:EvalHoleInstantiation}).}

The situations where a programmer does not understand why an error occurs generally arise due to a \textit{misunderstanding} about the types of the code. In which case, existing error localisation is not so useful as the location of an error is determined while making \textit{differing assumptions} about the types than the programmer. Type slicing, along with the context inspector (which shows the type, term, and describes the error of a selected expression: \cref{fig:ContextInspector}), can help the programmer understand which assumptions the type system is making and \textit{why} it is. 

\begin{figure}[h]\centering
\includegraphics[width=0.7\textwidth]{Media/Figures/context_inspector}
\caption{Selected Static Error described by Hazel Context Inspector}
\label{fig:ContextInspector}
\end{figure}

On the other hand, when the programmer and system agree on the types of a program, bidirectional typing generally localises the error(s) correctly and intuitively \cite{BidirectionalTypes, MarkedLocalisation}. However, Hazel has explored introducing global inference, where errors involve wider interaction between multiple regions of code, making error localisation (even when the type checker and user agree on the types) more difficult \cite{StudentTypeErrorFixes}. Further, deviations between the systems and the programmer mental model become more common with fewer annotations. Dynamic witnesses can help here \cite{SearchProc}, and forms of slicing supporting global inference is an interesting further direction.\footnote{Global constraint-based inference would require some form of additional constraint slicing. Similar ideas have been explored in error slicing \cite{ErrSlice, HaackErrSlice}.}

Finally, there may be errors in \textit{dynamic regions} of the code. These are not found statically. The search procedure can test dynamic code for such type errors automatically. Indeterminate evaluation could also, in future, be used to perform more general property testing of code, in the sense of SmallCheck \cite{SmallCheck}. Adding annotations to find type errors in dynamic code is time-consuming, large numbers of annotations are required to make the code static enough to detect errors. In this case, it can make sense to use the search procedure to find errors more quickly. For example, \cref{fig:HalfAnnotated} shows a relatively annotated map functionwhich still does not provide enough type information to detect the error statically, that concatenation (\code{@}) operator is mistakenly used instead of cons (\code{::}).


\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Media/Figures/partial_annotations}
\caption{Partially Annotated Program misses Error}
\label{fig:HalfAnnotated}
\end{figure}

\subsection{Examples}
\label{sec:EvalExamples}
Returning to the map example, when fully annotated a static error can located at \code{f(x)} which has type \code{Int} but expects \code{[Int]}. The error slice shows (in pink) why \code{Int} is synthesised (due to input \code{f} being annotated \code{Int -> Int} and applied) and (in blue) why a list\footnote{A full analysis slice would also include the full \code{[Int]} annotation. But the inconsistency arises from the list part.} is expected (being an argument of list concatenation). See \cref{fig:MapExample}.
\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/map_example}
\caption{Error}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/map_example_sliced}
\caption{With Error Slice}
\end{subfigure}
\caption{Example: Type Slice of Inconsistent Expectations}
\label{fig:MapExample}
\end{figure}

Similarly, error slicing works for inconsistent branches: \cref{fig:InconsistentBranchesExample} demonstrates a somewhat more complex inconsistency involving non-local bindings. With minimal error slices highlighting only the bindings, application, and the addition \code{+} and string concatenation \code{++} operators.

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/inconsistent_branches_example}
\caption{Error}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/inconsistent_branches_example_sliced}
\caption{With Error Slice}
\end{subfigure}
\caption{Example: Type Slice of Inconsistent Branches}
\label{fig:InconsistentBranchesExample}
\end{figure}

However, not every static type error slice is concise or obvious, especially if the user is still misunderstanding the types of operators involved. I demonstrate this by returning to the example from the introduction (\cref{fig:ConcatError}) where the user is mistaking list cons \code{::} for list concatenation \code{@}. The type slice here does highlight the \code{::} and annotation as enforcing \code{x} to be an \code{Int}, but the user would not understand why if they still think expect \code{::} to perform concatenation. Additionally, there is considerable noise\footnote{A large synthesis slice part explaining why \code{x} is a list} distracting from this. In comparison, the type witnesses demonstrate concretely how the program goes wrong: that the lists are not being concatenated. The user can cycle through increasingly larger witnesses\footnote{Looking at larger witnesses can be useful to spot pattern with what is happening overall, i.e. that the lists are being consed, not concatenated.} (the 2nd fully determinate witness given in the figure). Finally, the cast slice to \code{Int} concisely retrieves relevant part of the original type slice.
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/concat_error}
\caption{Error: Mistaken Cons for Concat}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/concat_error_type_slice}
\caption{Verbose Type Slice}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/concat_error_cast_slice}
\caption{A Witness \code{([[], []])} leading to concise Cast Slice}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/concat_error_trace}
\caption{Last 3 steps in witness trace: Shows \code{::} being List Cons}
\end{subfigure}
\caption{Example: Witnesses and Cast Slice more Understandable than Type Slice}
\label{fig:ConcatError}
\end{figure}
Another, more subtle, example of this is confusing curried and non-curried functions: \cref{fig:TastyCurry} implements a \code{fold} function taking curried functions, but tries to apply an uncurried \code{add} function to sum int lists. This example shows how a single error can result in \textit{multiple} cast errors, each having a more concise slice explaining them. See how the type slice had two inconsistencies (in both the argument and return type of add), which were then separated into two different cast errors: \code{add} expected it's argument to be a tuple, \code{fold} expected the result of \code{add} to be a function.

\begin{figure}[h]
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/curries}
\caption{Confused uncurried for curried \code{add}}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/curries_type_slice}
\caption{Somewhat Verbose Type Slice}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/curries_expects_tuple}
\caption{Witness (\code{[0]}) expects input to \code{add} to be a tuple}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/curries_expects_function}
\caption{Witness (\code{[0]}) expects output of \code{add(0)} to be a function}
\end{subfigure}
\caption{Example: Subtle Currying Error}
\label{fig:TastyCurry}
\end{figure}

Finally, when code is dynamic, errors might not be statically found, and type slices have less information to work with. \Cref{fig:DynamicExample} considers finding a simple error in a dynamic function. Cast slicing still works even without type annotations, allowing errors to blame regions of the source code.
\begin{figure}[h]
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/dynamic_code_error}
\caption{Error}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=1\textwidth]{Media/Figures/dynamic_code_error_cast_slice}
\caption{Automated witness found with cast slice blaming \code{+} operator}
\end{subfigure}
\caption{Example: Searching for Error in Dynamic Code}
\label{fig:DynamicExample}
\end{figure}


\subsection{Usability Improvements}\label{sec:UIImprovements}
As designing an intuitive user interface was not a core goal, therefore actual use of the features can be quite awkward or unintuitive. Below, I propose various improvements, for which the architecture, of both Hazel and the newly implemented features, is sufficiently abstract and flexible to easily support:\footnote{Some of which were detailed in the Implementation chapter.}


\begin{itemize}
\item Displaying type slices only \textit{upon request} using the Hazel context inspector, which displays the \textit{analysing} and \textit{synthesising} types of the selected expression, see \cref{fig:ContextInspector}.
\item Allowing the user to deconstruct type slices to query, for example: if a term has a function type, they could select the \textit{return type} and get only the part of the slice relevant to that, or select the function arrow to get the part that makes it a \textit{function} (ignoring the argument and return slice parts). The UI for this could again be via clicking on the type within the context inspector.

This interactive version could really help a user to understand how parts of code come together to produce their types. This is a powerful feature made possible by type-indexed slices.
\item Graphs visualising dependence of casts. Showing the \textit{execution} context that leads to a cast error; these would be more concise summaries than full evaluation traces.
\item UI for visualising the search procedure's execution traces and instantiations. Currently, the search procedure only gives the final result containing cast error. This can be integrated with Hazel's existing trace visualiser. Further, compression of traces to make them more readable, e.g. skipping over irrelevant\footntoe{Irrelevant to the error.} function calls.
\item Key bindings to more quickly cycle through indeterminate evaluation instantiation paths.
\end{itemize}