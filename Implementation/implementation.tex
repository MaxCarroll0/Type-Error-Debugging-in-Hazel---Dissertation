\chapter{Implementation}\label{chap:Implementation}
This project was conducted in two major phases:

First, I constructed a core mathematical theory for \textit{type slicing} and \textit{cast slicing} formalising what these ideas actually were and considered the changes to the system presented by Seidel et al. for the \textit{type error witnesses search procedure} to work in Hazel.  

Then, I implemented the theories, making it suitable for implementation and extending it to the majority of the Hazel language. Further, suitable deviations from the theory were made upon critical evaluation and are detailed throughout.

\textbf{Annotate the above with the relevant section links!}
\section{Type Slicing Theory}\label{sec:TypeSlicingTheory}
\textbf{Might be worth deferring even more mathematical definitions to the appendices in favour shorter worded definitions in this section}

\textbf{Replace all occurrences of `typing context' with `typing assumptions' to avoid name clash with expression/program contexts.}

I develop a novel method I term \textit{type slicing} as a mechanism to aid programmers in understanding \textit{why} a term has a given type via static means. Three slicing mechanism have been devised with differing characteristics, all of which associate terms with their typing derivation to produce a \textit{typing slices}. 

The first two criteria attempt to give insight on the structure of the typing derivations, and hence how types are decided. While the third criterion gives a complete picture of the regions of code which contribute to the a term's type.

I would like to stress that the second and third criterion were \textit{very challenging} to formalise, requiring extensive mathematical machinery: \textit{context slices} (\cref{sec:ContextTypingSlices}), \textit{type flows} (\textbf{ref appendix}), \textit{checking context} (\cref{def:CheckingContext}), \textit{type-indexed slices} (\cref{sec:TypeIndexedSlices}).

\textbf{Clarify on and ensure terminology is consistent (typing vs checking/analysis vs synthesis)}

\textbf{Make some brief arguments into why the first two criteria are still useful.}

\textbf{PLACE ALL IMPORTANT DEFINITIONS INSIDE DEFINITION ENVIRONMENTS}

\subsection{Expression Typing slices}\label{sec:ExpressionTypingSlices}
A \textit{expression typing slice} $\rho$, is a pair $\varsigma^\gamma$, consisting of an \textit{expression slice} $\varsigma$ and \textit{typing context slice} $\gamma$ which are calculated based on some typing \textit{criterion}\footnote{One of the three slicing mechanisms.} based on the typability of the slice $\varsigma$ under context $\gamma$. 

Intuitively, an expression slice is a Hazel external expression highlighting the sub-terms of relevance to the \textit{typing criterion}. For example if my criterion is to \textit{omit terms which are typed as} \code{Int}, then the following expressions highlights as:

\[\hlcmaths[yellow!30]{(\lambda x: \code{Int}.\ \lambda y : \code{Bool}.}\ x\hlcmaths[yellow!30]{)(}1\hlcmaths[yellow!30]{)}\]

Formally, I represent this by specifying which sub-terms are omitted in the highlighted expression. So, Replace each omitted sub-term with a \textit{gap}, notated $\gap$. This is the same definition of a slice as presented in \cite{FunctionalProgExplain}.\footnote{With their `holes' equating with my `gaps'. Different terminology used to distinguish with Hazel's holes} i.e. representing the above highlighting we get slice:
\[(\lambda x : \code{Int}.\ \lambda y : \code{Bool}.\ \gap)(\gap)\]


Additionally, it is useful to omit variable names. For this I introduce \textit{patterns} $p$ for variable bindings: 
\[p ::= \gap \mid x\]

This gives the following extended syntax of expression slices, $\varsigma$, extending \cref{fig:syntax}:
\[\varsigma ::= \gap \mid  c \mid x \mid \lambda p : \upsilon.\ \varsigma \mid \lambda x.\ \varsigma \mid \varsigma(\varsigma) \mid \hole^u \mid \hole[\varsigma]^u \mid \varsigma : \upsilon\]
Where $\upsilon$ are types, similarly with potential omitted sub-term gaps:
\[\upsilon ::= \gap \mid \dyn \mid b \mid \upsilon \to \upsilon\]
These slices are then allowed to be \textit{typed} by representing gaps $\gap$ by holes of fresh metavariables $\hole^u$ in \textit{expressions}, fresh variables in \textit{patterns}, and the dynamic type in \textit{types}, see \textbf{(fig APPENDIX)}. From here-on consider $\gap$ as interchangeable with a hole $\hole^u$ of fresh metavariable $u$ or the dynamic type.

We then have a \textit{precision} relation on expression slices, $\varsigma_1 \sqsubseteq \varsigma_2$ meaning $\varsigma_1$ is less or equally precise than $\varsigma_2$, that is $\varsigma_1$ matches $\varsigma_2$ structurally except that some subterms may be gaps, see \textbf{ref appendix}. For example, see this precision chain:
\[\gap \sqsubseteq\gap + \gap\sqsubseteq 1 + \gap \sqsubseteq 1 + 2\]
We have that $\sqsubseteq$ is a partial order (\textbf{cite}), that is, satisfies relexivity, antisymmetry, and transitivity. Respectively:
\[\inference{}{\varsigma \sqsubseteq \varsigma} \quad \inference{\varsigma_1 \sqsubseteq \varsigma_2 & \varsigma_2 \sqsubseteq \varsigma_1}{\varsigma_1 = \varsigma_2} \quad \inference{\varsigma_1 \sqsubseteq \varsigma_2 & \varsigma_2 \sqsubseteq \varsigma_3}{\varsigma_1 \sqsubseteq \varsigma_3}\]
We also have a \textit{bottom} (least) element, $\gap \sqsubseteq \varsigma$ (for all $\varsigma$). This relation is trivially extended to include complete expressions $e$ which satisfy that: if $e \sqsubseteq \varsigma$ then $e = \varsigma$, i.e. complete terms are always upper bounds of precision chains.

An expression slice $\varsigma$ \textit{of} $e$ is a slice such that $e \sqsubseteq e$.

\textit{Typing context slices} are simply a typing context $\Gamma$, which is used to represent the notion of \textit{relevant typing assumption}. Typing contexts are just functions mapping variables to types notated $x : \tau$ (see \cref{sec:TypingJudgements}). Functions are sets, so they also have a partial order of subset inclusion, $\subseteq$. Again, we have a bottom element, $\emptyset$.  These are notated $\gamma$ and if $\gamma \subseteq \Gamma$ then $\gamma$ is a slice \textit{of} $\Gamma$.

The precision relation and subset inclusion can be extended pointwise to give a partial order, $\sqsubseteq$, on expression typing slices:
\[[\varsigma_1\mid \gamma_1] \sqsubseteq [\varsigma_2\mid \gamma_2] \iff  \varsigma_1 \sqsubseteq \varsigma_2 \text{ and } \gamma_1 \subseteq \gamma_2\]

expression typing slices will often be grouped and indexed upon expressions and typing contexts, $P_e^{\Gamma}$ which contains all slices $\rho \sqsubseteq (e, \Gamma)$. So, the set $P_e^{\Gamma}$ forms a lattice (\textbf{cite}) with unique least upper bound $[e, \Gamma]$ and greatest lower bound $[\gap, \emptyset]$. Similarly, an element $\rho$ in $P_e^{\Gamma}$ can referred to as a expression typing slice \textit{of} $e$ under $\Gamma$.
\subsection{Context Typing Slices}\label{sec:ContextTypingSlices}
\textbf{add pattern context. }

\newcommand{\C}{\mathdcal{C}}
Formally, an \textit{expression context} $\mathdcal{C}$ is an expressions with \textit{exactly one} sub-term marked as $\cmark$:\footnote{The two separate syntax definition for application allow a \textit{mark} to be in either the left or right expression, but \textit{not both}.}
\[\C ::=  \cmark \mid \lambda x : \tau.\ \C \mid \lambda x.\ \C \mid \C(e) \mid e(\C) \mid \C : \tau\]

Where $\C\{e\}$ substitutes expression $e$ for the mark $\cmark$ in $\C$, the result of this is necessarily an expression. Additionally, contexts are composable: substituting a context into a context, $\C_1\{\C_2\}$ produces another valid context, notate this by $\C_1 \circ \C_2$\footnote{Context can alternatively be though of as functions from expressions to expressions.}.


\newcommand{\Cs}{\mathdcal{c}}
\newcommand{\p}{\mathdcal{p}}
Similarly to expressions, contexts can be extended to \textit{context slices} by allowing slices within:
\[\Cs ::= \cmark \mid \lambda p : \upsilon. \Cs \mid \Cs(\varsigma) \mid \varsigma(\Cs) \mid \Cs : \upsilon\]

However, the precision relation $\sqsubseteq$ is defined differently, requiring that the mark $\cmark$ must remain in the same position in the context structurally speaking. For example $\cmark(\gap) \sqsubseteq \cmark(1)$, but $\cmark \not \sqsubseteq \cmark(1)$. This can be concisely defined by \textit{extensionality} (\textbf{cite}):

\begin{definition}[Context Precision]\label{def:ContextPrecision}
If $\Cs'$ and $\Cs$ are context slices, then $\Cs' \sqsubseteq \Cs$ if and only if, for all expressions $e$, that $\Cs'\{e\} \sqsubseteq \Cs\{e\}$.
\end{definition}
Again, we refer to a context slice $\Cs$ of $\C$ as one satisfying that $\Cs' \sqsubseteq \C$.

We also get that filling contexts preserves the precision relations both on expression slices \textit{and} context slices:
\begin{conjecture}[Context Filling Preserves Precision]
For expression slice $\varsigma$ and context slice $\Cs$. Then if we have slices $\varsigma' \sqsubseteq \varsigma$, $\Cs' \sqsubseteq \Cs$ then also $\Cs'\{\varsigma'\} \sqsubseteq \Cs\{\varsigma\}$.
\end{conjecture}
Therefore, context slices $\Cs$ can be though of as monotone function (\textbf{cite})...

Show that composition is also preserved over precision, i.e. it is a functor.

\textit{Make some references to category theory, i.e. category of slices with morphisms being context slices. Or that slices form categories on expression $e$ and contexts are a functor between expression typing slice categories. i.e. contexts are monotonic functions}

\textbf{rewrite}
The accompanying notion of a \textit{typing assumption slice} can be extended to \textit{functions on typing assumptions}. This function can represent what typing assumptions need to be \textbf{added}, or can be \textit{removed} when placing $e$ in it's context slice.

Functions can be composed and a precision partial order can also be defined via extensionality:
\begin{definition}[Function Precision]\label{def:FunctionPrecision}
If $f'$ and $f$ are functions, then $f' \sqsubseteq f$ if and only if, for all typing contexts $\Gamma$, that $f'(\Gamma) \subseteq f(\Gamma)$.
\end{definition}
Again, such functions are monotone \textbf{(find the name of this mathematical property... sorta an extended monotonicity)}:
\begin{conjecture}[Function Application Preserves Precision]
For typing context $\Gamma$ and function $f$. Then if we have slices $\Gamma' \subseteq \Gamma$, $f' \sqsubseteq f$ then also $f'(\Gamma') \sqsubseteq f(\Gamma)$.
\end{conjecture}

This pair of context slice and function $f$ will be referred to as a \textit{context slice} and notated $\p^f$, should $f$ be the identity it may be omitted in shorthand.

Extend composition to program contexts and substitution of expression typing slices. Again as slices are a superset of expressions, then this extends to expression etc.

When 

\subsection{Type-Indexed Slices}\label{sec:TypeIndexedSlices}
\textbf{Do type-indexed expressions slices actually need contexts on the sub-slices? Are these every not claculable from the rule they are being used in}

\textbf{Have context i.e. contexts with a $\_$ and let the syntax write it directly as a \textit{type-indexed context slice}, i.e. a type-indexed slice, but with the slice $\rho$s replaced with $\p$s. Then allow easy syntax to create from a type indexed slice i.e.}
\[(\p\{\dyn \mid \rho\})(\cmark) = \p\{\dyn \mid \rho(\cmark)\}\]
Reverse composition order for this?? Also, syntax for going from indexed context to indexed expression i.e. $\{\}$


For \textit{criteria 3 and cast slicing}, there is a need to decompose slices to find sub-slices which contribute to specific portions of a compound type. For example, which part of the expression typing slice was related to the argument type of a function specifically.

\textbf{Give EXAMPLE}

A \textit{type(-indexed) slice} $s$ consists of: a expression typing slice, a context slice, and a \textit{type} \textit{index} $i$. This index is either an \textit{atomic} type label or is \textit{compound}, consisting of type slices conforming to the structure of types:
\[i ::= \dyn \mid b \mid s \to s\]
\[s ::= \p\{i \mid \rho\}\]
The type that a type slice $s$ \textit{represents} is the slice retaining only it's type labels. This will be notated by $\type{s}$, defined inductively:
\[\type{\p\{\dyn\mid \rho\}} = \dyn \quad \type{\p\{b \mid \rho\}} = b\]
\[\type{\p\{s_1 \to s_2 \mid \rho\}} = \type{s_1} \to \type{s_2}\]

This same notation will also be used to extract the type of a type index $i$, similarly defined.

A term $e$ in some context $\C$ will be associated with a \textit{type slice} with the meaning that $\rho$ contains a expression typing slice for typing $e$ and $\p$ contains a context slice for typing $e$ in context $\C$ according to some criterion. Typically the context slice would correspond to type analysis and the expression typing slice would correspond to type synthesis.

The compound type slices must satisfy the crucial property that the sub-terms are sub-\textit{slices} of $\rho$. That is:
\[\p\{\p_1\{i_1 \mid \rho_1\} \to \p_2\{i_2 \mid \rho_2\} \mid \rho\} \implies \p_1\{\rho_1\} \sqsubseteq \rho\text{ and }\p_2\{\rho_2\} \sqsubseteq \rho\]

The precision relation can be extended to slices pointwise upon the expression typing slice and context slice for atomic types $a$ (i.e. $a ::= \dyn \mid b$):
\[\p'\{a \mid \rho'\} \sqsubseteq \p\{a \mid \rho\} \iff \p' \sqsubseteq \p\text{ and } \rho' \sqsubseteq \rho\]
And recursively for compound slices:\footnote{Note, function arguments are \textit{covariant} for this.}
\begin{align*}
\p'\{s_1' \to s_2' \mid \rho'\} \sqsubseteq \p\{s_1 \to s_2 \mid \rho\} \iff &\p' \sqsubseteq \p,\ \rho' \sqsubseteq \rho,\\
&s_1' \sqsubseteq s_1,\text{ and }s_2' \sqsubseteq s_2
\end{align*}

Composition of expression typing slices to the outer context is possible, $(\p' \circ \p)\{i \mid \rho\}$, and is notated shorthand as $\p'\{s\}$ for $s = \p\{i \mid \rho\}$.

Additionally, if $\p$ is empty, $\cmark^\emptyset$, a type slice may be notated without it: $i \mid \rho$. Equally, when $\rho$ is empty, $\gap^\emptyset$, then notate $\p'\{i\}$. This means that if both $\p, \rho$ are both empty then we have $i$ which is syntactically identical to types $\tau$, so we can trivially treat types as \textit{empty type slices}.  

Then function matching $\funmatch$ can be extended to slices in multiple different ways with different uses depending on the context, see the appendix \textbf{ref}.

\subsection{Criterion 1: Synthesis Slices}
\label{sec:SynthesisSlices}
For \textit{synthesis type slices} we consider an expression synthesising a type $\tau$ under some context $\Gamma$:
\[\synthesis{e}{\tau}\]
And consider the slices in $P_e^{\Gamma}$ and attempt to find the minimum slice $\rho = [\varsigma\mid \gamma]$ constraining that $\rho$ also synthesises the same type $\tau$ under the restricted context $\gamma$:
\[\synthesis[\gamma]{\varsigma}{\tau}\]
Where minimality requires that no other (strictly) less precise slice satisfies the criterion. That is: for any slice $\rho' = [\varsigma'\mid \gamma']$, if $\synthesis[\gamma']{\varsigma'}{\tau}$ and $\rho' \sqsubseteq \rho$, then $\rho' = \rho$.

\textbf{GIVE CONCRETE EXAMPLE HERE, use highlighting}

I conjecture that, under the Hazel type system, there exists a unique minimum slice for each $\synthesis{e}{\tau}$:\footnote{Would follow from uniqueness of typing derivations in Hazel.}
\begin{conjecture}[Uniqueness]\label{conj:SynthesisSliceUniqueness}
If $\rho$ and $\rho'$ are \textit{minimum synthesis slices} for $\synthesis{e}{\tau}$, then $\rho = \rho'$.
\end{conjecture}

These slices can be found by omitting portions of the program which are \textit{type checked}. If, $\analysis{e}{\tau}$, then by use of the subsumption rule we also have that $\analysis{\gap}{\tau}$:
\[\inference[Subsumption]{\synthesis{\gap}{\dyn} & \tau \sim \dyn}{\analysis{e}{\tau}}\] 
As the dynamic type is consistent with any type: $\dyn \sim \tau$.

Then, to find the \textit{minimum synthesis slice}, we can mimic the Hazel type synthesis rules (see \cref{fig:typing}), replacing uses of type analysis with gaps. Creating a judgement $\synthesisslice{e}{\tau}{\rho}$ meaning: \textit{$e$ that synthesises type $\tau$ under context $\Gamma$ produces minimum synthesis slice $\rho$}.

To demonstrate, the expression slice of a variable $x$ can only be either $x$, requiring the use of $x : \tau$ from the context:
\[
\inference[SVar]{x : \tau \in \Gamma & \tau \neq \dyn}{\synthesisslice{x}{\tau}{[x\mid x:\tau]}}\]
But if $x : \dyn$, then the (empty) slice $[\gap, \emptyset]$ also synthesises $\dyn$, so instead use this. 

For functions, we can recursively find the slice of the function body (which synthesises it's type in the original rules, hence having a minimum synthesis slice) and place inside a function. 
If the assumption $x : \tau_1$ was \textit{required} in synthesising that type, then this name must be present in the expression slice and the context slice no longer requires this assumption to type check the sliced function:
\[\inference[\tiny SFun]{\synthesisslice[\Gamma,x:\tau_1]{e}{\tau_2}{[\varsigma \mid \gamma, x : \tau_1]} }{\synthesisslice{\lambda x:\tau_1.\ e}{\tau_1 \to \tau_2}{[\lambda x : \tau_1.\ \varsigma \mid \gamma]}}\]
Otherwise, if $\gamma$ does not use variable $x$ then this binding may be omitted:
\[\inference[\tiny SFunConst]{\synthesisslice[\Gamma,x:\tau_1]{e}{\tau_2}{[\varsigma \mid \gamma]} & x \not \in \mathrm{dom}(\gamma)}{\synthesisslice{\lambda x:\tau_1.\ e}{\tau_1 \to \tau_2}{[\lambda \gap : \tau_1.\ \varsigma \mid \gamma]}}\]

For function applications we can simply omit the argument, while the slice for the function can be obtained as it synthesises it's type.
\[\inference[\tiny SApp]{\synthesisslice{e_1}{\tau_1}{[\varsigma_1 \mid \gamma_1]}}{\synthesisslice{e_1(e_2)}{\tau}{[\varsigma_1(\gap) \mid \gamma_1]}}\]
The remaining rules are in \cref{fig:SynthesisSlices}.

It is \textit{expected} \textbf{(Proof TODO)} that these rules do indeed always produce a slice for any expression which synthesises a type, and that this slice is minimal:
\begin{conjecture}[Correctness]
\label{conj:SynthesisSliceCorrectness}
If $\synthesis{e}{\tau}$ then:
\begin{itemize}
\item $\synthesisslice{e}{\tau}{\rho}$ where $\rho = [\varsigma \mid \gamma]$ with $\synthesis[\gamma]{\varsigma}{\tau}$.
\item For any $\rho' = [\varsigma' \mid \gamma'] \sqsubseteq [e\mid \Gamma]$ such that $\synthesis[\gamma']{\varsigma'}{\tau}$ then $\rho \sqsubseteq \rho'$.
\end{itemize}
\end{conjecture}

\subsubsection{Extension to Type-Indexed Slices}
This mechanism can be trivially extended to type-indexed slices, where types being synthesised can be replaced by slices directly, i.e. \textit{slice synthesis}. See the appendix \textbf{ref AND FIX}.

The only interesting case is the fact that slices for argument types to functions can now be accessed, so must be represented:

\subsection{Criterion 2: Analysis Slices}\label{sec:AnalysisSlices}
A similar idea can be devised for analysis slices. Essentially, we do the opposite of \textit{criterion 1} and omit sub-terms where \textit{synthesis} was used. The objective is to show \textit{why} a term is required to be checking against a type.

The useful notion to represent these are \textit{context slices}. It is the terms immediately \textit{around} the sub-term where the type checking is enforced:

For example, when checking this annotated term:
\[(\lambda x. \hole^u) : \code{Bool} \to \code{Int}\]
The \textit{inner hole term} $\hole^u$ (underlined from now on) is required to be consistent with \code{Int} due to the annotation. The context slice will then be:
\[\hlcmaths[yellow!30]{(\lambda} x\hlcmaths[yellow!30]{.} \underline{\hole^u} \hlcmaths[yellow!30]{) : } \code{Bool} \hlcmaths[yellow!30]{\to \code{Int}}\]
Intuitively, this means that the \textit{contextual} reason for $\hole^u$ to be required to be an \code{Int} is: that it is within a function which the annotation enforced to check against $\code{Bool} \to \code{Int}$, of which only the return type ($\code{Int}$) is relevant.

In other words, if the slice was type synthesised, then the hole term would still be \textit{required} to check against \code{Int}.

For this criterion we consider only the smallest context that resulted in a term being type checked, for example in the following term:
\[\underline{1} \hlcmaths[yellow!30]{: \code{Int}} : \dyn : \code{Bool}\]
The context that enforced 1 to be checked against an \code{Int} was \textit{only} the inner annotation. We refer to this as the \textit{checking context}. The term when inside it's checking context will always synthesis a type.\footnote{If it analysed against a type, then this would have it's own checking context. We merge these contexts.}

\textbf{These definitions below are verbose and it might be better to just leave it intuitive for this, with definition deferred to appendix.}

To represent this, we use a notion of a types \textit{flowing} from other types inside a derivation, i.e. if a type is decomposed, then it's parts \textit{flow} from the complete type. This can be formalised by creating a \textit{type flow} rules \textbf{(Ref to appendix)}.\footnote{This type flow is closely related to mode correctness.} Under this flow, every checked type $\tau'$ in a derivation has an \textit{origin} synthesis rule producing \textit{first} type which $\tau'$ flows from.

\textbf{Maybe give example?}

Then, the \textit{checking context} we want to consider is that of the conclusion of the rule containing the source of the type:
\begin{definition}[Checking Context]
\label{def:CheckingContext}
For a derivation $\synthesis{e}{\tau}$ containing a sub-derivation $\analysis[\Gamma'']{e''}{\tau''}$ and where the \textit{origin} of $\tau''$ is another sub-derivation $\synthesis[\Gamma']{e'}{\tau'}$. Then either:
\begin{itemize}
\item $e''$ is a sub-term of $e'$: the checking context of $e''$ is $\C$ such that $e' = \C\{e''\}$.
\item Otherwise, $\synthesis[\Gamma'']{e''}{\tau''}$ must be a premise in another rule whose conclusion is a synthesis $\synthesis[\Gamma''']{e'''}{\tau'''}$ where $e'$ is a sub-term of $e'''$. The checking context is $\C$ such that $e''' = \C\{e''\}$
\end{itemize}
\end{definition}
It is clear that this must then be the smallest context containing derivations of both the \textit{checked expression} and it's \textit{origin}. This checking context can be defined more intuitively using rules (\textbf{see appendix}) and proven to coincide with the definition above.

An \textit{analysis slice} is a slice of a checked term's \textit{checking context}:
\begin{definition}[Analysis Slice]\label{def:analysisslice}
For a derivation $\synthesis{e}{\tau}$, containing a sub-derivation $\analysis[\Gamma']{e'}{\tau'}$ with checking context $\C$. Then we have that $\synthesis[\Gamma]{\C\{e'\}}{\tau''}$. 

An \textit{analysis slice} of $e'$ is a program context slice $\Cs^f$ such that:
\begin{itemize}
\item $\Cs$ is a slice of $\C$, that is, $\Cs \sqsubseteq \C$.
\item $f$ is a slice of $\Gamma' \mapsto \Gamma$. \textbf{Formalise this better...}
\item $\Cs\{e\}$ synthesises a type consistent\footnote{$e'$ within the sliced context might now synthesise a \textit{less precise} type.} with $\tau''$ under typing context $f(\Gamma')$ and still contains the sub-derivation checking $e'$ against $\tau'$ in checking context $\Cs$:
\[\synthesis[f(\Gamma')]{\Cs\{e'\}}{\tau'''}, \quad \tau''' \sim \tau'', \quad \analysis[\Gamma']{e'}{\tau'}\]
\end{itemize}
\end{definition}

The \textit{minimum analysis slice} is just the minimum under the precision ordering $\sqsubseteq$. And it must be unique:

\begin{conjecture}[Uniqueness]\label{conj:AnalysisSliceUniqueness}
If $\rho$ and $\rho'$ are \textit{minimum analysis slices} for sub-terms $e'$ of $e$ where $\analysis{e}{\tau}$, then $\rho = \rho'$.
\end{conjecture}

This can again be represented by a judgement read as, \textit{$e$ which type checks against $\tau$ in checking context $\C$ has analysis slice $\p$}:
\[\analysisslice{\C}{e}{\tau}{\p}\]

There are two situations which enforce \textit{checking contexts}, annotations:

\[\inference{\analysis{e}{\tau}}{\analysisslice{\cmark : \tau}{e}{\tau}{\cmark : \tau}}\]
And applications, for which we need a slice of the application for the \textit{argument} type of the function, which has previously been devised for \textit{type-indexed synthesis slices} (\textbf{ref}):
\[\inference{\synthesissliceindexed{e_1}{\p\{\Cs_1^{f_1}\{i_1 \mid \varsigma_1^{\gamma_1}\} \to s_2 \mid \rho\}} & \analysis{e_2}{\type{i_1}}}{\analysisslice{e_1(\cmark)}{e_2}{\type{i_1}}{(\Cs_1\{\varsigma_1\})(\cmark)^{f_1}}}\]
That is, if $e_1$ synthesises some type $\tau_1 \to \tau_2$ (i.e. $\type{i_1} \to \type{s_2}$), then the slice for $\tau_1$ in the context as a slice $e_1$ is $\Cs_1\{\varsigma_1\}$.

Finally, type analysis rules pass the context down (i.e. sub-terms have the same checking context) and also records that the context now needs to remove the $x:\tau_1$ assumption:
\[\inference{\analysisslice{\C}{\lambda x.\ e}{\tau_1 \to \tau_2}{\p}}{\analysisslice[\Gamma, x:\tau_1]{\C \circ (\lambda x.\ \cmark)}{e}{\tau_2}{\mathrm{ret}(\p)}\circ(\lambda x.\ \cmark)^{(-)\backslash x:\tau_1}}\]
\textbf{Todo, case for if x is not needed...}


Where $\mathrm{ret}(\p)$ takes the part of the slice that was required in synthesising $\tau_2$, defined in \textbf{appendix}. This direct definition is quite complex; checking against \textit{type-indexed slices} representing the synthesis of $\tau_1 \to \tau_2$ is \textit{much easier}.

This definition does indeed find the minimum analysis slice:
\begin{conjecture}[Correctness]\label{conj:AnalysisSliceCorrectness}
If $\synthesis{e}{\tau}$ with sub-derivation $\analysis{e'}{\tau'}$ in checking context $\C$ then we also have that $\analysisslice{\C}{e'}{\tau'}{\p}$ and:
\begin{itemize}
\item $p$ is an analysis slice for $e'$.
\item For any $\p' = [\Cs' \mid \gamma'] \sqsubseteq [\C\mid \Gamma]$ such that $\p'$ is an analysis slice of $e'$ then $\p \sqsubseteq \p'$.
\end{itemize}
\end{conjecture}

\subsubsection{Extension to Type-Indexed Slices}
As mentioned previously, using \textit{type-indexed synthesis slices} calculated via \textit{criterion 1} as input makes analysis slices much easier to calculate. 

Additionally, the rules in this form end up being more closely tied to the Hazel typing rules and hence easier to formalise.

\textbf{See appendix}

\subsection{Criterion 3: Contribution Slices}
\label{sec:ContributionSlices}
This criterion aims to highlight all regions of code which \textit{contribute} to the given type (either synthesised or analysed). Where \textit{contribute} means that if the sub-term changed it's type, then the overall term would\footnote{No matter which type it was changed to specifically.} also, or would become ill-typed. Importantly, we also consider \textit{contexts} for expression which are analysed, again considering any component terms which having their type changed would result in an error when trying to analyse against the type. For example in the following term:

\[(\lambda f: \code{Int} \to \dyn.\ f(1))(\lambda x : \code{Int}.\ x)\]
The terms which \textit{contribute} to the \textit{second}\footnote{Underlined below from now on.} lambda term checking successfully against $\code{Int} \to \dyn$ is everything \textit{except} the $x$ term, while context slice is just the annotation (and required structural constructs). Highlighting related to synthesis will be a darker shade:
\[\hlcmaths[yellow!30]{(\lambda} f\hlcmaths[yellow!30]{: \code{Int} \to \dyn}.\ f(1)\hlcmaths[yellow!30]{)}\underline{\hlcmaths[yellow!70]{(\lambda} x \hlcmaths[yellow!70]{: \code{Int}.}\ x\hlcmaths[yellow!70]{)}}\]

Notice that, in any typing derivation the only sub-terms which \textit{can} have their type changed without causing the rule to no longer apply are those which use type \textit{consistency}.\footnote{Note that this logic does \textit{not} extend to globally inferred languages.} The only such rule is \textit{subsumption}. Further, the only time a term could be changed to \textit{any} type and still remain valid is when it is checked for consistency with the dynamic type $\dyn$.

Therefore, this criterion just \textit{omits all dynamically annotated regions} of the program.\footnote{But does not omit the dynamic annotations themselves.} And, the contextual part of the slices are just \textit{analysis slices}.

\begin{definition}[Contribution Slices]\label{def:ContributionSlice}
For $\synthesis{e}{\tau}$ containing sub-derivation $\analysis[\Gamma']{e'}{\tau'}$ with checking context $\C$.

A \textit{contribution slice} of $e'$ is an \textit{analysis slice} for $e'$ in $\C$ paired with an expression typing slice $\varsigma^\gamma$ such that:
\begin{itemize}
\item $\varsigma$ is a slice of $e'$, that $\varsigma \sqsubseteq e'$.
\item Under restricted typing context $\gamma$, that $\varsigma$ checks against any $\tau'_2$ at least as precise as $\tau'$:\footnote{Essentially, sub-terms that check against $\dyn$ also synthesise $\dyn$. Defined this way to include the case of unannotated lambdas (which do not synthesise).}
\[\forall \tau'_2.\ \tau' \sqsubseteq \tau'_2 \implies \analysis[\gamma]{e'}{\tau'_2}\]
\end{itemize}
A contribution slice for a sub-term $e''$ involved in sub-derivation $\synthesis[\Gamma'']{e''}{\tau''}$ where $e'' \neq e'$ is an expression typing slice $\varsigma''^{\gamma''}$ which also synthesises $\tau''$ under $\gamma''$, that $\synthesis[\gamma'']{\varsigma''}{\tau''}$. Further, any sub-term of $e''$ which has a contribution slice of the above variety, is replaced inside $\varsigma$ by that corresponding expression typing slice.
\end{definition} 
This is most naturally calculated using \textit{type-indexed slices} exactly replicating the Hazel typing rules:

\textbf{Think more about type indexed context slices. Think how to reconstruct }
\newcommand{\s}{\mathdcal{s}}
\begin{figure}[h]
\[\inference[\tiny SConst]{}{\synthesissliceindexed{c}{b\mid c}} \quad
\inference[\tiny SVar]{x : \tau \in \Gamma}{\synthesissliceindexed{x}{\mathrm{map}{(x^{\{x:\tau\}}, \tau)}}}\]
\[ 
\inference[\tiny SFun]{s_1 = (\gap : \cmark)\circ \mathrm{annot}(\tau_1) &\synthesissliceindexed[\Gamma,x:\tau_1]{e}{s_2}\\ s_2 = i_2 \mid \varsigma^\gamma & x \match{\gamma} p}{\synthesissliceindexed{\lambda x:\tau_1.\ e}{(\lambda \cmark.\ \gap) \circ s_1 \to (\lambda p : \tau_1.\ \cmark) \circ s_2 \mid (\lambda p : \tau_1.\ \varsigma)^{\gamma\backslash x:\tau_1}}}\]
\textbf{TODO: fix SFun annotations for the argument slice, they need to remove unused requirements?}
\[\inference[\tiny SApp]{\synthesissliceindexed{e_1}{s_1} & s_1 \funmatch s_2 \to s \\ \analysissliceindexed{e_2}{s_2(\cmark)}{s_2'}}{\synthesissliceindexed{e_1(e_2)}{\text{app}(s_2')}}\]
 
\[\inference[\tiny SEHole]{}{\synthesissliceindexed{\hole^u}{\dyn \mid \hole^u}} \quad \inference[\tiny SNEHole]{\synthesissliceindexed{e}{\p\{i \mid \varsigma^\gamma\}}}{\synthesissliceindexed{\hole[e]^u}{\dyn \mid {\hole[\varsigma]^u}^\gamma}}\]
\[\quad 
\inference[\tiny SAsc]{c = \cmark : \text{annot}(\tau) & \analysissliceindexed{e}{\s}{s}}{\synthesis{e : \tau}{\mathrm{app}(s)}}\]

\[\inference[\tiny AFun]{\s \funmatch \s_1 \to \s_2\\ \analysissliceindexed[\Gamma, x:\type{\s_1}]{e}{\s_2 \circ (\lambda x.\ \cmark)}{s_2}}{\analysissliceindexed{\lambda x.e}{\s}{\s_1\{\lambda \gap. \gap\} \to s_2}}\] 
\[\inference[\tiny ASubsume]{\synthesissliceindexed{e}{s}\\ \type{s} \sim \type{\s}}{\analysissliceindexed{e}{\s}{\bigsqcup\mathrm{static}(\type{\s}, s)}}\]
\caption{Contribution Slices}
\label{fig:ContributionSliceRules}
\end{figure} 

Where $\mathrm{map}(\rho, \tau)$ creates a type-indexed slice of type $\tau$ with the slice context $\cmark$ and expression typing slice $\rho$ tagged on all components of $\tau$. Annot is as defined in criterion 1 extended to type-indexed slices. $x \match\gamma = p$ matches $p$ with $x$ if $x \in \text{dom}(\gamma)$, otherwise returns $\gap$. $\funmatch$ is extended to slices as follows:
\[\p\{s_1 \to s_2 \mid \rho\} \funmatch \p\circ s_1 \to \p \circ s_2\]
\[\p\{\dyn \mid \rho\} \funmatch \p\{\dyn \mid \rho\} \to \p\{\dyn \mid \rho\}\]
static replaces a (sub)slice term if it is in the position of a $\dyn$ on the input. Reconstructing the term is just taking the join down one level \textbf{(PROVE This)}. 

\subsection{Join Types}\label{sec:JoinTypesTheory}
The Hazel core calculus is very primitive, only consisting of \textit{base types}, \textit{annotations}, and \textit{functions}. Extensions to gradual types \cite{GradualJoins}, and Hazel \cite{MarkedLocalisation}\footnote{Here as the \textit{meet} of the opposite of my \textit{precision} order.}: \textit{if} expressions, \textit{pattern matching}, \textit{sum types} etc. all require\footnote{Or are easiest formulated with.} \textit{join types}. 

A join of two types $\tau_1 \sqcup \tau_2$ (if one exists) is the least precise (most general) type that more precise than both $\tau_1, \tau_2$: that $\tau_1 \sqsubseteq \tau_1 \sqcup \tau_2$ and $\tau_1 \sqsubseteq \tau_1 \sqcup \tau_2$. Therefore, the join is therefore is consistent with both $\tau_1, \tau_2$. Type consistency can be reformulated in terms of joins: $\tau_1, \tau_2$ are consistent \textit{if and only if} they have a join. This is the \textit{order-theoretic} \textbf{(cite)} join with respect to the precision partial order on types. 
For example, the type of an \textit{if statement} would be the join of the types of it's branches.

These add an additional way to generate a \textit{new type} other than by synthesis or from annotations. 

Hence, type flow needs to be extended to allow these. Equally, slices themselves can be joined if they have common contexts (though there is a decision whether to include both branches of a join).

\subsection{Type-Indexed Slicing Context}
\label{sec:TypeSliceContext}
It seems natural to extend the typing context to a type-indexed slicing context. Then, when accessing typing assumptions via variable references, the source information about the derivation for this type is revealed. But, the problem is, there is no context propagated, these slices are slices of the orignal term; propagating all this would be a big pain.

\section{Cast Slicing Theory}\label{sec:CastSlicingTheory}

Fairly trivial, just treat slices as types and decompose accordingly. The whole reason of indexing by type was to allow this.

The idea of it being a minimal expression typing slice producing the same cast doesn't really work here due to dynamics. Explore the maths of this. Either way, it is a useful construct in practice. Exploring this in more detail, looking at \textit{dynamic program slicing} could be a good future direction.

\textbf{Mention and compare with blame tracking}

\subsection{Indexing Slices by Types}


\subsection{Elaboration}
All casts inserted come from type checking, so can be sliced

\subsection{Dynamics}
Ground type casts will be added, but their addition is purely technical and we can treat their reasoning to just be extracting the relevant portion of the original non-ground type.

\subsection{Cast Dependence}
This in combination with the indexed slices could have some nice mathematical properties.

Though, these would need to retrieve information about parts of slices that were lost when decomposing slices. i.e. when a slice $\tau_1 \to \tau_2$ extracts the argument type $\tau_2$, the slicing criterions lose track of the original lambda binding. A way to reinsert these bindings such that we get a minimal term which \textit{Evaluates to the same cast} e.g. But this will have lots of technicalities with correctly tracking the restricted contexts $\gamma'$ for closures (i.e. functions returning functions which have been applied once). \textbf{TALK ABOUT THIS IN THE FURTHER DIRECTIONS SECTION.}

\section{Type Slicing Implementation}\label{sec:TypeSlicingImplementation}
Here I detail how the theories above were adapted to produce an implementation for Hazel.
\subsection{Hazel Terms}
\label{sec:HazelTerms}
Hazel represents its abstract syntax tree (AST) \textbf{(cite)} in a standard way by creating a mutually recursive algebraic data-type (\textbf{cite}). 

Terms are classified into similar groups as described in the calculus (see \cref{sec:HazelSyntax}), though combining external and internal expressions, and adding \textit{patterns}:
\begin{itemize}
\item \textbf{Expressions}: The primary encompassing term including: constructors\footnote{The basic language constructs: integers, lists, labelled tuples etc. Also, user-defined constructors via sum types.}, holes, operators, variables, let bindings, functions \& closures, type functions, type aliases, pattern match expressions, casts, explicit fix\footnote{Fixed point combinator for recursion.} expression.
\item \textbf{Types}: Unknown type\footnote{Either dynamic type, or a type hole.}, base types, function types, product types, record types, sum types, type variables, universal types, recursive types.
\item \textbf{Patterns}: Destructuring constructs for bindings, used in functions, match statements, and let bindings, including: Holes, variables (to bind values to), wildcard, constructors, annotations (enforcing type requirements on bound variables). 
\item \textbf{Closure Environments}: a closure mapping variables to their assigned values in it's syntactic context.
\end{itemize}

For example \cref{fig:tupletermstructure} shows a let binding expression whose binding is a tuple pattern (in blue) binding two variables \code{x}, \code{y} annotated with a type (in purple).

\begin{figure}[h]
\center\includegraphics[width=0.75\textwidth]{Media/Figures/tuple_term_structure}
\caption{Let binding a tuple with a type annotation.}
\label{fig:tupletermstructure}
\end{figure}

Every term (and sub-term) is annotated with an \textit{identifier} (ID, \code{Id.t}) in the AST which refers back to the syntax structure tree. In a sense, this is the equivalent of \textit{code locations}, but Hazel is edited via a structure editor. 
\subsection{Type Slice Data-Type}\label{sec:TypeSliceDataType}
I detail here \textit{how} and \textit{why} I implement type slices for Hazel.

\subsubsection{Expression slices as ASTs}
\textbf{Actually the below might be inaccurate, check the structure of the typing system to see if we would ever need to re traverse an AST?}

\textbf{Either way, there is opportunity to speak about persistence with how type slices are combined and decomposed and how they overlap. etc.}

Directly storing expression slices directly as ASTs is both \textit{space and time inefficient}. The AST type is a persistent data structures \cite[ch. 2]{PurelyFunctionalDataStructures}, meaning that any update to the tree will retain both the old and updated tree. All nodes on the path to the updated sub-tree must be copied; for example, \cref{fig:PersistentTrees} shows how a node G (in blue) can be sliced off from the tree while the old tree (in black) and the new tree (in red) both persist and share some unmodified nodes structurally. Expression slices do exactly this slicing operation extensively, so would require significant copying.
\begin{figure}[h]
\center\includegraphics[width=0.75\textwidth]{Media/Figures/persistenttree}
\caption{Persistent Tree}
\label{fig:PersistentTrees}
\end{figure}

\textbf{Maybe talk about a destructive version (always retain the original tree but from the perspective of various cursors}

There are ways to partially avoid this, for example the \textit{Zipper} data structure represents trees by from the perspective of a \textit{cursor} node rather than the \textit{root}. The part of the tree above the cursor is stored as an \textit{upside-down}. This allows the cursor to be 

We can derive an analogous \textit{one-hole context} type for the AST by \textit{differentiating} it's type \cite{OneHoleContext, TypeDerivatives}. 


\textbf{Zippers only useful if we need access to the parent node which cannot just be done by passing down info.}

However, we still have to copy nodes when shifting the cursor; during type-checking all nodes will be visited by the cursor so we would get at least a \textit{doubling} in space used. Additionally, converting a zipper back into a tree would take linear time\footnote{In the depth of the cursor.} and still require copying the path to the cursor as before.\footnote{Still advantageous as it delays the copying only for when the slice is actually \textit{used} in a way that requires this conversion. UI for slices would still work directly on the zipper structure without copying.} 

\begin{figure}[h]
\textbf{Produce Tikz Diagram Here.}
\caption{Tree Zipper}
\label{fig:PersistentZipper}
\end{figure}

However, the biggest issue is that expressions slices have \textit{multiple} gaps, so cannot be represented by a \textit{one}-hole context. Further, the slices vary in their number of holes, so generalising one-hole contexts to two-hole contexts etc. is not an option as each would require it's own distinct type. There are extensions to zippers allowing multiple holes which would work, \textit{multi-zippers} \cite{MultiZipper} for example, but has large constant overhead and would be very complicated to implement for such an extensive AST.

\subsubsection{Unstructured Code Slices}
With this in mind, given that the structure of expression slices does not actually matter for highlighting\footnote{Only matters for type checking slices, which always succeeds by design.}, I represent slices indirectly by these IDs in an \textit{unstructured} list, referred to now as a \textit{code slice} (\code{code_slice} type).

 Additionally, this has the side effect of allowing more \textit{granular} control over slices, as they now need not conform with the structure of expressions which is taken advantage of to reduce slice size, \textbf{(ref to section discussing this + evaluation)}.

Equally, the typing assumption slices are only required for formal type checking, however I maintain these is code slices to allow for different UI styling of slices originating from the use of the typing context (bold border).

\subsubsection{Type-Indexed Slices}
Cast slicing and contribution slices required \textit{type-indexed} slices. I therefore tag type constructors with slices recursively, i.e.:

\begin{figure}[h]
\begin{minted}[fontsize=\small]{reason}
type typslice_typ_term = 
  | Unknown
  | Arrow(slice_t, slice_t)  // Function type
  | ... // Type constructors
and typslice_term = (typslice_typ_term, code_slice)
and typslice_t = IdTagged.t(slice_term)
\end{minted}
\caption{Initial Type Slice Data-Type}
\end{figure}

However, this did not model the structure of type slices particularly well. Slices are generally incrementally constructed. Synthesis slices build upon slices of sub-terms and analysis slices , demonstrated in \cref{fig:IncrementalSlices}. 

The original data-type works well for synthesis slices as the list data-type is persistent and sub-slices are never modified. But not for analysis slices, which require an id to be added to \textit{all} sub-slices. Hence, analysis slices had \textit{quadratic} space complexity in the depth of the type (\textbf{Get numbers in evaluation to show it being worse if possible}).

\begin{figure}[h]
\begin{minted}{reason}
let f : Int -> Int = fun x -> x in f(0)
\end{minted}
Synthesis slice for f(0) is just constructed incrementally from the slices for 0 and f. 

Analysis slice for 0, f and all it's sub-slices all depend upon the annotation and binding etc.
\caption{Slicing is Incremental}
\label{fig:IncrementalSlices}
\end{figure}



\subsubsection{Incremental Slices}
Therefore, I explicitly represent slices incrementally, with two modes, for synthesis slice parts (termed \textit{incremental slice tags}) and analysis slice parts (termed \textit{global slice tags}).

I now tag each type constructor with incremental or global slices representing:
\begin{itemize}
\item Incremental Slice Tag: The slice of an expression is it's sub-slices adding it's incremental slice. But the sub-slices do not depend on the incremental slice.
\item Global Slice Tag: All the sub-slices of this term depend on this slice.
\end{itemize}
So instead of tagging an id in an analysis slice to all subslices, I tag it only to the constructor as a \textit{global slice}, and \textit{lazily} tag it to sub-slices upon usage (during type destructuring).

We get the following type:
\begin{figure}[h]
\begin{minted}[fontsize=\small]{reason}
type typslice_typ_term = 
  | Unknown
  | Arrow(slice_t, slice_t)  // Function type
  | ... // Type constructors
and typslice_term = 
  | SliceGlobal(slice_t, code_slice)
  | SliceIncr(slice_t, code_slice)
and typslice_t = IdTagged.t(typslice_term)
\end{minted}
\caption{Incremental Slice Data-Type}
\end{figure}

A key change when using this model is that when deconstructing types via function matching, list matching, etc. used throughout type checking and unboxing, the global slice must be pushed inside the resulting deconstructed types. This ensures that no part of the analysis slice context is lost \textbf{(compare to function matching in the type-indexed slice theory)}. 


\subsubsection{Usability \& Efficiency}
The type was further changed and many utility functions were added to enforce invariants and to ease development.

\paragraph{Empty Slices} 
Many type constructors have no associated incremental slice part (\textbf{example}), especially during evaluation. Equally, when type slicing is turned off. A \code{TypSlice(typslice_typ_term)} constructor is added to represent this case.

\paragraph{Fully Empty Slices} For the case when type slicing is turned off we also know that \textit{all} sub-slices are empty, and we get an isomorphism between slices and the original \textit{type}. For convenience in integrating with existing code, a fourth slice type \code{Typ(typ_term)} is added allowing a trivial and \textit{efficient}\footnote{Conversion to the empty slice type would instead take linear time.} injection from types to typeslices by tagging with \code{Typ}. Note that this means the empty slices \code{TypSlice} no longer needs to include atomic types.\footnote{These being equivalent to types, as no sub-slices exist.}

\paragraph{Slice Tag Duplicates}
The previous formulation allowed structures a type constructor with multiple global slices. For example: \begin{minted}[fontsize=\small]{reason}
SliceGlobal(SliceIncr(SliceGlobal(Arrow(...,...),...),...),...)\end{minted}
The possibility of any permutation of slices makes programming awkward when conceptually all we need is \textit{at most one} global and incremental slice tag. 
I enforce this invariant by refining the type to the actual type used in the final implementation:\footnote{Modulo some insignificant refactorings.}
\textbf{TODO: Refactor code to use empty type-slices}
\begin{minted}[fontsize=\small]{reason}
...
and typslice_empty_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
]
and typslice_incr_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
  | `SliceIncr(typslice_typ_term, code_slice)
]
and typslice_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
  | `SliceIncr(typslice_empty_term)
  | `SliceGlobal(typslice_incr_term, code_slice)
]
and typslice_t = IdTagged.t(typslice_term)
...
\end{minted}
\textit{Polymorphic Variants} \cite[ch. 7.4]{RealWorldOCaml}, notated \texttt{[ | ... ]} are used here for convenience in incrementally writing functions, explained for an example \textit{apply} function below. These are variants which exhibit \textit{row polymorphism} \cite{PolymorphicVariants} \cite[ch. 10.8]{ATTAPL} where these variants are related by a \textit{structural subtyping} relation \cite{StructuralSubtyping} where polymorphic variants of the same \textit{structure}, with constructors of the same name and types, are subtypes. 

We have that, \code{typslice_empty_term} is a subtype of\\ \code{typslice_incr_term} which is a subtype of \code{typslice_term}. All other type constructors are either co-variant or contra-variant \cite[ch. 2]{BasicCatTheory} with respect to the subtyping relation. For example, id tagging is covariant\footnote{It is just a labelled pair type.}, so an\\ \code{IdTagged.t(typslice_incr_term)} is a subtype of\\ \code{IdTagged.t(typslice_incr_term) = typslice_t}. 

Equally, a function of type\\ 
$\code{typslice_incr_term} \to \code{typslice_incr_term}$ is a subtype of $\code{typslice_empty_term} \to \code{typslice_term}$, being contravariant in it's argument and covariant in it's result. This function subtyping property significantly reduces work in defining functions on this type as seen below.
\paragraph{Utility Functions}
Functions on slices often do not concern the slices, but only the structure of it's underlying type, for example in unboxing\footnote{e.g. checking if a slice is a \textit{list} type.} (\textbf{ref to sec}). In which case it is more convenient to just write a $\code{typ_term} \to 'a$ and $\code{typslice_term} \to 'a$ function directly on the possible empty slice types. 

An \code{apply} function is provided to apply this onto the slice term. See how the bottom two branches can both be passed into the \code{apply} function even though they have different types (but are both subtypes of \code{typslice_term}).  
\begin{minted}[fontsize=\small]{reason}
let rec apply = (f_typ, f_slc, s) =>
  switch (s) {
  | `Typ(ty) => f_typ(ty)
  | `TypSlice(slc) => f_slc(slc)
  | `SliceIncr(s, _) => apply(f_typ, f_slc, s)
  | `SliceGlobal(s, _) => apply(f_typ, f_slc, s)
  }
\end{minted}
Similarly, mapping function which map a $\code{typ_term} \to \code{typ_term}$ and similar functions onto slices are provided while maintaining the slice tags. Another particularly useful one is a mapping function that maps $\code{typ_term} \to \code{typslice_term}$ etc. and merges the slices around the input term and the output slice of the mapped function. Other utility functions include wrapping functions, unpacking functions, matching functions etc.

\subsubsection{Type Slice Joins}
Type joins are extensively used in the Hazel implementation for \textit{branching statements}. The type of a branching statement is the \textit{least specific} type which is still \textit{at least as specific} as all the branches. This corresponds to the \textit{lattice join} of the types of the branches with respect to the precision relation. 

Previously in \cref{sec:JoinTypesTheory}, I stated how slices could be joined. To implement this, first any contextual code slices required to place the branches within a common context are wrapped onto the branch slices. Then the slices are joined, unioning the incremental code slices of branches.

For basic synthesis and analysis slices, I decide to take the code slices for each atomic type in the joined type from only \textit{one} branch. This retains a complete explanation of \textit{why} the joined type is synthesised/checked, but does \textit{not} constitute a valid contribution slice.

This slicing is not as easy as just taking the slice of a single branch, as the most specific sub-parts of the joined type may come from differing branches. For example in \cref{fig:TypeSliceJoinExample}, the \textit{then} branch has a type $\code{Int} \to (\dyn,\code{Int})$ and the \textit{else} branch has type $\code{Int} \to (\code{Int}, \dyn)$ meaning the joined type is $\code{Int} \to (\code{Int}, \code{Int})$. We can omit one\footnote{The figure, and my implementation, omits the left branches.} of the redundant annotations on the argument but still must retain a slice of the 0 term in both branches to get the \code{(Int, Int)} slice.
\begin{figure}[h]
\textit{if ? then fun x : Int -> (?, 0) else fun x : Int -> (0, ?)}
\caption{Type Slice Join}
\label{fig:TypeSliceJoinExample}
\end{figure}

The unstructured nature of code slices also allows the type constructors to select only \textit{one} branch to take the slice from. The given figure would \textit{not} highlight the function constructor in the \textit{then} branch. However, this is not be a valid expression slice in the theoretic sense and could be confusing for the user \textbf{(discuss in evaluation, missing info)}.

\subsubsection{Contribution Slices}
\label{sec:ContributionSliceImplementation}
To create a \textit{contribution slice} (definition and correctness reasoning in \cref{sec:ContributionSlices}) we will need to combine analysis and synthesis slices on the same term that:
\begin{itemize}
\item Matches compound type constructor slice tags are combined.
\item Atomic \textit{dynamic} type parts of the \textit{analysis slice} are retained in preference to the synthesis slice part. 
\item Other atomic type constructors have slices combined.
\end{itemize} 
We get a type slice whose type is equivalent to the analysis slice, but includes relevant parts of the synthesis slice.
\begin{figure}[h]
\textbf{Use same example here as in theory}
\caption{Contribution Slice}
\label{fig:ContributionSliceExample}
\end{figure}

\subsubsection{Weak Head Normalisation}
Describe where this is used and how slices do this. This subsection could be elided.

\subsection{Static Type Checking}\label{sec:TypeChecking}
The Hazel implementation is \textit{bidirectionally typed}. During type checking, the typing \textit{mode} in which to check the term is specified with \code{Mode.t} type: \textit{synthesising}\footnote{Additionally split into synthesising functions and type functions, but this detail is elided here.} (\code{Syn}) or \textit{analysing} (\code{Ana(Typ.t)}).

The type checker associates each term with a type information object \code{Info.t}, stored in a map by term id with efficient access. The \textit{type information} stores the following info:
\begin{itemize}
\item The term itself and it's ancestors.
\item \textbf{Mode}: Typing expectations enforced by the context.
\item \textbf{Self}: Information derived independent from the \textit{mode}, e.g. synthesised type, or type errors arising from inconsistent branch types, syntax errors.
\item \textbf{Typing context and co-context}. A co-context
\item Status: Is it an error, what type of error? For example, inconsistency between type expectations and synthesised/actual type.
\item \textbf{Type}: The \textit{actual} type of the expression after \textit{accounting for errors}. Errors are placed in holes, so synthesise the dynamic type. 
\item Constraints: Patterns also store constraints to determine redundant branches and inexhaustive match statements, in the sense of \cite[ch. 13]{PracticalFoundationsEd1}\footnote{Only present in the \textit{first} edition.}. Hazel-specific details in \cite{LivePatternMatching}.
\end{itemize}

As suggested by my slicing theory, I augment the four bolded fields to refer to type slices, returning type slices from synthesis and analysing directly against type slices. This results in wide-changing code, but I only detail the general workings here.

\subsubsection{Self}
The \textit{self} data-structure now returns \textit{types slices} instead of \textit{types}. Every expression construct which can be synthesised has a corresponding function in \code{Self.re} to construct the slice from it's sub-derivations (slices of synthesised types of sub-expressions). Hence, the synthesis slicing behaviour for each type of expression can be easily configured uniformly via editing these functions.

For example, the slice of a pattern matching statement, given slices of all it's branches (\code{tys}) is the join of it's branches wrapped in an incremental slice consisting of the ids of the match statement itself. Otherwise if the branches are inconsistent it returns a failure tagging the branch slices with ids of the branches:
\begin{figure}[h]
\begin{minted}[fontsize=\footnotesize]{reason}
let of_match =
    (ids: list(Id.t), ctx: Ctx.t, tys: list(TypSlice.t), 
    c_ids: list(Id.t)): t =>
  switch (
    TypSlice.join_all(
      ~empty=`Typ(Unknown(Internal)) |> TypSlice.fresh,
      ctx,
      tys,
    )
  ) {
  | None => NoJoin(Id, add_source(c_ids, tys))
  | Some(ty) => Just(ty |> TypSlice.(wrap_incr(slice_of_ids(ids))))
  };
\end{minted}
\caption{Match Statement \code{Self.t}}
\end{figure}

This stage also included factoring out some expectation-independent code from the type checking function which had been missed by others.

\subsubsection{Mode}
The \textit{mode} now analyses against \textit{type slices} instead of \textit{types}. Again, each construct which could deconstruct an analysing type has a corresponding function in \code{Mode.re} which outputs the mode(s) to check the inner expressions. The inner analysis slices are tagged with a \textit{global}\footnote{Being part of the analysis slice, relevant to all sub-slices.} slice tag describing \textit{why} the slice was deconstructed. As mentioned before \textbf{(ref)}, deconstructing types retains the contextual (global) parts of the analysis slice.

For example, I can deconstruct a list slice with a list matching function \code{matched_list}. Using this, the mode to check a term inside a \textit{list literal} is the matched inner list slice wrapped (globally) in the ids of list literal itself (\code{ids}) which enforced this matching. We use this mode to check each element in the list literal.

\begin{figure}
\begin{minted}[fontsize=\footnotesize]{reason}
let of_list = (ids: list(Id.t), ctx: Ctx.t, mode: t): t =>
  switch (mode) {
  | Syn
  | SynFun
  | SynTypFun => Syn
  | Ana(ty) =>
    Ana(TypSlice.(matched_list(ctx, ty) 
    |> wrap_global(slice_of_ids(ids))))
  };
\end{minted}
\caption{List Literal \code{Mode.t}}
\end{figure}

\subsubsection{Typing (Co-)Context}
The typing context and co-contexts are modified to use type slices, given that we now always have a slice to accompany a type in any situation. 

\Cref{sec:TypeSliceContext} discusses one major consequence of allowing this. Slices for variables may now include the slice binding it's type. 

This \textit{deviates} from the theoretical notion of an expression slice: the structural context in which the variable is used is untracked when passing through the context. But, it is easy to implement using \textit{unstructured} code slices and is a useful addition conceptually (\textbf{discuss in evaluation}).

\subsubsection{Type}
The \textit{type} field is extended to a \textit{type slice}. This has special behaviour for contribution slices and errors.
\paragraph{Basic Slices} 
If there are no errors, just use the analysed type slice, or if in synthesis mode use the synthesis slice. 
\paragraph{Contribution Slices: IMPL TODO}
When synthesis and analysis slices exist, they can be combined here. \Cref{sec:ContributionSliceImplementation} defines and explains this combination.

\paragraph{Error Slices: IMPL TODO}
Type inconsistency error checking can be extended to type slices via using type slice joins. The analysed and synthesised type slices are inconsistent if and only if a join exists. We can show both conflicting slices in this situation to explain the error \textbf{(Implement)}. Additionally, syntax errors could have a slicing mechanism implemented here.\footnote{Not within the scope of this project. Empty slices are given instead.}

\subsection{Sum Types}
\textbf{Sum types are the hardest to work with, describe general design and difficulties in all the previous parts.}

\subsection{User Interface}
Click on analysis or synthesis slices from context inspector

Show figures.

\textbf{Implement UI from cursor inspector. Implement disabling of slicing.}

\section{Cast Slicing Implementation}\label{sec:CastSlicingImplementation}
To implement cast slicing, I replace casts between \textit{types} by casts between \textit{type slices}. The required type-indexed nature of type slices is already implemented, allowing these casts the be decomposed.

\subsection{Elaboration}\label{sec:Elaboration}
Cast insertion recursively traverses the unelaborated term, inserting casts to the term's statically determined type as stored in the \code{Info} data-structure and from the type as can be determined directly from the term. 

For example, for list literals we can recursively elaborate the list's terms and join their static slices into \code{inner_type}. Then, intuitively we would know during dynamics from these elaborated sub-terms that the list has a list type of \code{List(inner_type)}:
\begin{figure}[h]
Maybe a more diagrammatic option here
\caption{List Literal Elaboration}
\end{figure}

We can therefore construct the source type slices in these casts directly form the term during this traversal. \textbf{(TODO impl for atomic types like ints)}

Ensuring that all the type slice information from the \code{Info} map is retained and/or reconstructed during elaboration was a meticulous and error-prone process.

\subsection{Cast Transitions}
\Cref{sec:HazelDynamics} gave an intuitive overview of how casts are treated at runtime. Type-indexed slices allows cast slices to be decomposed in exactly the same way. 

However, as Hazel only checks consistency between casts between \textit{ground types} (\cref{fig:groundtypes}), there are two rules where new\footnote{As opposed to being derived from decomposition.} casts are \textit{inserted} ITGround, ITExpand (\cref{fig:instructions}). The new types are both created via a \textit{ground matching} relation (\cref{fig:groundmatch}) which takes the topmost compound constructor. 

As we already store type slices incrementally, the part of the slice which corresponds \textit{only} to the outer type constructor is just the outer slice tag.

\begin{figure}
list example
\caption{Ground Matching List}
\end{figure}

\subsection{Unboxing}
When a final form (\cref{sec:HazelFinalForms}) has a type, Hazel often needs to extract parts according to this type during evaluation. But due to casts and holes, this is not trivial.

For example, if a term is a final form of type list, then it could be either:
\begin{itemize}
\item A list literal.
\item A list with casts wrapped around it.
\item A list cons with indeterminate tail, e.g. \code{1::2::?}.
\end{itemize}
Additionally, when the input is not a list at all, it can return \code{DoesNotMatch}. Hence, allowing dynamic errors to be caught and also for use in pattern matching. 

To allow for the varying outputs to unboxing depending on different patterns to match by, GADTs are used \textbf{(cite and explain)}.

Various helper functions for unpacking type slices into it's sub-slices to significantly simplify pattern matching.\footnote{These differ from matching functions, which also match the dynamic type to functions, lists etc.} A uniform function for this could be implemented with a GADT, in a similar way to unboxing, but is not required.

\subsubsection{Hazel Unboxing Bug}
While writing the search procedure I found an unboxing \textit{bug} which would always \textit{indeterminately match} a cons with indeterminate tail with \textit{any} list literal pattern (of \textit{any} length), even when it is known that it could never match. For example a list cons \code{1::2::?} represents lists with length $\geq 2$, but even when matching a list literal of length 0 or 1 it would indeterminately match rather than explicitly \textit{not} match. 

Pattern matching checks if each pattern matches the scrutinee with the following behaviour, starting from the first branch:
\begin{itemize}
\item \textit{Branch matches?} Execute the branch.
\item \textit{Branch does not match?} Try the next branch.
\item \textit{Branch indeterminately matches?} Hazel cannot assume the branch doesn't match so cannot move on and must safely stop evaluation here classifying the entire match as an indeterminate term.
\end{itemize}
\Cref{fig:PatternMatchingBug} demonstrates a concrete example which would get stuck in Hazel, but does \textit{not} need to.

\begin{figure}[h]
See PR example
\caption{Pattern Matching Bug}
\label{fig:PatternMatchingBug}
\end{figure}

I reported and fixed this bug and added additional tests to ensure the bug never reappears. A PR was merged into the dev branch \textbf{(pending)}.
\subsection{User Interface}
Mainly talk about the Model-view architecture and passing the cursor into the evaluator view to allow clicking on casts in evaluation result/stepper.

Difficulties in ensuring id tags for slices are not subtly aliased during elaboration and evaluation. This caused problems in selecting slices with UI. Look through commits to give example

\section{EV\_MODE Evaluation Abstraction}
This section describes in detail the evaluator abstraction present in hazel, which allows ...

Very complex!!! The reader could skip this section, it is purely technical.

\section{Indeterminate Evaluation}\label{sec:IndetEval}
Dynamic type errors may only occur when an expression is given \textit{specific} inputs. However, a dynamic error is accompanied by an evaluation trace, which is often a \textit{useful debugging aid} \cite{TraceVisualisation}. When debugging static type errors, traces leading to a corresponding dynamic error are normally \textit{unavailable}.\footnote{As an ill-typed program will not run.} This section concerns the building blocks leading up to a search procedure that finds inputs which lead to dynamic errors \textit{automatically}.

Seidel et al. \cite{SearchProc} provides an algorithm for this in OCaml and provides evidence for the usefulness of traces via a \textit{user study}. This algorithm \textit{lazily} narrows hole terms non-deterministically to a \textit{least specific} value based on it's expected type in the context it was used. For example, if a hole is used within the \code{(+)} operator, it is non-deterministically instantiated to an integer. \textbf{(give better example with lists)}

In Hazel, we already have a notion of hole terms and can already run program with static errors, with runtime type information being maintained by runtime casts. This section introduces \textit{indeterminate evaluation} as a natural analogue to Seidel's idea: to \textit{lazily} narrow holes during evaluation to \textit{least specific} values by exploiting the runtime type information available within Hazel's runtime casts. My implementation extends Seidel's to consider more classes of expressions\footnote{Notably, sum types.} and differs mechanically in many ways due to language differences and fundamental design differences.\footnote{Differences, and Hazel-specific challenges are noted throughout. \textbf{(ENSURE THIS!)}} Notably, indeterminate evaluation is a \textit{generic} evaluation method, not specifically relating only to searching for cast errors, which is covered in \cref{sec:SearchProcedure}.

This section covers the following, answering each question:
\begin{enumerate}
\item[\ref{sec:ResolvingNondeterminism}] First, how should we resolve the non-determinism in instantiating holes? Unlike Seidel's approach, my implementation exhaustively considers all possibilities -- at least, for countable types.
\item[\ref{sec:IndetEvalAlgorithm}] Second, I give a generic algorithm for indeterminate evaluation. How is termination ensured? Is every possibility explored fairly? How can we abstract details of evaluation\footnote{Differing evaluation methods, e.g. one giving up after some limit to avoid non-termination.} and which classes of expressions\footnote{e.g. those with cast errors.} to consider?
\item[\ref{sec:HoleInstantiation}] Third I discuss hole instantiation and substitution. What does lazy instantiation actually entail, when exactly should a hole be instantiated? Which hole\footnote{There may be multiple.} should be instantiated in order to continue evaluation to make progress? How should holes be substituted with their narrowed values; the same hole may exist in multiple locations within the expression? 
\item[{\parbox[t]{1\linewidth}{\raggedleft \ref{sec:CastLaziness}\\ \& \\\ref{sec:PatternMatching}}}] {\parbox[t]{1\linewidth}{Finally, I consider Hazel-specific problems. Once we know which hole to instantiate, how can we get it's \textit{expected type}? Hazel's lazy treatment of pushing casts into compound data types means not all such holes will be wrapped directly in casts. Additionally, the case of pattern matching is difficult, allowing holes to be \textit{non-uniformly} cast to \textit{differing types}. How can holes be instantiated in these situations?}}
\end{enumerate}
As always, a UI is implemented in \cref{sec:UIIndetEval}.

\subsection{Resolving Non-determinism}
\label{sec:ResolvingNondeterminism}
To model non-determinism I decide to use a \textit{monadic} high-level  representation. I choose this due to it's uniform and familiar workings for other developers in the codebase. Additionally, the Jane Street \code{Base} module (\textbf{CITE}) is industry-tested and contains a flexible lazy list/sequences module \code{Sequence}.

\Cref{sec:Nondeterminism} considered multiple ways to represent non-determinism, I did not chose the other options as: multiple continuation effect handlers were not supported by JSOO \textbf{(cite)}, directly writing continuations is difficult and generally unfamiliar to OCaml developers \textbf{(cite)}, introducing a DSL \cite{NondetDSL} would introduce additional dependencies, is less industry-tested, and is non-standard.

Sequences may model infinite non-determinism completely by way of \textit{interleaving} sequences. Letting each element in the sequence correspond to a possible choice, then two sequences of choices can be interleaved to get a new sequences of all choices. As this is done \textit{lazily}, then no calculation is actually performed until the first element is \textit{accessed.}

The \code{Sequence.t} data-type is a lazy (possible infinite) sequence. The relevant non-determinism related operations here as specified in \cref{sec:Nondeterminism} are:
\begin{itemize}
\item \code{empty}, corresponding to \texttt{fail}. Represents no solution, no choices are possible.
\item \code{append} or \code{(++)}, corresponding to \texttt{choice}. In this model, every possible choice will be retained in full in the sequence.
\item \code{interleave}, corresponding to \textit{fair choice}. The ordering of the choices will be interleaved in some way, with elements from each sequence still occurring in the same order. If we assume \textit{commutativity} of choice,\footnote{Of course, append is \textit{not commutative}. What I mean here is that any \textit{sequence ordering} could be considered as an equally correct solution to a non-deterministic algorithm.} then interleaving gives the same results as \texttt{choice}. Additionally, interleaving supports choice between an infinite sequences of sequences.
\item \code{bind} or \code{(>>=)}, corresponding to \texttt{bind}. Intuitively represents guessing
\item \code{singleton}, corresponding to \texttt{return}. Represents the identity... \textit{explain...}
\end{itemize}

Additionally, to define sequences lazily in OCaml (which is strict), an \code{unfold} function is defined. This wraps sequence tails in closures

\subsection{The Non-Deterministic Evaluation Algorithm}
\label{sec:IndetEvalAlgorithm}
\textbf{Factor out the evaluation code, to be replaced by evaluation which fails if there is no cast error, or evaluation which iteratively deepens etc..}

Basically a DFS stuff


\subsection{Threading Evaluation State}
\textbf{TODO IMPL, maybe unneeded. Add to introduction if needed}

(Probably) Cannot use evaluator state for tracking trace lengths etc. because it is mutable (and IndetEval is non-deterministic). Instead thread through evaluation :(

\subsection{Hole Instantiation \& Substitution}\label{sec:HoleInstantiation}
Small Hole hypothesis, quick check


\subsubsection{Choosing which Hole to Instantiate}
Use EV\_Mode to select next hole to instantiate

\subsubsection{Synthesising Terms for Types}
Difficulties instantiating strings...
Functions with or without annotations?

Seems that there was actually a refine hole editor action that I didn't notice?

\subsubsection{Substituting Holes}\label{sec:HoleSubstitutionImplementation}
Detail that this was an unexpected extra task, and is therefore not exactly the same as hole substitution as detailed in Preparation (i.e. no metavars or contexts annotated on holes, but it is enough for the search procedure to work)

\subsection{Cast Laziness}\label{sec:CastLaziness}
\textbf{Is this actually a problem?}

Ref the original cast slicing paper, which is not lazy apparently? Laziness sort of breaks the idea that runtime errors evaluate to cast errors. There can be compound values of the wrong type being cast, but the error will only be found upon accessing parts of the compound type.

Making casts eager is a major change to the actual transitions.

Eager casts also catch `spurious' errors (see Evaluation).


\subsection{Pattern Matching}
\label{sec:PatternMatching}
Hazel match expressions can be dynamic, allowing scrutinees to be of varying type:

\texttt{case ? | 0 => 0 | [] => 1 end} etc.

This means that casts are placed on the \textit{branches} rather than the scrutinee, meaning I need a special case for instantiating match scrutinees on a per-branch-type basis. This can be done by listing the possible types for the branches, then inserting a cast $\scastcast{\dyn}{T}{\dyn}$ on the scrutinee hole. Then re-running instantiation (which will use this inner cast to instantiate.

There was an \textit{unboxing} bug which was discovered here.

\subsubsection{Extended Match Expression Instantiation}
One extension I attempted in order to improve code coverage was instantiating holes to fairly explore branches in a match statement (as opposed to just going by increasing length list etc.). 

The \textit{pattern-matching-instantiation} branch implements a way to instantiate sub-parts of an indet term such that the new term can be deconstructed by the the pattern in the most general way. \textbf{show example}

\textbf{(note: bug was found here with matching 1::?)}

Hence, we could try instantiating the match scrutinee with least specific versions which match the patterns on each branch, e.g. \code{?::?} for \code{x::xs}. i.e. 

\texttt{case ?::? | [] => [] | x::xs => xs}

However, this is not always enough to actually \textit{allow} deconstruction in a match statement. The possibility that \textit{more specific} patterns could be present above the current branch means this term is still an indeterminate match. For example, the following would be an indeterminate match:

\texttt{case ?::? | [] => [] | x::y::[] => [] | x::xs => xs}

To solve this\footnote{Also, having the side effect of allowing the representation of inequalities symbolically, i.e. \textit{not 1} $\overline{1}$. This is useful for symbolic execution of if statements.}, I introduce the idea of a \textit{not patterns}, and \textit{or patterns}, \textit{and patterns}, \textit{truth patterns}, and \textit{impossible patterns}. This is just a theory to transform pattern matching into equivalents with \textit{no overlapping branches}, these patterns do not actually need to be implemented directly, except for not pattern instantiation on base types (though \textit{or patterns} would be useful).

\[\overline{\overline{p}} = p \qquad \overline{[\ ]} = \top::\top \qquad \overline{p :: q} = [\ ] \mid \overline{p}::q \mid p::\overline{q} \mid \overline{p} :: \overline{q}\]
\[\overline{\top} = \bot \qquad \bot :: p = q :: \bot = \bot \qquad p \mid \bot = p \qquad x = \top\]
\[ p \mid \overline{p} = \top \qquad p :: q \mid \overline{p} :: q = \top ::q \qquad p :: q \mid p :: \overline{q} = p::\top\]
Therefore: 
{\tiny
\begin{align*}
\overline{[1,2]} = \overline{1::2::[]} = &[] \mid \overline{1} :: 2::[] \mid 1 :: \overline{2 :: []} \mid \overline{1} :: \overline{2::[]}\\
= &[] \mid \overline{1} :: 2::[] \mid 1 :: ([] \mid \overline{2} :: [] \mid 2 :: \overline{[]} \mid \overline{2} :: \overline{[]}) \\
&\mid \overline{1} :: ([] \mid \overline{2} :: [] \mid 2 :: \overline{[]} \mid \overline{2} :: \overline{[]})\\
= &[] \mid \overline{1} :: 2::[] \mid 1 :: [] \mid 1 :: \overline{2} :: [] \mid 1::2 :: \overline{[]} \mid 1::\overline{2} :: \overline{[]}\\
&\mid \overline{1} :: [] \mid \overline{1} ::\overline{2} :: [] \mid \overline{1} ::2 :: \overline{[]} \mid \overline{1} ::\overline{2} :: \overline{[]}\\
= &[] \mid \overline{1} :: 2::[] \mid 1 :: [] \mid 1 :: \overline{2} :: [] \mid 1::2 :: \top::\top\\
&\mid 1::\overline{2} :: \top::\top \mid \overline{1} :: [] \mid \overline{1} ::\overline{2} :: [] \mid \overline{1} ::2 :: \top::\top \mid \overline{1} ::\overline{2} :: \top :: \top\\
= &[] \mid \top::[] \mid 1::\overline{2}::[] \mid \overline{1}::2::[] \mid \overline{1}::\overline{2}::[] \mid \top::\top::\top::\top
\end{align*}
}
Basically, a boolean algebra... To ensure no overlapping branches we must disallow the Idempotent law in one direction ($p \to p \mid p$)...

Very closely related to \textit{exhaustiveness checking}. Difference being is we don't allow idempotence. \textbf{See {https://github.com/hazelgrove/hazel/issues/1127}}

\textbf{Note: Or and As patterns are pending unimplemented tasks for Hazel}

\subsection{User Interface}\label{sec:UIIndetEval}


\section{Evaluation Stepper}\label{sec:Stepper}
Stepper and user defined instantiations. \textbf{All TODO IMPL}
\subsection{Evaluation Contexts}
\subsection{Customisable Hole Instantiation}
\subsection{User Interface}


\section{Search Procedure}\label{sec:SearchProcedure}
\subsection{Detecting Relevant Cast Errors}
i.e. failed cast at head of term
\subsection{Filtering Indeterminate Evaluation}
Done via EV\_MODE similarly to finding which hole to instantiate.

\subsection{Monad Transformers \& Iterative Deepening}\label{sec:IterativeDeepening}
Required after evaluating that infinite loops break the thing

Use a monad transformer! https://okmij.org/ftp/Computation/LogicT.pdf
\subsection{User Interface}


