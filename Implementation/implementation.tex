\chapter{Implementation}\label{chap:Implementation}
This project was conducted in two major phases:

First, I constructed a core mathematical theory for \textit{type slicing} and \textit{cast slicing} formalising what these ideas actually were and considered the changes to the system presented by Seidel et al. for the \textit{type error witnesses search procedure} to work in Hazel.  

Then, I implemented the theories, making it suitable for implementation and extending it to the majority of the Hazel language. Further, suitable deviations from the theory were made upon critical evaluation and are detailed throughout.

\textbf{Annotate the above with the relevant section links!}
\section{Type Slicing Theory}\label{sec:TypeSlicingTheory}
\textbf{Replace all occurrences of `typing context' with `typing assumptions' to avoid name clash with expression/program contexts.}

I develop \textit{type slicing} as a mechanism to aid programmers in understanding \textit{why} a term has a given type via static means. Three slicing mechanism have been devised with differing characteristics, all of which associate terms with their typing derivation to produce a \textit{program slice}. 

The first two criteria attempt to give insight on the structure of the typing derivations, and hence how types are decided. While the third criterion gives a complete picture of the regions of code which, if changed, could cause a change in type of the whole expression.

\textbf{Make some brief arguments into why the first two criteria are still useful.}

\textbf{PLACE ALL IMPORTANT DEFINITIONS INSIDE DEFINITION ENVIRONMENTS}

\subsection{Program Slices}
A \textit{program slice} $\rho$, is a pair $\varsigma^\gamma$, consisting of an \textit{expression slice} $\varsigma$ and \textit{typing context slice} $\gamma$ which are calculated based on some typing \textit{criterion}\footnote{One of the three slicing mechanisms.} based on the typability of the slice $\varsigma$ under context $\gamma$. 

Intuitively, an expression slice is a Hazel external expression highlighting the sub-terms of relevance to the \textit{typing criterion}. For example if my criterion is to \textit{omit terms which are typed as} \code{Int}, then the following expressions highlights as:

\[\hlcmaths[yellow!30]{(\lambda x: \code{Int}.\ \lambda y : \code{Bool}.}\ x\hlcmaths[yellow!30]{)(}1\hlcmaths[yellow!30])}\]

Formally, I represent this by specifying which sub-terms are omitted in the highlighted expression. So, Replace each omitted sub-term with a \textit{gap}, notated $\gap$. This is the same definition of a slice as presented in \cite{FunctionalProgExplain}.\footnote{With their `holes' equating with my `gaps'. Different terminology used to distinguish with Hazel's holes} i.e. representing the above highlighting we get slice:
\[(\lambda x : \code{Int}.\ \lambda y : \code{Bool}.\ \gap)(\gap)\]


Additionally, it is useful to omit variable names. For this I introduce \textit{patterns} $p$ for variable bindings: 
\[p ::= \gap \mid x\]

This gives the following extended syntax of expression slices, $\varsigma$, extending \cref{fig:syntax}:
\[\varsigma ::= \gap \mid  c \mid x \mid \lambda p : \upsilon.\ \varsigma \mid \lambda x.\ \varsigma \mid \varsigma(\varsigma) \mid \hole^u \mid \hole[\varsigma]^u \mid \varsigma : \upsilon\]
Where $\upsilon$ are types, similarly with potential omitted sub-term gaps:
\[\upsilon ::= \gap \mid \dyn \mid b \mid \upsilon \to \upsilon\]
These slices are then allowed to be \textit{typed} by representing gaps $\gap$ by holes of fresh metavariables $\hole^u$ in \textit{expressions}, fresh variables in \textit{patterns}, and the dynamic type in \textit{types}, see \textbf{(fig APPENDIX)}. From here-on consider $\gap$ as interchangeable with a hole $\hole^u$ of fresh metavariable $u$ or the dynamic type.

We then have a \textit{precision} relation on expression slices, $\varsigma_1 \sqsubseteq \varsigma_2$ meaning $\varsigma_1$ is less or equally precise than $\varsigma_2$, that is $\varsigma_1$ matches $\varsigma_2$ structurally except that some subterms may be gaps, see \textbf{ref appendix}. For example, see this precision chain:
\[\gap \sqsubseteq\gap + \gap\sqsubseteq 1 + \gap \sqsubseteq 1 + 2\]
We have that $\sqsubseteq$ is a partial order (\textbf{cite}), that is, satisfies relexivity, antisymmetry, and transitivity. Respectively:
\[\inference{}{\varsigma \sqsubseteq \varsigma} \quad \inference{\varsigma_1 \sqsubseteq \varsigma_2 & \varsigma_2 \sqsubseteq \varsigma_1}{\varsigma_1 = \varsigma_2} \quad \inference{\varsigma_1 \sqsubseteq \varsigma_2 & \varsigma_2 \sqsubseteq \varsigma_3}{\varsigma_1 \sqsubseteq \varsigma_3}\]
We also have a \textit{bottom} (least) element, $\gap \sqsubseteq \varsigma$ (for all $\varsigma$). This relation is trivially extended to include complete expressions $e$ which satisfy that: if $e \sqsubseteq \varsigma$ then $e = \varsigma$, i.e. complete terms are always \textit{unique} upper bounds of precision chains.

An expression slice $\varsigma$ \textit{of} $e$ is a slice such that $e \sqsubseteq e$.

\textit{Typing context slices} are simply a typing context $\Gamma$, which is used to represent the notion of \textit{relevant typing assumption}. Typing contexts are just functions mapping variables to types notated $x : \tau$ (see \cref{sec:TypingJudgements}). Functions are sets, so they also have a partial order of subset inclusion, $\subseteq$. Again, we have a bottom element, $\emptyset$.  These are notated $\gamma$ and if $\gamma \subseteq \Gamma$ then $\gamma$ is a slice \textit{of} $\Gamma$.

The precision relation and subset inclusion can be extended pointwise to give a partial order, $\sqsubseteq$, on program slices:
\[[\varsigma_1\mid \gamma_1] \sqsubseteq [\varsigma_2\mid \gamma_2] \iff  \varsigma_1 \sqsubseteq \varsigma_2 \text{ and } \gamma_1 \subseteq \gamma_2\]

Program slices will often be grouped and indexed upon expressions and typing contexts, $P_e^{\Gamma}$ which contains all slices $\rho \sqsubseteq (e, \Gamma)$. So, the set $P_e^{\Gamma}$ forms a lattice (\textbf{cite}) with unique least upper bound $[e, \Gamma]$ and greatest lower bound $[\gap, \emptyset]$. Similarly, an element $\rho$ in $P_e^{\Gamma}$ can referred to as a program slice \textit{of} $e$ under $\Gamma$.
\subsection{Program Context Slices}
\textbf{add pattern slices. }

\newcommand{\C}{\mathcal{C}}
Formally, an \textit{expression context} $\mathcal{C}$ is an expressions with \textit{exactly one} sub-term marked as $\mark$:\footnote{The two separate syntax definition for application allow a \textit{mark} to be in either the left or right expression, but \textit{not both}.}
\[\C ::=  \mark \mid \lambda x : \tau.\ \C \mid \lambda x.\ \C \mid \C(e) \mid e(\C) \mid \C : \tau\]

Where $\C\{e\}$ substitutes expression $e$ for the mark $\mark$ in $\C$, the result of this is necessarily an expression. Additionally, contexts are composable: substituting a context into a context, $\C_1\{\C_2\}$ produces another valid context, notate this by $\C_1 \circ \C_2$\footnote{Context can alternatively be though of as functions from expressions to expressions.}.


\newcommand{\Cs}{\mathcal{c}}
\newcommand{\p}{\mathcal{p}}
Similarly to expressions, contexts can be extended to \textit{context slices} by allowing slices within:
\[\Cs ::= \mark \mid \lambda p : \upsilon. \Cs \mid \Cs(\varsigma) \mid \varsigma(\C) \mid \Cs : \upsilon\]

However, the precision relation $\sqsubseteq$ is defined differently, requiring that the mark $\mark$ must remain in the same position in the context structurally speaking. For example $\mark(\gap) \sqsubseteq \mark(1)$, but $\mark \not \sqsubseteq \mark(1)$. This can be concisely defined by \textit{extensionality} (\textbf{cite}):

\begin{definition}[Context Precision]\label{def:ContextPrecision}
If $\Cs'$ and $\Cs$ are context slices, then $\Cs' \sqsubseteq \Cs$ if and only if, for all expressions $e$, that $\Cs'\{e\} \sqsubseteq \Cs\{e\}$.
\end{definition}
Again, we refer to a context slice $\Cs$ of $\C$ as one satisfying that $\Cs' \sqsubseteq \C$.

We also get that filling contexts preserves the precision relations both on expression slices \textit{and} context slices:
\begin{conjecture}[Context Filling Preserves Precision]
For expression slice $\varsigma$ and context slice $\Cs$. Then if we have slices $\varsigma' \sqsubseteq \varsigma$, $\Cs' \sqsubseteq \Cs$ then also $\Cs'\{\varsigma'\} \sqsubseteq \Cs\{\varsigma\}$.
\end{conjecture}
Therefore, context slices $\Cs$ can be though of as monotone function (\textbf{cite})...

Show that composition is also preserved over precision, i.e. it is a functor.

\textit{Make some references to category theory, i.e. category of slices with morphisms being context slices. Or that slices form categories on expression $e$ and contexts are a functor between program slice categories. i.e. contexts are monotonic functions}

\textbf{rewrite}
The accompanying notion of a \textit{typing co-context slice} is retained representing the notion of \textit{irrelevant typing assumptions}, those assumptions which would be used within portions of the context which were omitted. i.e. for a a term placed in the mark, we could type it using the original context minus the ones used only in typing the context.

This pair of context slice and typing assumptions slice is together referred to as a \textit{program context slice} and notated $\p^\gamma$, should $\gamma$ be empty it may be omitted shorthand.

Extend composition to program contexts and substitution of program slices. Again as slices are a superset of expressions, then this extends to expression etc.

When 

\subsection{Type-Indexed Slices}
For \textit{criteria 2 \& 3 and cast slicing}, there is a need to decompose slices to find sub-slices which contribute to specific portions of a compound type. For example, which part of the program slice was related to the argument type of a function specifically.

\textbf{Give EXAMPLE}

A \textit{type(-indexed) slice} consists of: a program slice, a program context slice, and a \textit{type} \textit{index} $i$. This index is either an \textit{atomic} type label or is \textit{compound}, consisting of type slices conforming to the structure of types:
\[i ::= \dyn \mid b \mid s \to s\]
\[s ::= \p\{i \mid \rho\}\]
The type that a type slice $s$ \textit{represents} is the slice retaining only it's type labels. This will be notated by $\type{s}$, defined inductively:
\[\type{\p\{\dyn\mid \rho\}} = \dyn \quad \type{\p\{b \mid \rho\}} = b\]
\[\type{\p\{s_1 \to s_2 \mid \rho\}} = \type{s_1} \to \type{s_2}\]

A term $e$ in some context $\C$ will be associated with a \textit{type slice} with the meaning that $\rho$ contains a program slice for typing $e$ and $\p$ contains a program context slice for typing $e$ in context $\C$ according to some criterion. Typically the context slice would correspond to type analysis and the program slice would correspond to type synthesis.

The compound type slices must satisfy the crucial property that the sub-terms are sub-\textit{slices} of $\rho$. That is:
\[\p\{\p_1\{i_1 \mid \rho_1\} \to \p_2\{i_2 \mid \rho_2\} \mid \rho\} \implies \p_1\{\rho_1\} \sqsubseteq \rho\text{ and }\p_2\{\rho_2\} \sqsubseteq \rho\]

The precision relation can be extended to slices pointwise upon the program slice and context slice, and atomic types $a$ (i.e. $a ::= \dyn \mid b$):
\[\p'\{a' \mid \rho'\} \sqsubseteq \p\{a \mid \rho\} \iff \p' \sqsubseteq \p,\ \rho' \sqsubseteq \rho,\text{ and }a' \sqsubseteq a\]
And recursively for compound slices:\footnote{Note, function arguments are \textit{covariant} for this.}
\begin{align*}
\p'\{s_1' \to s_2' \mid \rho'\} \sqsubseteq \p\{s_1 \to s_2 \mid \rho\} \iff &\p' \sqsubseteq \p,\ \rho' \sqsubseteq \rho,\\
&s_1' \sqsubseteq s_1,\text{ and }s_2' \sqsubseteq s_2
\end{align*}

Composition of program slices to the outer context is possible, $(\p' \circ \p)\{i \mid \rho\}$, and is notated shorthand as $\p'\{s\}$ for $s = \p\{i \mid \rho\}$.

Additionally, if $\p$ is empty, $\mark^\emptyset$, a type slice may be notated without it: $i \mid \rho$. Equally, when $\rho$ is empty, $\gap^\emptyset$, then notate $\p'\{i\}$. This means that if both $\p, \rho$ are both empty then we have $i$ which is syntactically identical to types $\tau$, so we can trivially treat types as \textit{empty type slices}.  

Then function matching $\funmatch$ can be extended to slices in multiple different ways with different uses depending on the context, see the appendix \textbf{ref}.

\subsection{Criterion 1: Synthesis Slices}
\label{sec:SynthesisSlices}
For \textit{synthesis type slices} we consider an expression synthesising a type $\tau$ under some context $\Gamma$:
\[\synthesis{e}{\tau}\]
And consider the slices in $P_e^{\Gamma}$ and attempt to find the minimum slice $\rho = [\varsigma\mid \gamma]$ constraining that $\rho$ also synthesises the same type $\tau$ under the restricted context $\gamma$:
\[\synthesis[\gamma]{\varsigma}{\tau}\]
Where minimality requires that no other (strictly) less precise slice satisfies the criterion. That is: for any slice $\rho' = [\varsigma'\mid \gamma']$, if $\synthesis[\gamma']{\varsigma'}{\tau}$ and $\rho' \sqsubseteq \rho$, then $\rho' = \rho$.

\textbf{GIVE CONCRETE EXAMPLE HERE, use highlighting}

I conjecture that, under the Hazel type system, there exists a unique minimum slice for each $\synthesis{e}{\tau}$:\footnote{Would follow from uniqueness of typing derivations in Hazel.}
\begin{conjecture}[Uniqueness]\label{conj:SynthesisSliceUniqueness}
If $\rho$ and $\rho'$ are \textit{minimum synthesis slices} for $\synthesis{e}{\tau}$, then $\rho = \rho'$.
\end{conjecture}

These slices can be found by omitting portions of the program which are \textit{type checked}. If, $\analysis{e}{\tau}$, then by use of the subsumption rule we also have that $\analysis{\gap}{\tau}$:
\[\inference[Subsumption]{\synthesis{\gap}{\dyn} & \tau \sim \dyn}{\analysis{e}{\tau}}\] 
As the dynamic type is consistent with any type: $\dyn \sim \tau$.

Then, to find the \textit{minimum synthesis slice}, we can mimic the Hazel type synthesis rules (see \cref{fig:typing}), replacing uses of type analysis with gaps. Creating a judgement $\synthesisslice{e}{\tau}{\rho}$ meaning: \textit{$e$ that synthesises type $\tau$ under context $\Gamma$ produces minimum synthesis slice $\rho$}.

To demonstrate, the expression slice of a variable $x$ can only be either $x$, requiring the use of $x : \tau$ from the context:
\[
\inference[SVar]{x : \tau \in \Gamma & \tau \neq \dyn}{\synthesisslice{x}{\tau}{[x\mid x:\tau]}}\]
But if $x : \dyn$, then the (empty) slice $[\gap, \emptyset]$ also synthesises $\dyn$, so instead use this. 

For functions, we can recursively find the slice of the function body (which synthesises it's type in the original rules, hence having a minimum synthesis slice) and place inside a function. 
If the assumption $x : \tau_1$ was \textit{required} in synthesising that type, then this name must be present in the expression slice and the context slice no longer requires this assumption to type check the sliced function:
\[\inference[\tiny SFun]{\synthesisslice[\Gamma,x:\tau_1]{e}{\tau_2}{[\varsigma \mid \gamma, x : \tau_1]} }{\synthesisslice{\lambda x:\tau_1.\ e}{\tau_1 \to \tau_2}{[\lambda x : \tau_1.\ \varsigma \mid \gamma]}}\]
Otherwise, if $\gamma$ does not use variable $x$ then this binding may be omitted:
\[\inference[\tiny SFunConst]{\synthesisslice[\Gamma,x:\tau_1]{e}{\tau_2}{[\varsigma \mid \gamma]} & x \not \in \mathrm{dom}(\gamma)}{\synthesisslice{\lambda x:\tau_1.\ e}{\tau_1 \to \tau_2}{[\lambda \gap : \tau_1.\ \varsigma \mid \gamma]}}\]

For function applications we can simply omit the argument, while the slice for the function can be obtained as it synthesises it's type.
\[\inference[\tiny SApp]{\synthesisslice{e_1}{\tau_1}{[\varsigma_1 \mid \gamma_1]}}{\synthesis{e_1(e_2)}{\tau}{[\varsigma_1(\gap) \mid \gamma_1]}}\]
The remaining rules are in \cref{fig:SynthesisSlices}.

It is \textit{expected} \textbf{(Proof TODO)} that these rules do indeed always produce a slice for any expression which synthesises a type, and that this slice is minimal:
\begin{conjecture}[Correctness]
\label{conj:SynthesisSliceCorrectness}
If $\synthesis{e}{\tau}$ then:
\begin{itemize}
\item $\synthesisslice{e}{\tau}{\rho}$ where $\rho = [\varsigma \mid \gamma]$ with $\synthesis[\gamma]{\varsigma}{\tau}$.
\item For any $\rho' = [\varsigma' \mid \gamma'] \sqsubseteq [e\mid \Gamma]$ such that $\synthesis[\gamma']{\varsigma'}{\tau}$ then $\rho \sqsubseteq \rho'$.
\end{itemize}
\end{conjecture}


\subsection{Criterion 2: Analysis Slices}\label{sec:AnalysisSlices}
A similar idea can be devised for analysis slices. Essentially, we do the opposite of \textit{criterion 1} and omit sub-terms where \textit{synthesis} was used. The objective is to show \textit{why} a checked term is required to check against it's type.

The useful notion to represent this are \textit{context slices}. It is the terms immediately \textit{around} the sub-term where the type checking deconstructs a checking type:

For example, when checking this term against $\code{Bool} \to \code{Int}$:
\[(\lambda x. \hole^u)\]
The slice context of the inner hole term $\hole^u$, which is required to be consistent with \code{Int}, would be the following:
\[\hlcmaths[yellow!30]{(\lambda} x\hlcmaths[yellow!30]{.} \hole^u \hlcmaths[yellow!30]{)}\]
Intuitively, this means that the \textit{contextual} reason for $\hole^u$ to be required to be an \code{Int} is that it was within a function (and the context was initially checked against $\code{Bool} \to \code{Int}$ and deconstructed to retrieve the return type $\code{Int}$.

There are also subterms:
\[\hole^u : \code{Int} : \dyn : \code{Bool}\]

\textbf{GO BACK TO ORIGINAL IDEA OF INPUT CONTEXT AND OUTPUT EXPRESSION SLICE. Then the actual slice for a term is it's context slice plus expression slice. Minimum slice that still checks same type without subsumption.}

\begin{definition}[Analysis Slice]\label{def:analysisslice}
For a derivation $\analysis{e}{\tau}$ containing a sub-derivation $\analysis[\Gamma']{e'}{\tau'}$ in context $\C$, that is, $e = \C\{e'\}$. An \textit{analysis slice} of $e'$ is a \textit{program context slice} $\p = [\Cs, \gamma]$ such that:
\begin{itemize}
\item $\Cs$ is a slice of $\C$, that is, $\Cs \sqsubseteq \C$.
\item $e'$ also checks against $\tau$ within slice context $\Cs$ under $\Gamma$, that is, $\analysis[\Gamma]{\Cs \{e'\}}{\tau}$, and still requires a sub-derivation for $\analysis[\Gamma'\backslash \gamma]{e'}{\tau'}$ using . 
\item $\gamma$ retains enough assumptions to type (only) the context slice $\Cs$, that is, $\analysis[\gamma]{\Cs\{\gap\}}{\tau}$.
\end{itemize}
\end{definition}

The \textit{minimum analysis slice} is just the minimum under the precision ordering $\sqsubseteq$. And it must be unique:

\begin{conjecture}[Uniqueness]\label{conj:AnalysisSliceUniqueness}
If $\rho$ and $\rho'$ are \textit{minimum analysis slices} for sub-terms $e'$ of $e$ where $\analysis{e}{\tau}$, then $\rho = \rho'$.
\end{conjecture}

Similarly, the minimum slice can be defined syntactically by a judgement, notated (with $e = \C\{e'\}$ implicit):
\[\analysisslice[\Gamma]{\C}{e'}{\tau}{\p}\]
Meaning that, for $e$ which type checks against $\tau$ under context $\Gamma$, we have a minimum analysis slice $\p$.

This judgement can be defined syntactically upon possible contexts. For the empty context:
\[\inference[ ASEmpty]{\analysis{e}{\tau}}{\analysisslice{\mark}{e}{\tau}{[\mark\mid \emptyset]}}\]

Then (unannotated) function contexts mimic the rule for checking against functions, again with two rules depending on if $x$ is used in typing the context:
\[\inference[ASFun]{\tau \funmatch \tau_1 \to \tau_2 & \analysisslice[\Gamma, x:\tau_1]{\C}{e}{\tau_2}{[\Cs \mid \gamma, x: \tau_1]}}{\analysisslice{(\lambda x.\ \C)}{e}{\tau}{[\lambda x.\ \Cs \mid \gamma]}}\]
\[\inference[ASFunConst]{\tau \funmatch \tau_1 \to \tau_2 & \analysisslice[\Gamma, x:\tau_1]{\C}{e}{\tau_2}{[\Cs \mid \gamma]}\\ x \not \in \mathrm{dom}(\gamma)}{\analysisslice{(\lambda x.\ \C)}{e}{\tau}{[\lambda \gap.\ \Cs \mid \gamma]}}\]
All the remaining situations will use \textit{consistency}. Any term that used consistency could have been checked 
\[\inference[ASAnnot]{\analysisslice{\C}{e}{\tau'}{[\Cs\mid\gamma]} & \tau'' \sqsubseteq \tau' & \tau'' \sim \tau \\ \tau'' \text{ is \textit{minimal} satisfying the above}}{\analysisslice{(\C : \tau')}{e}{\tau}{[\Cs : \tau''\mid\gamma]}}\]
This minimal $\tau''$ does indeed exist as it is known that $\tau$ is consistent with $\tau$.\footnote{As the original term type checked successfully.}
The same is applied to annotated functions. Then for applications, a type is received by synthesis but we still check using this minimal 
\[\inference[ASApp1]{\synthesis{\C\{e_1\}}{\tau_1} & \tau_1 \funmatch \tau_2 \to \tau'\\ \tau' \sim \tau & \analysisslice{\C}{e_1}{\tau_2 \to \tau}{[\Cs \mid \gamma]}}{\analysisslice{\C(e_2)}{e_1}{\tau}{[\Cs(\gap)\mid \gamma]}}\]
In the second case:
\[\inference[ASApp2]{\synthesis{\C\{e_1\}}{\tau_1} & \tau_1 \funmatch \tau_2 \to \tau'\\ \analysisslice{\C}{e_2}{\tau_2}{[\Cs \mid \gamma]}}{\analysisslice{e_1(\C)}{e_2}{\tau}{[\gap(\Cs) \mid \gamma]}}\]
This formulation does indeed find the minimum such slice in the context:
\begin{conjecture}[Correctness]\label{conj:AnalysisSliceCorrectness}
If $\analysis{e}{\tau}$ and $e = \C\{e'\}$ then:
\begin{itemize}
\item $\analysisslice{\C}{e'}{\tau}{\p}$ where $\p = [\Cs \mid \gamma]$ with $\analysis[\Gamma]{\Cs\{e'\}}{\tau}$ and $\analysis[\gamma]{\Cs\{\gap\}}{\tau}$.
\item For any $\p' = [\Cs' \mid \gamma'] \sqsubseteq [\C\mid \Gamma]$ such that $\analysis[\Gamma']{\Cs'\{e'\}}{\tau}$ and $\analysis[\gamma']{\Cs'\{\gap\}}{\tau}$ then $\p \sqsubseteq \p'$.
\end{itemize}
\end{conjecture}

In the case of Hazel, it turns out that this minimum program context slice is always just the least precise version of the input context and never requires typing assumptions, so $\gamma$ is empty. But, other language features may have more complex behaviour\footnote{Type functions, for example.} and formalising it this way helps build intuition for \textit{criterion 3} and it's extension in \textit{cast slicing}.

\subsection{Criterion 3: \textit{good\_name\_here}}
Both the previous criterion only extracted information about one half of the typing rules, either synthesis or analysis.

This criterion aims to highlight all regions of code which \textit{contribute} to the given type (either synthesised or analysed). Where \textit{contribute} means that if the sub-term changed it's type, then the overall term would also, or would become ill-typed.

\textbf{Give example here}

Notice that, in any typing derivation the only sub-terms which \textit{can} be changed without causing the rule to no longer apply are those which the premises and conclusion are \textit{not} linked \textit{syntactically}. There is only one such rule: \textit{subsumption}. The only way a replacing a subterm with a new type could be valid is if the new type is 

Talk about annotations as analysis slices?

The objective of this criterion is to determine which parts of an expression required to be typed checked. Therefore, it omits the sub-terms which could be changed to a different type, but still have the whole expression check against the same type. 

\textbf{Give example here with highlighting.}

Formally, this criterion considers expressions $e$ type checked against a type $\tau$ under typing context $\Gamma$:
\[\analysis{e}{\tau}{\varsigma}\]
And finds the minimum slice of $e$ in this context such that 


Mention pattern slices.

\subsection{Join Types}\label{sec:JoinTypesTheory}
The Hazel core calculus is very primitive, only consisting of \textit{base types}, \textit{annotations}, and \textit{functions}. Extensions to gradual types \cite{GradualJoins}, and Hazel \cite{MarkedLocalisation}\footnote{Here as the \textit{meet} of the opposite of my \textit{precision} order.}: \textit{if} expressions, \textit{pattern matching}, \textit{sum types} etc. all require\footnote{Or are easiest formulated with.} \textit{join types}. 

A join of two types $\tau_1 \sqcup \tau_2$ (if one exists) is the least precise (most general) type that more precise than both $\tau_1, \tau_2$: that $\tau_1 \sqsubseteq \tau_1 \sqcup \tau_2$ and $\tau_1 \sqsubseteq \tau_1 \sqcup \tau_2$. Therefore, the join is therefore is consistent with both $\tau_1, \tau_2$. Type consistency can be reformulated in terms of joins: $\tau_1, \tau_2$ are consistent \textit{if and only if} they have a join. This is the \textit{order-theoretic} \textbf{(cite)} join with respect to the precision partial order on types. 
For example, the type of an \textit{if statement} would be the join of the types of it's branches.

These add an additional way to generate a \textit{new type} other than by synthesis or from annotations.\textit{ add some machinery to demonstrate how a program slice could be constructed by taking the `deepest' branch in the join and working out if it was the left/right branch etc.}

Also has application in combining slices at subsumption (so each term has one slice, rather than some having two).

\section{Cast Slicing Theory}\label{sec:CastSlicingTheory}

Fairly trivial, just treat slices as types and decompose accordingly. The whole reason of indexing by type was to allow this.

The idea of it being a minimal program slice producing the same cast doesn't really work here due to dynamics. Explore the maths of this. Either way, it is a useful construct in practice. Exploring this in more detail, looking at \textit{dynamic program slicing} could be a good future direction.


\subsection{Indexing Program Slices by Types}


\subsection{Elaboration}

\subsection{Dynamics}

\subsection{Cast Dependence}
This in combination with the indexed slices could have some nice mathematical properties.

Though, these would need to retrieve information about parts of slices that were lost when decomposing slices. i.e. when a slice $\tau_1 \to \tau_2$ extracts the argument type $\tau_2$, the slicing criterions lose track of the original lambda binding. A way to reinsert these bindings such that we get a minimal term which \textit{Evaluates to the same cast} e.g. But this will have lots of technicalities with correctly tracking the restricted contexts $\gamma'$ for closures (i.e. functions returning functions which have been applied once). \textbf{TALK ABOUT THIS IN THE FURTHER DIRECTIONS SECTION.}

\section{Proofs}\label{sec:Proofs}
Do if there is time. Prove that analysis slice contexts actually make sense, that they maintain a valid term that is \textit{still} minimal.

\section{Type Slicing Implementation}\label{sec:TypeSlicingImplementation}
\subsection{Type Slice Data-Type}\label{sec:TypeSliceDataType}
Detail initial implementation (just tagging existing types). Compare with final implementation, quantitative numbers for improve could be obtained but would require quite a bit of coding work... Polymorphic Variants :))
\subsubsection{Code Slices}\label{sec:CodeSlices}
id based. Has ctx used but not actually required as I decide to directly store typslices in context (explain how this differs from the theory).

Mention that full slices as in the theory is very inefficient.
\subsubsection{Integration with Existing Type Data-Type}
Use of `Typ. Explain how this allows it to easily be disabled and saves space (maybe).
\subsubsection{Synthesis \& Analysis Slices}
Incremental/Global. Detail the choice to not annotate many analysis slices.
\subsubsection{Mapping Functions}
\subsubsection{Type Slice Joins}
Just unions of the lists. Double check that we can't have elements from more than one branch highlighted.\footnote{Type variables might make this possible??}

\subsection{Static Type Checking}\label{sec:TypeChecking}
Detail the statics and Info types. Explain the distinction between Self.re and Mode.re, which links very nicely with the synthesis and analysis slice distinctions.

\subsection{Elaboration}\label{sec:Elaboration}
Make casts use type slices, insertion is mostly the same. Just need to ensure that the slices are preserved (i.e. types are threaded through without being replaced)

\subsection{User Interface}
Click on analysis or synthesis slices from context inspector

\section{Cast Slicing Implementation}\label{sec:CastSlicingImplementation}
\subsection{Cast Transitions}

Cast transition and unboxing logic

\subsection{User Interface}
Mainly talk about the Model-view architecture and passing the cursor into the evaluator view to allow 


\section{EV\_MODE Evaluation Abstraction}
Very complex!!!

\section{Indeterminate Evaluation}\label{sec:IndetEval}
\subsection{Futures Data-Type}\label{sec:Futures}
Lazy lists, often infinite
\subsection{Hole Instantiation}\label{sec:HoleInstantiation}
Small Hole hypothesis, quick check


\subsubsection{Choosing which Hole to Instantiate}
Use EV\_Mode to select next hole to instantiate

\subsubsection{Synthesising Terms for Types}
Difficulties instantiating strings...

\subsubsection{Substituting Holes}\label{sec:HoleSubstitutionImplementation}
Detail that this was an unexpected extra task, and is therefore not exactly the same as hole substitution as detailed in Preparation (i.e. no metavars or contexts annotated on holes, but it is enough for the search procedure to work)

\subsection{Cast Laziness}\label{sec:CastLaziness}
Ref the original cast slicing paper, which is not lazy apparently? Laziness sort of breaks the idea that runtime errors evaluate to cast errors. There can be compound values of the wrong type being cast, but the error will only be found upon accessing parts of the compound type.

Making casts eager is a major change to the actual transitions.

Eager casts also catch `spurious' errors (see Evaluation).


\subsection{User Interface}

\section{Evaluation Stepper}\label{sec:Stepper}
\subsection{Evaluation Contexts}
\subsection{Customisable Hole Instantiation}
\subsection{User Interface}


\section{Search Procedure}\label{sec:SearchProcedure}
\subsection{Detecting Relevant Cast Errors}
i.e. failed cast at head of term
\subsection{Filtering Indeterminate Evaluation}
Done via EV\_MODE similarly to finding which hole to instantiate.

\subsection{Iterative Deepening}\label{sec:IterativeDeepening}
Required after evaluating that infinite loops break the thing
\subsection{User Interface}


