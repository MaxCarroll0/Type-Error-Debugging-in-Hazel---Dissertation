\chapter{Implementation}\label{chap:Implementation}
This project was conducted in \textit{two} major phases:
\begin{enumerate}
\item I devised a mathematical theory for \textit{type slicing} and \textit{cast slicing}. I considered changes to the system of Seidel et al \cite{SearchProc}. for the \textit{type error witnesses search procedure} to work in Hazel.  

\item I implement the theories, extending to the majority of Hazel. Suitable deviations from the theory, made upon critical evaluation, are detailed throughout.
\end{enumerate}

\section{Type Slicing Theory}\label{sec:TypeSlicingTheory}

I develop a novel method, \textit{type slicing}, to aid programmers in understanding \textit{how} a bidirectional type system works. First I define typing slices. Then, three slicing criteria: synthesis, analysis, and contribution slices, each associate typing derivations with different explanatory slices. 

The first two criteria give insight on the synthesised and analysed type contributions. The third completes a picture of code regions contributing in any way to a term's type.

The second and third criterion were \textit{very challenging} to formalise, requiring non-obvious mathematical machinery: \textit{context typing slices} (\cref{sec:ContextTypingSlices}), \textit{checking contexts} (\cref{sec:CheckingContexts}), and \textit{type-indexed slices} (\cref{sec:TypeIndexedSlices}). Only the basic definitions are given here, the full theory is found in \cref{sec:SlicingTheory}.

\subsection{Expression Typing Slices}\label{sec:ExpressionTypingSlices}
First, I introduce what \textit{slices} are in this context. The aim is to provide a formal representation of term \textit{highlighting}. 

\subsubsection{Term Slices}
A \textit{term slice} is a term with some sub-terms omitted. The omitted terms are those that are \textit{not} highlighted. For example if my slicing criterion is to \textit{omit terms which are typed as} \code{Int}, then the following expressions highlights as:

\[\hlcmaths[yellow!30]{(\lambda} x: \code{Int}\hlcmaths[yellow!30]{.\ \lambda y : \code{Bool}.}\ x\hlcmaths[yellow!30]{)(}1\hlcmaths[yellow!30]{)}\]


Omitted sub-terms are replaced by a \textit{gap} term, notated $\gap$. Representing the example above, we get:
\[(\lambda \gap.\ \lambda y : \code{Bool}.\ \gap)(\gap)\]

We can then define a \textit{precision} partial order \cite{PartialOrder} on term slices: $\varsigma_1 \sqsubseteq \varsigma_2$ meaning $\varsigma_1$ is less or equally precise than $\varsigma_2$. That is, $\varsigma_1$ matches $\varsigma_2$ structurally except that some sub-terms may be gaps. For example:
\[\gap \sqsubseteq\gap + \gap\sqsubseteq 1 + \gap \sqsubseteq 1 + 2\]

\paragraph{Lattice Structure:}\label{sec:JoinTypesTheory} For any \textit{complete term}\footnote{Having no gaps.} $t$, the slices of $t$ form a \textit{bounded lattice structure} \cite{Lattice}. That is, every pair $\varsigma_1, \varsigma_2$ has a \textit{join} $\varsigma_1 \sqcup \varsigma_2$ and \textit{meet} $\varsigma_1 \sqcap \varsigma_2$. In general, not all slices slices have joins: $1 \centernot\sqcup\ 2$, but do have meets as $\gap \sqsubseteq \varsigma$ for all $\varsigma$.
 
\subsubsection{Typing Assumption Slices}
Expression typing is performed given a set of \textit{typing assumptions}. Therefore, in addition, we also desire a slice taking the \textit{relevant}\footnote{To the given criterion.} assumptions. Typing assumptions are \textit{partial functions} mapping variables to types (see \cref{sec:TypingJudgements}). 

Hence, their slices slices are partial functions to \textit{type slices}. Such that, a slice maps no more variables to no more precise types. This, and meets and joins, can be extended by extensionality \cite{Extensionality}:

\begin{definition}[Typing Assumption Slice Precision]
For typing assumption slices $\gamma_1, \gamma_2$. Where $\mathrm{dom}(f)$ is the set of variables for which a partial function $f$ is \textit{defined}:
\[\gamma_1 \sqsubseteq \gamma_2 \iff \mathrm{dom}(\gamma_1) \subseteq \mathrm{dom}(\gamma_2) \text{ and } \forall x \in  \mathrm{dom}(\gamma_1).\ \gamma_1(x) \sqsubseteq \gamma_2(x)\]
\end{definition}
\begin{definition}[Typing Assumption Slice Joins and Meets]
For typing slices $\gamma_1, \gamma_2$, and any variable $x$: 

If $\gamma_1(x) = \bot$ then $(\gamma_1 \sqcup \gamma_2)(x) = \gamma_2(x)$ and $(\gamma_1 \sqcap \gamma_2)(x) = \bot$, analogously if $\gamma_2(x) = \bot$. Otherwise, $(\gamma_1 \sqcup \gamma_2)(x) = \gamma_1(x) \sqcup \gamma_2(x)$.
\end{definition}
Again, slicing complete typing assumptions $\Gamma$ forms a bounded lattice. In general, some slices have no join: consider $x : \code{Int}$ and $x : \code{String}$.

\subsubsection{Expression Typing Slices}
Finally, an \textit{expression typing slice}, $\rho$, is a pair, $\varsigma^\gamma$, of a term slice and a typing slice. Precision, joins and meets, can be extended pointwise to term typing slices with all the same properties.

\paragraph{Typing Checking:} \textit{Expression slices} can be \textit{type checked} under the \textit{type assumption slices} by replacing gaps $\gap$ by: holes of arbitrary metavariable $\hole^u$ in \textit{expressions}, fresh variables in \textit{patterns}, and the dynamic type in \textit{types}. Notated by $\type{\cdot}$.

\begin{definition}[Expression Typing Slice Type Checking]
For expression typing slice $\varsigma^{\gamma}$ and type $\tau$. $\synthesis[\gamma]{\varsigma}{\tau}$ iff $\synthesis[\type{\gamma}]{\type{\varsigma}}{\tau}$ and $\analysis[\gamma]{\varsigma}{\tau}$ iff $\analysis[\type{\gamma}]{\type{\varsigma}}{\tau}$.
\end{definition}
\subsection{Context Typing Slices}\label{sec:ContextTypingSlices}
Next, some of an expression's type might be enforced by the surrounding \textit{context}. For example, the  type of the underlined expression below is enforced by the surrounding highlighted annotation:
\[\underline{(\lambda x. \hole^u )} \hlcmaths[yellow!30]{:  \code{Bool} \hlcmaths[yellow!30]{\to \code{Int}}}\]

\subsubsection{Contexts and Their Slices}
\renewcommand{\C}{\mathdcal{C}}
We represent these surrounding contexts by a \textit{term context} $\mathdcal{C}$. Which marks \textit{exactly one} sub-term as $\cmark$. Where $\C\{t\}$ substitutes term $t$ for the mark $\cmark$ in $\C$, only allowed if the marked position expects a term of the same class as $t$ (pattern \code{Pat}, type \code{Typ}, or expression \code{Exp}). Notate the classes by $\C : \code{X} \to \code{Y}$. Contexts are \textit{composable}: $(\C_1 \circ \C_2)(t) = \C_1\{\C_2\{t\}\}$ when $\C_1 : \code{X} \to \code{Y}$ and $\C_2 : \code{Y} \to \code{Z}$. Composition is associative.


\newcommand{\Cs}{\mathdcal{c}}
\newcommand{\p}{\mathdcal{p}}
These extend to slices analogously to term slices. However, the precision relation $\sqsubseteq$ more restrictive, requiring the mark $\cmark$ to remain in the same structural position. For example: $\cmark(\gap) \sqsubseteq \cmark(1)$, but $\cmark \not \sqsubseteq \cmark(1)$. Concisely defined by \textit{extensionality}:

\begin{definition}[Context Precision]\label{def:ContextPrecision}
If $\Cs : \code{X} \to \code{Y}$ and $\Cs' : \code{X} \to \code{Y}$ are context slices, then $\Cs' \sqsubseteq \Cs$ if and only if, for all terms $t$ of class $\code{X}$, that $\Cs'\{t\} \sqsubseteq \Cs\{t\}$.
\end{definition}

We find that filling contexts preserves the precision relations both on term slices \textit{and} context slices:
\begin{proposition}[Context Filling Preserves Precision]
For context slice $\Cs : \code{X} \to \code{Y}$ and term slice $\varsigma$ of class \code{X}. Then if we have slices $\varsigma' \sqsubseteq \varsigma$, $\Cs' \sqsubseteq \Cs$ then also $\Cs'\{\varsigma'\} \sqsubseteq \Cs\{\varsigma\}$.
\end{proposition}

Joins and meets can be defined extensionally as before, still forming bounded lattices over complete contexts. The lattice bottom is the \textit{purely structural context}, consisting of only gaps with the mark in the correct position. In general, in addition to joins, not all contexts have meets: $\cmark\centernot \sqcap \cmark(\gap)$.

\subsubsection{Typing Assumption Contexts and Their Slices}
The accompanying notion typing notion can be represented by \textit{endomorphisms on typing assumption slices}. These functions represents which \textit{relevant} typing assumptions must be \textit{added}, and those safely \textit{removable} when typing an expression within a context slice.

\renewcommand{\F}{\mathdcal{F}}
\newcommand{\f}{\mathdcal{f}}

Precision, joins, and meets can be defined via extensionality. As usual forming bounded lattices on complete functions, the bottom being the constant function to the empty typing assumptions. Again, such functions are monotone:
\begin{proposition}[Function Application Preserves Precision]
For typing assumption slice $\gamma$ and typing assumption context slice $\f$. Then if we have slices $\gamma' \sqsubseteq \gamma$, $f' \sqsubseteq f$ then also $f'(\gamma') \sqsubseteq f(\gamma)$.
\end{proposition}
\subsubsection{Context Typing Slices}
Finally, an \textit{expression context typing slice}, $\p$, is a pair, $\Cs^\f$, of an expression context slice and a typing assumption context slice defined pointwise, with all the same properties (including composition).

\paragraph{Type Checking: } An analogous $\type{\cdot}$ translation can be defined.

\subsection{Type-Indexed Slices}\label{sec:TypeIndexedSlices}
Decomposing slices by their type is required for cast slicing and useful in calculating slices according to analysis slices and hence also contribution slices. For example consider the following context slice explaining why the underlined term analyses $\code{Bool} \to \code{Int}$:
\[\underline{(\lambda x. \hole^u )} \hlcmaths[yellow!30]{:  \code{Bool} \hlcmaths[yellow!30]{\to \code{Int}}}\]
This would be tagged with the type $\code{Bool} \to \code{Int}$ where a sub-slice considering \textit{only} the \code{Int} return type (omitting the \code{Bool} annotation) can be extracted:
\[\underline{(\lambda x. \hole^u )} \hlcmaths[yellow!30]{:}  \code{Bool} \hlcmaths[yellow!30]{\to \code{Int}}\]
This section will only consider \textit{context slices}, but term slices are type-indexed analogously.

The main property that indexed-slices should maintain is that slices can be \textit{reconstructed} from their sub-parts. Joining the sub-slices will produce the full type. As sub-slices may slice different regions of code, we pair them with contexts which place the sub-slices within the same context, making them join-able.

\renewcommand{\S}{\mathdcal{S}}
\renewcommand{\s}{\mathdcal{s}}
\begin{definition}[Type-Indexed Context Typing Slices]
Syntactically defined: 
\[\S ::= \p \mid \p * \S \to \p * \S\] 
With any $\S$ only being valid if it has a full slice. The full slice of $\S$, notated $\overline{\S}$, is defined:
\[\overline{\p} = \p\]
\[\overline{\p_1 * \S_1 \to \p_2 * \S_2} = \p_1 \circ \overline{\S_1} \sqcup \p_2 \circ \overline{S_2}\]
\end{definition}
Then left \textit{(incremental)} composition and right \textit{(global)} composition can be defined, by composing at the upper type constructor or at the leaves respectively:
\begin{definition}[Type-Indexed Context Typing Slice Composition]
For type-indexed context typing slices $\S$ and $\S'$.  If $\S = \p$ and $\S' = \p'$:
\[\p' \circ \p = \overline{\p'} \circ \overline{\p}\qquad \p \circ \p' = \overline{\p} \circ \overline{\p'}\]
If $\S = \p$ and $\S' = \p_1' * \S_1' \to \p_2' * \S_2'$:
\[S \circ \S' = (\p \circ p_1') * \S_1' \to (\p \circ \p_2') * \S_2'\]
If $\S = \p_1 * \S_1 \to \p_2 * \S_2$:
\[\S \circ \S' = \p_1 * (\S_1 \circ \S') \to \p_2 * (\S_2 \circ \S')\]
\end{definition}

This definition stems from it representing regular context typing slice composition over it's full slices.
\begin{proposition}[Type-Indexed Composition Preserves Full Slice Composition]
For type-indexed slices $\S$ and $\S'$: 
\[\overline{\S \circ \S'} = \overline{\S} \circ \overline{\S'}\]
\end{proposition}
Application, notated $\mid>$ be defined similarly, converting indexed context slices into valid indexed expression slices. The opposite direction is more difficult and can be found in appendix (\cref{def:Application}).


\subsection{Criterion 1: Synthesis Slices}
\label{sec:SynthesisSlices}

Synthesis slices aim to explain why an expression \textit{synthesises} a type. They omits all sub-terms which analyse against a type retrieved from synthesising some other part of the program. For example, the following term synthesises a $\code{Bool} \to \code{Bool}$ type, and the variable $x : \code{Int}$ and argument are irrelevant:

\[\hlcmaths[yellow!30]{(\lambda} x: \code{Int}\hlcmaths[yellow!30]{.\ \lambda y : \code{Bool}.\ y)(}1\hlcmaths[yellow!30]{)}\]

\begin{definition}[Synthesis Slices]
For a synthesising expression, $\synthesis{e}{\tau}$. A synthesis slice is an expression typing slice $\varsigma^{\gamma}$ of $e^\Gamma$ which also synthesises $\tau$, that is, $\synthesis[\type{\gamma}]{\type{\varsigma}}{\tau}$.
\end{definition}
\begin{proposition}[Minimum Synthesis Slices]
A minimum synthesis slice of $\synthesis{e}{\tau}$, under $\sqsubseteq$, exists and is unique.
\end{proposition}

These slices can be calculated via a typing judgement $\synthesisslice{e}{\tau}{\S}$, meaning $\S$ is the type-indexed synthesis slice of $e^\Gamma$ synthesising $\tau$. The judgement rules mimic Hazel's typing rules, giving an algorithm to calculate minimum synthesis slices (see \cref{sec:SynthesisSlicesJudgement}).

\subsection{Criterion 2: Analysis Slices}\label{sec:AnalysisSlices}
A similar idea can be devised for type analysis, represented using \textit{context slices}. After all, it is the terms immediately \textit{around} the sub-term where the type checking is enforced. For example, when checking this annotated term:
\[(\lambda x. \hole^u) : \code{Bool} \to \code{Int}\]
The \textit{inner hole term} $\hole^u$ (underlined) is required to be consistent with \code{Int} due to the annotation and lambda constructor present in it's context. The analysis slice will be:
\[\hlcmaths[yellow!30]{(\lambda} x\hlcmaths[yellow!30]{.} \underline{\hole^u} \hlcmaths[yellow!30]{) : } \code{Bool} \hlcmaths[yellow!30]{\to \code{Int}}\]

In other words, if the context slice was type checked, then the inner hole would \textit{still} required to analyse against \code{Int}. However, the overall synthesised type may differ, this sliced example would synthesis $\dyn \to \code{Int}$ vs. the unsliced $\code{Bool} \to \code{Int}$.


\subsubsection{Checking Context}
\label{sec:CheckingContexts}
We only want to consider the smallest context scope\footnote{The \textit{size} of the context around the marked term.} that enforced the type checking. For example, the below term has 3 annotations, but only the inner one enforces the \code{Int} type on the integer 1:
\[\underline{1} \hlcmaths[yellow!30]{: \code{Int}} : \dyn : \code{Bool}\]
I refer to this as the \textit{minimally scoped checking context}. Note that checking contexts will always synthesise a type. 

To give another example, an integer argument's type is enforced by the annotation on the function:
\[\hlcmaths[yellow!30]{(\lambda} x \hlcmaths[yellow!30]{: \code{Int}.}\ \hole^u\hlcmaths[yellow!30]{)(}\underline{1}\hlcmaths[yellow!30]{)}\]


\begin{definition}[Checking Context]
\label{def:CheckingContext}
For term $e$ checking against $\tau$: $\analysis[\Gamma]{e}{\tau}$. A checking context for $e$ is an expression context $\C$ and typing assumption context $\F$ such that: 
\begin{itemize}
\item $\C \neq \cmark$.
\item $\synthesis[\F(\Gamma)]{\C\{e\}}{\tau'}$ for some $\tau'$.
\item The above derivation has a sub-derivation $\analysis[\Gamma]{e}{\tau}$.
\end{itemize}
\end{definition}
\begin{definition}[Minimally Scoped Checking Context]
For a derivation $\analysis[\Gamma]{e}{\tau}$, a minimally scoped expression checking context is a checking context of $e$ such that no sub-context is also a checking context.
\end{definition}

All minimally scoped checking contexts can be constructed syntactically via rules (\cref{sec:CheckingContexts}). For a sub-term in a program, there will be a unique minimally scoped context which matches with the program structure (appendix \cref{def:CheckingContextInProgram}). Analysis slices are slices of minimally scoped checking contexts.
\begin{definition}[Analysis Slice]\label{def:analysisslice}
For $\analysis{e}{\tau}$ with a minimally scoped checking context $\C^\F$. An analysis slice is a context slice $\Cs^\f$ of $\C^\F$ where $\type{\Cs^\f}$ is also a checking context for $e$.
\end{definition}
\begin{conjecture}[Minimum Analysis Slices]\label{conj:AnalysisSliceUniqueness}
A minimum analysis slice of $\analysis{e}{\tau}$ in a checking context $\C^\F$, under $\sqsubseteq$, exists and is unique.
\end{conjecture}

This can again be calculated by a judgement reading as, \textit{$e$ which type checks against $\tau$ in checking context $\C$ has (type-indexed) analysis slice $\S$}:
\[\analysisslice{\C}{e}{\tau}{\p}\]

The rules build upon rules defining minimally scoped checking contexts, and are more involved, making use of \textit{type-indexed} synthesis slices\footnote{Hence, requiring conversion of (indexed) expression slices to context slices}, see \cref{fig:AnalysisSliceApplication}:
\begin{figure}[h]
\centering
\begin{subfigure}{\0.4\textwidth}
\[(\lambda x : \dyn. \lambda y : \code{Int}. y)(\code{true})\]
\caption{A function: synthesising $\code{Int} \to \code{Int}$.}
\end{subfigure}$\qquad$
\begin{subfigure}{\0.4\textwidth}
\[\hlcmaths[yellow!30]{(\lambda} x : \dyn\hlcmaths[yellow!30]{. \lambda y : \code{Int}. y)(}\code{true}\hlcmaths[yellow!30]{)}\]
\caption{Its slice.}
\end{subfigure}
\begin{subfigure}{\0.4\textwidth}
\[\hlcmaths[yellow!30]{(\lambda} x : \dyn\hlcmaths[yellow!30]{. \lambda} y \hlcmaths[yellow!30]{: \code{Int}.} y\hlcmaths[yellow!30]{)(}\code{true}\hlcmaths[yellow!30]{)}\]
\caption{The sub-slice relating \textit{only} to the input part $\code{Int}$.}
\end{subfigure}$\qquad$
\begin{subfigure}{\0.4\textwidth}
\[\hlcmaths[yellow!30]{(\lambda} x : \dyn\hlcmaths[yellow!30]{. \lambda} y \hlcmaths[yellow!30]{: \code{Int}.} y\hlcmaths[yellow!30]{)(}\code{true}\hlcmaths[yellow!30]{)(}\underline{1}\hlcmaths[yellow!30]{)}\]
\caption{The analysis slice of the function's argument ($\underline{1}$) when applied.}
\end{subfigure}
\caption{Analysis slice application uses synthesis slices}
\label{fig:AnalysisSliceApplication}
\end{figure}

\subsection{Criterion 3: Contribution Slices}
\label{sec:ContributionSlices}
This criterion highlights all regions of code which \textit{contribute} to typing succeeding. That is, all sub-terms who could change their type to make the term ill-typed. For example, for the underlined term:

\[((\lambda f: \code{Int} \to \dyn.\ f(1))\underline{(\lambda x : \code{Int}.\ x)}) : \code{Int} \to \code{Int}\]
Both contextual and synthetic parts (in dark yellow) contribute:
\[\hlcmaths[yellow!30]{((\lambda} f\hlcmaths[yellow!30]{: \code{Int} \to \dyn}.\ f(1)\hlcmaths[yellow!30]{)}\underline{\hlcmaths[yellow!70]{(\lambda} x \hlcmaths[yellow!70]{: \code{Int}.}\ x\hlcmaths[yellow!70]{)}}\hlcmaths[yellow!30]{) : \code{Int} \to \code{Int}}\]
Notice that the only sub-terms which do not contribute have their type changed without affecting the overall type must be expected to be dynamically annotated. Therefore, this criterion omits the dynamic code regions.

These can be calculated mimicking the Hazel typing rules. During subsumption, remove synthesis slice sub-part which match with dynamic parts of the analysing type. Contextual parts are found by passing context slices directly as inputs to type analysis rules.

\section{Cast Slicing Theory}\label{sec:CastSlicingTheory}
Cast slicing propagates type slice information during evaluation, by tagging casts types with type slices. A primary reason in formalising type-indexed slices was to make slices decomposable during evaluation, retaining only sub-slices relevant to the part of the type involved in the decomposed cast. This requires changing the syntax of casts $\scast{\S}{\S}$ and inserting casts between slices during elaboration. The first two criteria work together during elaboration, analysis slices inserted when casting on a elaborated expression who had it's type \textit{analysed}, and vice versa. The rules are found in
\cref{sec:CastSlicingElaboration}.

\section{Type Slicing Implementation}\label{sec:TypeSlicingImplementation}
Here I detail how the theories above were adapted to produce an implementation for Hazel.
\subsection{Hazel Terms}
\label{sec:HazelTerms}
Hazel represents its terms as a standard abstract syntax tree (AST) via mutual recursion. Every sub-term is tagged with an identifier (ID, \code{ID.t}). Terms are grouped similarly to the calculus (see \cref{sec:HazelSyntax}), but combining external and internal expressions, and adding patterns and environments, see \cref{fig:HazelTerms} \& \ref{fig:tupletermstructure}.


\begin{figure}
\centering
\resizebox{0.8\textwidth}{!}{\begin{tikzpicture}[
node distance=2cm and 4cm,
box/.style={draw, rounded corners, minimum width=5cm, align=left, text width=7cm, inner sep=10pt, thick},
exprbox/.style={box, draw={rgb,255:red,86;green,109;blue,117}},
pattsbox/.style={box, draw={rgb,255:red,24;green,156;blue,218}},
typesbox/.style={box, draw={rgb,255:red,159;green,106;blue,228}},
envsbox/.style={box, draw={rgb,255:red,135;green,119;blue,76}},
arrowExpr/.style={-{Latex[round]}, thick, draw={rgb,255:red,86;green,109;blue,117}},
arrowPatts/.style={-{Latex[round]}, thick, draw={rgb,255:red,24;green,156;blue,218}},
arrowTypes/.style={-{Latex[round]}, thick, draw={rgb,255:red,159;green,106;blue,228}},
arrowEnv/.style={-{Latex[round]}, thick, draw={rgb,255:red,135;green,119;blue,76}}
]

% Nodes
\node[exprbox] (expr) {
\textbf{\textcolor[rgb]{0.337,0.427,0.459}{Expressions}}:\
Constructors, holes, constants, casts, operators, variables,  lists, tuples, labelled tuples\
let bindings, functions, closures,\
type functions, type aliases,\
pattern matching expressions,\ fixed points.
};

\node[envsbox, right=of expr] (envs) {
\textbf{\textcolor[rgb]{0.529,0.467,0.298}{Closure Environments}}:\
Maps variables to values\
in a syntactic context.
};

\node[yshift=-0.74cm,typesbox, below=of envs] (types) {
\textbf{\textcolor[rgb]{0.624,0.416,0.894}{Types}}:\
Dynamic type, base types,\
functions types, product types,\
tuple label types, sum types,\
type variables, universal types,\
recursive types.
};

\node[pattsbox, below=of expr] (patts) {
\textbf{\textcolor[rgb]{0.094,0.612,0.855}{Patterns}}:\
Variables, holes, wildcards,\
constructors, lists, tuples, labelled tuples, annotations.
};

% Arrows for containment
\draw[arrowExpr] (expr.south) -- (patts.north);
\draw[arrowExpr] ($(expr.east)+(0,0.3)$) to[bend left=15] ($(envs.west)+(0,0.3)$);
\draw[arrowExpr] (expr.south east) to (types.north west);

\draw[arrowPatts] (patts.east) to (types.west);

\draw[arrowEnv] ($(envs.west)+(0,-0.3)$) to[bend left=20] ($(expr.east)+(0,-0.3)$);
\draw[arrowEnv] (envs.south) -- (types.north);

% Self-loops
\draw[arrowExpr, looseness=7, out=135, in=45] ($(expr.north)+(-0.3,0)$) to ($(expr.north)+(0.3,0)$);
\draw[arrowPatts, looseness=7, out=225, in=315] ($(patts.south)+(-0.3,0)$) to ($(patts.south)+(0.3,0)$);
\draw[arrowTypes, looseness=7, out=225, in=315] ($(types.south)+(-0.3,0)$) to ($(types.south)+(0.3,0)$);

\end{tikzpicture}
}
\caption{Mutually Recursive Hazel Terms.}
\label{fig:HazelTerms}
\end{figure}

\begin{figure}[h]
\center\includegraphics[width=0.6\textwidth]{Media/Figures/tuple_term_structure}
\caption{Let binding a tuple patter with a type annotation.}
\label{fig:tupletermstructure}
\end{figure}
 
\subsection{Type Slice Data-Type}\label{sec:TypeSliceDataType}

\subsubsection{Expression slices as ASTs}

Directly storing expression slices directly as ASTs is both \textit{space and time inefficient}, even when accounting for the persistence \cite[ch. 2]{PurelyFunctionalDataStructures} of trees in OCaml. 

For highlighting purposes, there is no need to retain the structure (unlike the theory, we do not need to type check the results, instead just assuming correctness). 

\subsubsection{Unstructured Code Slices}
\label{sec:UnstructuredSlices}
With this in mind, I represent slices indirectly by their IDs with an \textit{unstructured} list, referred to now as a \textit{code slice}. Additionally, this allows more \textit{granular} control over slices, as they need not conform with the structure of expressions, which is taken advantage of in reducing slice sizes \cref{sec:SlicingAnalysis}.

\subsubsection{Type-Indexed Slices}
Cast slicing and contribution slices required \textit{type-indexed} slices. I therefore tag type constructors with slices recursively, i.e.:

\begin{figure}[h]
\begin{minted}[fontsize=\small]{reason}
type typslice_typ_term = 
  | Unknown
  | Arrow(slice_t, slice_t)  // Function type
  | ... // Type constructors
and typslice_term = (typslice_typ_term, code_slice)
and typslice_t = IdTagged.t(slice_term)
\end{minted}
\caption{Initial Type Slice Data-Type}
\end{figure}

However, this did not model the structure of type slices particularly well. Analysis slices add ids to \textit{all} sub-slices, giving \textit{linear} space complexity in the depth of the type.

\subsubsection{Incremental Slices}
Therefore, slices are represented incrementally. With \textit{incremental slices} for synthesis slice parts and \textit{global slices} for analysis slice parts.

A \textit{global slice} is only tagged once, then \textit{lazily} tagged to sub-slices if used. That is, when de-constructing types, e.g. in function matching. We get the type in \cref{fig:SliceType}:
\begin{figure}[h]
\centering
\begin{minted}[fontsize=\small]{reason}
...
and typslice_empty_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
]
and typslice_incr_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
  | `SliceIncr(typslice_typ_term, code_slice)
]
and typslice_term = [
  | `Typ(typ_term)
  | `TypSlice(typslice_typ_term)
  | `SliceIncr(typslice_empty_term)
  | `SliceGlobal(typslice_incr_term, code_slice)
]
and typslice_t = IdTagged.t(typslice_term)
...
\end{minted}
\caption{The type slice data-type}
\label{fig:SliceType}
\end{figure}

The invariant that a slice has at most \textit{one} incremental and/or global slice is maintained by splitting into three types (\code{empty_term}, \code{incr_term}, \code{term}). Regular un-sliced types \code{`Typ(...)} are maintained to provide easier interoperability with the rest of the code-base, also allowing type slicing to be turned off.

\textit{Polymorphic Variants} \cite[ch. 7.4]{RealWorldOCaml}, notated \texttt{[ | ... ]} are used to more conveniently write functions on slices. This is possible due \textit{row polymorphism} \cite{PolymorphicVariants} \cite[ch. 10.8]{ATTAPL} relating the variants by a \textit{structural subtyping} relation \cite{StructuralSubtyping}. We have that:\footnote{$x :> y$ meaning $x$ a subtype of $y$.} 
\[\code{typslice_empty_term} :> \code{typslice_incr_term} :> \code{typslice_term}\]

Type constructors are either co-variant or contra-variant \cite[ch. 2]{BasicCatTheory} with respect to the subtyping relation. For example, id tagging is covariant, so:
\[\code{IdTagged.t(typslice_incr_term)} :> \code{IdTagged.t(typslice_incr_term) = typslice_t}\] 
Functions are bifunctors: contravariant in their input and covariant in their output, for example:
\[\code{typslice_incr_term} \to \code{typslice_incr_term} :> \code{typslice_empty_term} \to \code{typslice_term}\]

This function subtyping property significantly reduces work in defining functions on slices (see \cref{fig:Apply}).
\paragraph{Utility Functions}
Functions on slices often only concern the underlying type, e.g. checking if a slice is a list type. Writing direct pattern matching code on $\code{typ_term}$ and $\code{typslice_term}$ is easier. An \code{apply} function can apply these direct functions to the term inside a slice. The bottom two branches can both be passed into \code{apply} function as they are sub-types of \code{typslice_term}.  
\begin{figure}
\begin{minted}[fontsize=\small]{reason}
let rec apply = (f_typ, f_slc, s) =>
  switch (s) {
  | `Typ(ty) => f_typ(ty)
  | `TypSlice(slc) => f_slc(slc)
  | `SliceIncr(s, _) => apply(f_typ, f_slc, s)
  | `SliceGlobal(s, _) => apply(f_typ, f_slc, s)
  }
\end{minted}
\caption{Apply Utility Function}
\label{fig:Apply}
\end{figure}
Many other utility functions are implemented, including mapping functions, wrapping functions, unpacking functions, matching functions.

\subsubsection{Type Slice Joins}
Type joins (\cref{sec:JoinTypesTheory}) are extensively used in the Hazel implementation for \textit{branching statements} and in the theory of contribution slices.

For basic type slices, when the same type information is available from multiple branches, highlighting only one branch is required, but both for contribution slices. See that the 1 in \cref{fig:TypeJoins} is not highlighted (in green). However, we did still need some information from the left (the 0).

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Media/Figures/typejoin}
\caption{Type Slice Joins}
\label{fig:TypeJoins}
\end{figure}

\subsection{Static Type Checking}\label{sec:TypeChecking}
Hazel is \textit{bidirectionally typed}, where the \textit{mode} (synthesis, analysis) is specified by the \code{Mode.t} type. Type checking calculates a type information object \code{Info.t} for each term, stored efficiently in a map from ID keys. \code{Info.t} is demonstrated in \cref{fig:Info} with arrows representing dependencies (e.g. a term's type depends on it's mode, self, typing context, and status).
\begin{figure}[h]
\includegraphics[width=1\textwidth, trim={8cm 5cm 8cm 5cm}, clip]{Media/Figures/info}
\caption{\code{Info.t} data-structure}
\label{fig:Info}
\end{figure}

\subsubsection{Self and Mode}
Slicing logic relating to synthesis slices and analysis slices is factored into \code{Self.t} and \code{Mode.t} respectively, cleanly segregated from the type checking code. Although, doing this still required full understanding of the type checker implementation, ensuring the correct IDs are sliced. Two examples of the slicing logic are shown in \cref{sec:slicingstaticsexamples}.

\subsubsection{Typing (Co-)Context}
The typing context and co-contexts are modified to use type slices. This deviates from the theoretical notion of an expression slice: the structural context in which the variable is used is untracked when passing through the context. Therefore, it requires using \textit{unstructured} code slices. It is useful in practice allowing slices calculated during binding to be retrieved usage, see \cref{fig:VarSlice}.
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{Media/Figures/var_slice}
\caption{Type slice for variable \code{x}: includes it's binding and slice.}
\label{fig:VarSlice}
\end{figure}

\subsubsection{Status and Type}
The status, mode, and self are combined to determine a term's actual type, being dynamic if there is an error. When the expectations (mode, self) are inconsistent, the inconsistent slice information parts are tagged to the error status; \cref{sec:SlicingAnalysis} retrospectively considers what slice information to be extracted here. Additionally, contribution slices can extract the static parts of synthesis slices here. 

\subsection{Integration}
To support the full Hazel language, type slices needed to implement many functions, for example: type substitution\footnote{Note: this is the only feature which does not currently retain type slices fully.}, type normalisation, weak-head normalisation, tracking sum types, various structural matching functions etc. Additionally almost every usage of types in the codebase had to be refactored to use type slices (which are so easily pattern matched upon) while ensuring slices correctly maintained.

\subsection{User Interface \& Examples}
The type slices of the expression at the cursor (in red) are highlighted, see \cref{fig:SliceExamples}. Error slices distinguish between the synthesis and analysis parts with blue and red, see the evaluation examples \cref{sec:EvalExamples}.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/none_syn}
\caption{\code{None} synthesises \code{IntOption} due to it's type definition.}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/fun_syn}
\caption{The function synthesises \code{[Int]}$\to$ \code{IntOption} due to it's \code{[Int]} annotation and that the match branches synthesis \code{IntOption}. Both branches provide the same type information, only one branch (the last) is highlighted.}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/list_ana}
\caption{The list input is expected to be an \code{[Int]} as it is applied to \code{hd} which is a function annotated with input type \code{[Int]}.}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/hd_syn}
\caption{The variable usage of \code{hd} synthesises \code{[Int]}$\to$ \code{IntOption} similarly (and also due to the binding).}
\end{subfigure}
\caption{Type Slices}
\label{fig:SliceExamples}
\end{figure}

\section{Cast Slicing Implementation}\label{sec:CastSlicingImplementation}
To implement cast slicing, replace casts between \textit{types} by casts between \textit{type slices}. Type slices are already type-indexed and retain all type information so can be used equivalently.

\subsection{Elaboration}\label{sec:Elaboration}
Cast insertion recursively traverses the unelaborated term, inserting casts to the term's statically determined type as stored in the \code{Info} data-structure and from the type as can be determined directly from the term. 

For example, list literals recursively elaborate it's terms and join their slices, inserting a cast to this join.

Ensuring that all the type slice information from the \code{Info} map is retained and/or reconstructed during elaboration was a meticulous and error-prone process.

\subsection{Cast Transitions}
\Cref{sec:HazelDynamics} gave an intuitive overview of how casts are treated at runtime. Type-indexed slices allows cast slices to be decomposed in exactly the same way. 

However, as Hazel only checks consistency between casts between \textit{ground types}, there are two rules where new\footnote{As opposed to being derived from decomposition.} casts are \textit{inserted} (ITGround, ITExpand). The new types are both created via a \textit{ground matching} relation which takes only the topmost compound constructor, for example ground functions \cref{fig:GroundFunction}. Relevant portions of the appendix are \cref{fig:groundtypes}, \cref{fig:instructions}, \cref{fig:groundmatch}.

As we already store type slices incrementally, the part of the slice which corresponds \textit{only} to the outer type constructor is the outer slice tag.

\begin{figure}
\[\tau_1 \to \tau_2 \match{\text{ground}} \dyn \to \dyn\]
\caption{Ground Matching List}
\label{fig:GroundFunction}
\end{figure}

\subsection{Unboxing}
When a final form (\cref{sec:HazelFinalForms}) has a type, Hazel often needs to extract parts of the term according to the type during evaluation. But due to casts and holes, this is not trivial \cite{LivePatternMatching}.

For example, if a term is a final form of type list, then it could be either:
\begin{itemize}
\item A list literal: \code{[1,2,3]}.
\item A list with casts wrapped around it: \code{[1,2,3]}$\scast{\code{[Int]}}{\code{[}\dyn\code{]}}$.
\item A list cons with indeterminate tail: \code{1::2::?}.
\end{itemize}
Additionally, when the input is not a list at all, it returns \code{DoesNotMatch}, used in pattern matching. 

Unboxing makes use of GADTs to allow for varying output type depending on the type that the final form is being unboxed upon.

\subsubsection{Hazel Unboxing Bug}
While writing the search procedure I found an unboxing \textit{bug} which would always \textit{indeterminately match} a cons with indeterminate tail with \textit{any} list literal pattern (of \textit{any} length), even when it is known that it could never match. For example a list cons \code{1::2::?} represents lists with length $\geq 2$, but even when matching a list literal of length 0 or 1 it would indeterminately match rather than explicitly \textit{not} match. 

Pattern matching checks if each pattern matches the scrutinee with the following behaviour, starting from the first branch:
\begin{itemize}
\item \textit{Branch matches?} Execute the branch.
\item \textit{Branch does not match?} Try the next branch.
\item \textit{Branch indeterminately matches?} Cannot assume the branch doesn't match so must stop evaluation here. The match statement is then indeterminate.
\end{itemize}
\Cref{fig:PatternMatchingBug} demonstrates a concrete example which would get stuck in Hazel, but does \textit{not} need to. I reported and fixed this, with my PR merged into the dev branch.

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{Media/Figures/unboxing_bug}
\caption{Pattern Matching Bug}
\label{fig:PatternMatchingBug}
\end{figure}


\subsection{User Interface \& Examples}
Type slices within casts can be selected from the evaluation result and displayed. This require reworking some of the dependencies of Hazel's model-view-update architecture to make sure the cursor has access to the code editor cell when inside the evaluation result cell.
\begin{figure}[h]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/simple_cast_error}
\caption{A simple cast error blaming the plus operator for requiring the integer cast.}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/map_cast}
\caption{A hole cast to \code{Int} due to a mapped function annotated with an \code{Int} input.}
\end{subfigure}
\begin{subfigure}{0.65\textwidth}
\centering
\includegraphics[width=1\textwidth]{Media/Figures/decompose_casts}
\caption{A decomposed cast. The input of the function takes only slice for the argument part \code{Int} of the function type $\code{Int} \to \code{Float}$.}
\end{subfigure}
\caption{Cast Slicing Examples}
\end{figure}
\section{Indeterminate Evaluation}\label{sec:IndetEval}
Dynamic errors include evaluation traces, aiding debugging \cite{TraceVisualisation}. Static type errors lack such traces, as ill-typed programs do not run. Seidel et al. [29] offer an OCaml algorithm using lazy, non-deterministic narrowing of holes to least specific values based on context (e.g., instantiating a hole in + as an integer). 

This section creates a framework for non-deterministic evaluation of indeterminate expressions by lazily performing hole substitutions using type information from dynamic casts. Unlike Seidel, this supports more language features (all of Hazel), any number of inputs (holes), and exhaustive generation of these inputs. Further, it is a general evaluation method, not limited to cast error searches. Specifics relating to cast errors, are covered in \cref{sec:SearchProcedure}.

This section covers the following, answering each question:
\begin{enumerate}
\item[\ref{sec:ResolvingNondeterminism}] How should we resolve the non-determinism in instantiating holes \textit{fairly}? How can the search order be abstracted? Unlike Seidel's approach, my implementation is \textit{fair}: exhaustively considering all possibilities, and allows search methods that avoid (unnecessary) non-termination.
\item[\ref{sec:IndetEvalAlgorithm}] Describes an algorithm for indeterminate evaluation. Is every possibility explored fairly?
\item[\ref{sec:ThreadingState}] How can \textit{per-solution} state be maintained irrespective of evaluation order?
\item[\ref{sec:HoleInstantiation}] Discusses hole instantiation and substitution. What does lazy instantiation actually entail, when exactly should a hole be instantiated? Which hole\footnote{There may be multiple.} should be instantiated in order to continue evaluation to make progress? How should holes be substituted with their narrowed values; the same hole may exist in multiple locations within the expression?
\item[\ref{sec:OneStepEvaluator}] How to use the evaluation abstraction (\cref{sec:EVMODE}) to perform just one evaluation step?
\item[\ref{sec:TypesForHoles}] Finally, I consider Hazel-specific problems. Once we know which hole to instantiate, how can we get it's \textit{expected type}? Hazel's lazy treatment of pushing casts into compound data types means not all such holes will be wrapped directly in casts. Additionally, the case of pattern matching is difficult, allowing holes to be \textit{non-uniformly} cast to \textit{differing types}. How can holes be instantiated in these situations?
\end{enumerate}
As always, a UI is implemented in \cref{sec:UIIndetEval}.

\subsection{Resolving Non-determinism}
\label{sec:ResolvingNondeterminism}
To model infinite non-determinism I create a monadic DSL with an explicitly tree/forest-based representation. The forest model allows for varying low level search traversals. The module type of combinators is in \code{Nondeterminism.Search}; it's underlying parametric type is \code{t('a)} with \code{'a} being the type of the solutions. \Cref{sec:SearchMethods} discusses the actual implementations of this interface, giving four searching procedures.

\subsubsection{Monadic Non-determinism}
\cref{sec:Nondeterminism} described a high level monadic framework for nondeterminism. I extend this with some extra functions:

\begin{itemize}
\item Standard \code{map} and \code{join} functions. Mapping transforms all possible solutions, but does not change if a candidate \textit{is} a solution.
\item \code{once : t('a) => option('a)}, extracts any one solution, if one exists. This can be used to efficiently model \textit{don't care} non-determinism, where if one possibility fails, then we know all others fail also.
\item \code{run : t('a) => Sequence.t('a)} produces a lazy list (Jane Street's \code{Base.Sequence}) of all solutions.
\item \code{guard : bool => t(unit)}, represents success. When chaining binds, if any guard binding fails, then the whole computation fails.
\item \code{ifte : m('a) => ('a => m('b)) => m('b)}. This corresponds to Mercury's interpretation of an if-then-else construct \cite{Mercury}. It is put to good use for computations which explain the failure of the conditional.
\end{itemize}

\subsubsection{Abstracting Search Order: Forest Model}
Typical stream-based models of non-determinism \cite{ListOfSuccess} only admit the possibility of depth-first search (DFS). Stream concatenation provides no way of remembering choice points and backtracking before finishing a computation. 

Instead, monadic non-determinism can be represented by forests \cite{Bunches}. A forest is a list of trees, and can be defined as a monad. Choice, similarly to streams, is performed by concatenating forests. Finally, in order to build tree structure, a \code{wrap} combinator can wrap a forest as a tree whose root leads branches to each tree in the original forest, see \cref{fig:Wrap}.

\begin{figure}[h]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\ 1\quad ; \quad 2 \quad ; \quad 3\ \right]\]
\caption{\code{let x = return(1) <||>} \code{return(2)} \code{         <||> return(3)}}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\vcenter{\hbox{\begin{forest}
[$\cdot$ [4] [5]]
\end{forest}}}\right]\]
\caption{\code{let y = wrap(return(4) <||> return(5))}}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\vcenter{\hbox{\begin{forest}[$\cdot$ [1] [2] [3]]\end{forest}}} \quad ; \quad\vcenter{\hbox{\begin{forest}[$\cdot$ [4] [5]]\end{forest}}}\right]\]
\caption{\code{let z = wrap(x) <||> y}}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\vcenter{\hbox{\begin{forest}[$\cdot$ [$\cdot$ [1] [2] [3]][$\cdot$ [4] [5]]]\end{forest}}}\right]\]
\caption{\code{wrap(z)}}
\end{subfigure}
\caption{Forests Defined Using \code{wrap}}
\label{fig:Wrap}
\end{figure}

Therefore, I extend the DSL with a \code{wrap : t('a) => t('a)} combinator. Here, \code{wrap} is abstract, the underlying implementation does not actually need to use a forest data structure.\footnote{Therefore, DFS can still be efficiently implemented with regular streams.}

These trees can be traversed in various orders, e.g. breadth-first search (BFS)\cite{BFSCombinators}, or bounded DFS. With this in mind, \code{run} now represents performing the search on the tree, returning the solutions in sequence.

In essence, \code{wrap} allows encoding some notion of \textit{cost}\footnote{The depth of the node in the tree.} to solutions, which can then affect the search order.

\subsubsection{Recursive Functions}
As OCaml is strict, defining infinite choices via recursion can lead to non-termination during definition. I define a shorthand lazy application function \code{apply(f, x)} by \code{return(x) >>= f}, represented infix by \code{|>-}. Provided that bind lazily applies \code{f},\footnote{Which it usually does, consider streams as an example.} recursive functions can be written directly resulting in infinite choices without OCaml's strictness leading to infinite recursion.

\subsection{A Non-Deterministic Evaluation Algorithm}
\label{sec:IndetEvalAlgorithm}
This section demonstrates how we can indeterminately evaluation to return all possible values. Demonstrated by \cref{fig:IndetEvalBlock} and with code extract \cref{fig:IndetEval}.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1.8cm and 2.5cm,
  every node/.style={font=\small},
  startstop/.style={rectangle, rounded corners, draw, minimum width=3.5cm, minimum height=1cm, text centered, fill=blue!10},
  process/.style={rectangle, draw, minimum width=3.5cm, minimum height=1cm, text centered, fill=orange!20},
  decision/.style={diamond, draw, aspect=2, text centered, inner sep=1pt, fill=yellow!20},
  arrow/.style={-Latex}
]

% Nodes
\node (start) [startstop] {Bind input to $d$};

\node (classify) [process, below=of start] {\texttt{take\_step}(d)};

\node (isValue) [decision, left=of classify] {Final value?};
\node (isIndet) [decision, below=of classify] {Indeterminate?};
\node (isStep) [decision, below right=of classify] {Can step?};

\node (retVal) [startstop, below=of isValue] {Return $d$ as result};

\node (instantiate) [process, below=of isIndet] {\texttt{instantiate}(d)};

\node (inst1) [process, left= of instantiate] {Instantiation 1};
\node (inst2) [process, below=of inst1] {Instantiation 2};
\node (instN) [below=of inst2] {$\vdots$};

\node (evalNext) [process, below=of isStep] {Step to $d'$};

\node (fail) [startstop, right=of classify] {Exception: \texttt{fail}};

% Arrows
\draw [arrow] (start) -- (classify);

\draw [arrow] (classify) -- (isValue);
\draw [arrow] (classify) -- (isIndet);
\draw [arrow] (classify) -- (isStep);
\draw [arrow] (classify) -- (fail);

\draw [arrow] (isValue) -- node[right] {Yes} (retVal);
\draw [arrow] (isIndet) -- node[right] {Yes} (instantiate);
\draw [arrow] (isStep) -- node[right] {Yes} (evalNext);

% Recursive loop from evalNext to start
\draw [arrow] (evalNext.south) |- ++(4,-1) |- (start.east);

% Instantiate forks
\draw [arrow] (instantiate.west) |- (inst1.east);
\draw [arrow] (instantiate.south) |- (inst2.east);
\draw [arrow] (instantiate.south) |- (instN.east);

% Loop back from each instantiation to start
\draw [arrow] (inst1.west) -- ++(-1,0) |- (start.west);
\draw [arrow] (inst2.west) -- ++(-1,0) |- (start.west);
\draw [arrow] (instN.west) -| ($(inst2.west)+(-1,0)$) |- (start.west);
\end{tikzpicture}
\caption{Block diagram of indeterminate evaluation to values}
\label{fig:IndetEvalBlock}
\end{figure}

Instantiation is implemented by a \textit{non-deterministic} function \code{instantiate}, discussed in detail in \cref{sec:HoleInstantiation}: \[\code{Instantiation.instantiate : Exp.t => m(Exp.t)}\]

Classifying a term $d$, into values, indeterminate terms, and expressions with a possible step is done by a (deterministic) function \code{take_step}, discussed in \cref{sec:OneStepEvaluator}: \[\code{OneStepEvaluator.take_step : Exp.t => TryStep.t}\] 

To ensure that the search tree has finite branching factor, possibly infinite choices must be wrapped, e.g. evaluation steps. To allow for multiple searching methods, the algorithm is placed within a \textit{functor}, which takes a searching method, \code{S : Search}.

\begin{figure}[h]
\begin{minted}{reason}
module Make = (S: Search) => {
  module Instantiation = Instantiation.Make(S);
  open S;
  open S.Infix;
  
  let rec values = (d: DHExp.t) : S.t(DHExp.t) => {
    let step = OneStepEvaluator.take_step(d);
    switch (step) {
    | BoxedValue => return(d)
    | Indet => 
      d |>- Instantiation.instantiate
        >>- values;
    | Step(d') => wrap(d' |>- values);
    | exception (EvaluatorError.Exception(_)) => fail
    };
  };
};
\end{minted}
\caption{Indeterminate Evaluation to Values}
\label{fig:IndetEval}
\end{figure} 

\subsection{Threading Evaluation State}
In order to track statistics for use in the evaluation, state must be threaded through indeterminate evaluation. State tracked includes, on a \textit{per-solution} basis:
\begin{itemize}
\item Trace length.
\item Number of and size of instantiations performed.
\end{itemize}

So instead of threading only the solutions, \code{DHExp.t}, I also thread the state, \\\code{(DHExp.t, IndetEvaluatorState.t)}, updating according depending on the branch taken.

Additionally, Hazel evaluates expressions in an global environment (\code{env : Environment.t}) which must also be passed down, but does not change after performing a step.

\subsection{Hole Instantiation \& Substitution}\label{sec:HoleInstantiation}
There are three key problems to solve in order to instantiate terms.

\subsubsection{Choosing which Hole to Instantiate}
\label{sec:ChooseHole}
An indeterminate term may contain \textit{multiple} holes or even \textit{no} holes. Which hole needs to be instantiated in order to \textit{make progress}?

When attempting to evaluate the indeterminate term some transitions rules require a sub-term to be concrete (e.g. a function during application). We chose to instantiate the hole that blocks the \textit{first} blocked transition rule. If latter holes were instantiated, the term might \textit{still} be unevaluable due to this first hole.

This is implemented using Hazel's evaluator abstraction (\code{EV_MODE}), separating this logic from the transition semantics. Therefore, hole choice logic will automatically update to any future changes in the transition semantics.

\subsubsection{Synthesising Terms for Types}
Suppose we know which hole to instantiate and to which type (\cref{sec:TypesForHoles}). How do we refine these holes fairly and lazily, to the \textit{least specific} value that allows evaluation to continue?

Base types must be instantiated directly to their (possibly infinite set of) values, for example:
\paragraph{Booleans:} \code{return(true) <||> return(false)}.
\paragraph{Integers:} A recursive definition using lazy application \code{|>-}, see \cref{fig:Integers}:
\begin{figure}[h]
\begin{minted}{reason}
let rec ints_from = n => return(n) <||> wrap(n + 1 |>- ints_from)
let nats = ints_from(0)
let negs = ints_from(1) >>| n => -n
let ints = nats <||> wrap(negs) 
\end{minted}
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\ n \quad ; \quad \vcenter{\hbox{\begin{forest}
[$\cdot$ [$n+1$] [$\cdot$ [$n + 2$] [$\cdot$ [$n+3$] [...]]]]
\end{forest}}}\right]\]
\caption{\code{ints_from(n)}}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\[\left[\ 0 \quad ; \quad \vcenter{\hbox{\begin{forest}
[$\cdot$ [$1$] [$\cdot$ [$2$] [$\cdot$ [$3$] [...]]]]
\end{forest}}} \quad ; \quad \vcenter{\hbox{\begin{forest}
[$\cdot$ [$-1$] [$\cdot$ [$-2$] [$\cdot$ [$-3$] [...]]]]
\end{forest}}}\right]\]
\caption{\code{ints}}
\end{subfigure}
\caption{Enumerating Integers}
\label{fig:Integers}
\end{figure}
 
\paragraph{Strings:} A string is either \textit{empty} or is a string with a first character from a finite set. We can recursively wrap all strings, prefixed by each character. See \cref{fig:Strings}:
\begin{figure}[h]
\begin{minted}{reason}
let chars = // Every single letter string considered
let rec strings = () => return("") 
  <||> wrap(chars >>= chr => 
            (() |>- strings) >>- str => 
            chr ++ str)) 
\end{minted}
\caption{Enumerating Strings}
\label{fig:Strings}
\end{figure}
\

Other types are \textit{inductive}, these can be represented indirectly by lazily instantiating only their \textit{outermost} constructor: 
 
\paragraph{Lists:} A list is either the empty list, $[\ ]$ or a cons $\dyn_1 :: \dyn_2$. Note that, to retain the correct dynamic type information, $\dyn_2$ must be cast back to the list type. 
\paragraph{Sum Types:} Enumerate each of the sum's constructors with their least specific value.
\paragraph{Functions:} Constant functions have least specific values $\lambda \_.\ \dyn$. The function may then be applied to any value, and it's result synthesised after application. This can synthesise any return value, hence errors in the usage of the function will be detected. But, if the \textit{input} has an erroneous type but is uncaught as the function is dynamic, potential errors arising from this cannot be found.\footnote{This can occur in dynamic functions with inconsistent branches. But this is extremely rare, and can be fixed for most cases by generating the identity function where function hole's expected type allow.} Extensions to program synthesis (\cref{sec:LogicProgramming}) would require more sophisticated function instantiation.


\subsubsection{Maintaining Correct Casts}
Every hole has a dynamic type at runtime. Therefore, that hole's context \textit{expects} it to have the dynamic type. Therefore, we must cast every instantiation back to the dynamic type.

\subsubsection{Substituting Holes}\label{sec:HoleSubstitutionImplementation}
Holes can be bound to variables in the execution environment, and may also be duplicated, before they are required to be instantiated, see \cref{fig:HoleDuplication}. Therefore, instantiations must be substituted for every occurrence of the same hole.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{Media/Figures/duplicate_hole}
\caption{Duplicated Holes}
\label{fig:HoleDuplication}
\end{figure}

Consistent hole substitution was described as part of the Hazel calculus \cref{sec:HoleSubstitution}. Unexpectedly, the main Hazel branch did not yet implement it. A full implementation of metavariables and delayed closures is complex. Therefore, as hole closures are not required for hole instantiation,\footnote{Currently, there is no need to instantiate to a variable reference.} I instead use the existing term ids to represent metavariables, and ensure these ids are maintained and propagated correctly throughout statics, elaboration, and evaluation.\footnote{A huge portion of the code-base required checks.}

We can substitute terms for every hole with the given ID throughout the entire expressions. Substitutions within closures \textit{include} eagerly evaluating the resulting binding to ensure the invariant that closures bind variables to values. 

\subsection{One Step Evaluator}\label{sec:OneStepEvaluator}
The step evaluator uses the evaluation abstraction to create an \textit{evaluation context} from a term, to return:
\begin{itemize}
\item An evaluation context, with a \textit{mark} inserted in place of the sub-term which is evaluated according to the transition rules.
\item The environment under which the sub-term is evaluated.
\item The kind of rule that was used (e.g. substitution, addition, etc.).
\end{itemize}

This information allows indeterminate evaluation methods to treat different steps differently. For example one might wish to return only the substitution steps, to produce a compressed execution trace.

\subsection{Determining the Types for Holes}
\label{sec:TypesForHoles}
If we know which hole to instantiate, how do we know which type to instantiate it to?

For efficiency, my implementation both determines \textit{which hole}, and it's \textit{type information} during the same pass.

\subsubsection{Directly from Casts}
Most of the time, a hole is directly surrounded by a cast, whose type information can be used to perform an instantiation.

\subsubsection{Cast Laziness}\label{sec:CastLaziness}
However, this is \textit{not} always the case. For efficiency reasons, Hazel treats casts over compound data-types lazily, e.g. casts around tuples will only by pushed inside upon usage of a component of the tuple.

Treating casts eagerly is a significant change to the Hazel semantics, so was opted against. \Cref{sec:EvalCastLaziness} discusses the consequence of this choice. 


\subsubsection{Pattern Matching}
\label{sec:PatternMatching}
Does a hole even have only one possible type? The introduction of (dynamic) pattern matching actually allows terms to be matched against \textit{non-uniform} types if the scrutinee is dynamic. In \cref{fig:DynamicPatternMatching}, if \code{term} is an integer 0 then it returns 0, but if it's the string "one" then it returns 1. Hence instantiating \code{code} to either an int or string might allow progress.

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{Media/Figures/dynmatch}
\caption{Dynamic/Type-case Match Statement}
\label{fig:DynamicPatternMatching}
\end{figure}

Hazel implements this by placing casts on the \textit{branches}, rather than the scrutinee. Therefore, we can collect each of these possible types from the casts, and wrap them around the scrutinee, continuing evaluation accordingly.

\subsubsection{Extended Match Expression Instantiation (Pattern Instantiation)}
\label{sec:ExtendedPatternMatching}
An interesting extension was partially implemented which improves code coverage and additionally detects errors within patterns. 

It instantiates holes in a match expression according also to the \textit{structure} of each pattern, allowing the instantiation to prioritise searching along each branch. We instantiate the scrutinee with the least specific versions which match the patterns on each branch, e.g. \code{?::?} for \code{x::xs}. However, sometimes this instantiation is not specific enough to explicitly not match with previous statements, hence getting stuck. Proposed solutions can be found in \cref{sec:extendedmatching}.

\subsection{User Interface}\label{sec:UIIndetEval}
The default evaluation method returns every possible indeterminate or concrete values. The possibilities can be cycled through via some arrows buttons.

\section{Search Procedure}\label{sec:SearchProcedure}
Now that an framework for indeterminate evaluation has been specified, the following problems can be addressed:
\begin{itemize}
\item[\ref{sec:AbstractSearch}] How can indeterminate evaluation be abstracted into a generic search procedure?
\item[\ref{sec:CastFailureDetection}] What exactly are cast errors? How can they be detected? Which ones are actually \textit{relevant}, causing evaluation to get stuck?\footnote{Some cast errors are known, e.g. from static type checking, but don't actually relate to the evaluation path.}
\item[\ref{sec:SearchMethods}] How can different search methods be implemented: DFS, BFS, Bounded DFS, Interleaved DFS? 
\end{itemize}

\subsection{Abstract Indeterminate Evaluation}
\label{sec:AbstractSearch}
We reuse the previous framework (\cref{fig:IndetEvalBlock}), but abstract the the return logic to a higher-order \code{logic} function. This also allows returning other types, for example, the integer \textit{size} of values. Or return only specific classes of expressions, e.g. those with cast errors. An `expert' search abstraction is also provided which allows instantiation to be changed.

\subsection{Detecting Relevant Cast Errors}
\label{sec:CastFailureDetection}
To search for cast errors, we must first define what one is. A reasonable definition is terms which contain \textit{cast failures}, in Hazel these are casts between \textit{inconsistent ground types}. However, this has some issues:

\paragraph{Multiple Cast Failures:} Terms may have multiple cast failures, some of which discovered during static type checking and inserted via elaboration. But these failures won't stop evaluation until necessary. Therefore, we should consider only the casts which are directly \textit{causing} a term to get stuck, this is implemented similarly to choosing which hole to instantiate (\cref{sec:ChooseHole}).

\paragraph{Cast Laziness}
\label{sec:SearchCastLaziness}
Only casts between \textit{ground} types are checked for consistency. Due to cast laziness (\cref{sec:CastLaziness}), some compound terms will be cast between inconsistent types, but \textit{not} placed within a \textit{cast failure}. As before, this issue is ignored due to requiring large changes to Hazel semantics, the evaluation finds it to be a rare occurrence.

\paragraph{Dynamic Match Statements:} When matching dynamically on values with different types, the instantiations wrap the scrutinee in casts to each type. If any of these casts failed, they should not count as witnesses, as they were introduced entirely by the instantiation procedure. 

\subsection{Explaining Static Errors}
\label{sec:StaticCastError}
A static type error will place a term inside a cast error during elaboration. If the resulting cast error is dependent on the statically inserted cast error, then the resulting cast slice and trace can be considered as an explanation for the static error.

\subsection{Searching Methods}\label{sec:SearchMethods}
I implement \textit{four} different search methods, implementing the non-determinism signature specified in \cref{sec:ResolvingNondeterminism}.

\subsubsection{Depth First Search}
Modelling DFS by streams is a typical method: implementing choice and conjunction via appending leads to depth-first search. In which case, \code{wrap} is just the identity function, there is no tree structure internally.

\subsubsection{Breadth First Search}
Breadth first search represents forests by sequences of sequences \cite{BFSCombinators}, with the inner sequences being the concatenation of every node of that level in every tree in the forest. Then, \code{choice} concatenates each level. While \code{wrap} conses the empty list, that is, pushes down every level of solutions and inserts an empty level at the top.

The other monadic operators are complex, but \code{join : t(t('a)) => t('a)} can be shown visually. \Cref{fig:BFSJoin} demonstrates how to flatten trees of trees, then you can join a forest by folding this tree join over the trees in the forest.

\begin{figure}[h]
\vcenter{\hbox{\begin{subfigure}{0.45\textwidth}
\centering
\begin{tikzpicture}[scale=0.8, transform shape,
  main tree/.style={grow=right, sibling distance=3cm, level distance=3cm},
  sub tree/.style={grow=right, sibling distance=1cm, level distance=1cm},
  every node/.style={circle, draw, minimum size=6mm, inner sep=0pt},
  main node/.style={draw=red, thick, fill=red},
  sub node/.style={draw=blue, fill=blue},
  invisible/.style={draw=blue, fill=none, text=none}
  ]

% Main tree in x–z plane
\begin{scope}[canvas is xz plane at y=0, main tree]
% Level 1 rectangle
    \fill[cyan!40, fill opacity=0.5, rounded corners] (2.5,-2.8) rectangle (3.5,2.8);
    % Level 2 rectangle
    \fill[green!40, fill opacity=0.5,  rounded corners] (5.5,-4) rectangle (6.5,1);
\node[main node] (root) {}
  child {node[main node] (a) {}
    child {node[main node] (a1) {}}
    child {node[main node] (a2) {}}
  }
  child {node[main node] (b) {}
  };
\end{scope}

% Subtrees with rectangles directly inside

% Depth 3 subtrees
\foreach \n in {root} {
  \begin{scope}[shift={(\n)}, canvas is yz plane at x=0, sub tree]
    % Level 1 rectangle
    \fill[cyan!40, fill opacity=0.5, rounded corners] (0.4,-1.2) rectangle (1.5,1.2);
    % Level 2 rectangle
    \fill[green!40, fill opacity=0.5,  rounded corners] (1.5,-0.6) rectangle (2.6,1.6);
    
    \node[invisible] {}
      child {node[sub node] {}}
      child {node[sub node] {}
        child {node[sub node] {}}
        child {node[sub node] {}}
      };
  \end{scope}
}

% Depth 2 subtrees
\foreach \n in {a, b} {
  \begin{scope}[shift={(\n)}, canvas is yz plane at x=0, sub tree]
    % Level 1 rectangle
    \fill[green!40, fill opacity=0.5, rounded corners] (0.4,-1.2) rectangle (1.6,1.2);
    
    \node[invisible] {}
      child {node[sub node] {}}
      child {node[sub node] {}};
  \end{scope}
}

\end{tikzpicture}
\caption{Before Join}
\end{subfigure}}}
$\quad\mapsto\quad$
\vcenter{\hbox{\begin{subfigure}{0.45\textwidth}
\begin{tikzpicture}[scale=0.8, transform shape,
  main tree/.style={grow=right, sibling distance=3cm, level distance=3cm},
  level 2/.style={sibling distance=1cm},
  sub tree/.style={grow=right, sibling distance=1cm, level distance=2cm},
  every node/.style={circle, draw, minimum size=6mm, inner sep=0pt},
  main node/.style={draw=red, thick, fill=red},
  sub node/.style={draw=blue, fill=blue},
  invisible/.style={draw=blue, fill=none, text=none}
  ]

% Main tree in x–z plane
\begin{scope}[canvas is xz plane at y=0, main tree]
% Level 1 rectangle
    \fill[cyan!40, fill opacity=0.5, rounded corners] (2.5,-5.4) rectangle (3.5,5.4);
    % Level 2 rectangle
    \fill[green!40, fill opacity=0.5,  rounded corners] (5.5,-7) rectangle (6.5,6);
\node[main node] (root) {}
  child {node[main node] (a) {}
    child {node[main node] (a1) {}}
    child {node[sub node] (a3) {}}
    child {node[main node] (a2) {}}
    child {node[sub node] (a4) {}}
  }
  child {node[sub node] (c) {}
  }
  child {node[main node] (b) {}
    child {node[sub node] (b1) {}}
    child {node[sub node] (b2) {}}
  }
  child {node[sub node] (d) {}
    child {node[sub node] (d1) {}}
    child {node[sub node] (d2) {}}
  };
\end{scope}
\end{tikzpicture}
\caption{After Join}
\end{subfigure}}}

\caption{Tree Join}
\label{fig:BFSJoin}
\end{figure}
The standard definition for \code{bind} \cite{Bunches} does not work easily in OCaml due to it's strictness. Defining joins directly ensured that bind acted lazily.

\subsubsection{Bounded Depth First Search}
BFS is fair, avoiding non-termination (for finite branching factor), but it has \textit{exponential} space complexity in the depth of the level being explored \cite{NorvigAI}.

In comparison, DFS only requires space linear in the depth explored.

The use of bounded DFS (BDFS), successive depth-bounded depth first searches, retains low space complexity of DFS while also avoiding non-termination. While upper parts of are repeatedly explored, this does not increase the already exponential time complexity of the search.\footnote{For branching factor greater than 1.} 

We can use functions \code{Int => (Bool, [('a, int)])} to represent iterative deepening \cite{SearchAlgebra}. This calculates every solution found within an integer depth bound \code{d}, alongside it's remaining depth budget \code{r}. The boolean tag is false if the depth bound was never reached, in which case, no more solutions exist. The remaining depth budget allows only solutions on the fringe (zero budget) to be output upon each iteration, so the same solution will not appear twice.

Then, \code{return} represents a single solution with unused depth budget:\\
\code{return(x)  =  (d => (false, [(x, d)]))}. \code{fail} gives the empty list \code{fail  =  d => []}. Choice appends all solutions at any given depth bound \code{m <||> n  =  d => m(d) @ n(d)}. Binding, \code{m >>= f} takes solutions in \code{m} who have depth budget to use, and applies \code{f} and calculates all solutions from \code{f} using the remaining budget, concatenating these results to form a single list.

Then \code{wrap(m)} (the forest \code{m} wrapped up as a single tree) has solutions at depth incremented by 1, and none at depth 0: \code{wrap(m)(0) = []} and \code{wrap(m)(bound) = p(bound - 1)}.

I implement this as a functor which allows the how depth bound increases to be specified.


\subsubsection{Interleaved Streams}
The use of streams, but with fair choice and conjunction via interleaved ordering ensures termination for every solution. This has the advantage of working even for trees with infinite branching factor. However, interleaving has a linear space complexity, leading to the undesirable exponential space complexity as with breadth-first search. For this, \code{wrap} is the identity, as with DFS.

\subsection{User Interface}
The user can switch the search procedure on via a switch in the settings.

\section{Repository Overview}
\subsection{Branches}
Up to date with dev branch up to \textbf{DATE}

\subsection{Hazel Architecture}



