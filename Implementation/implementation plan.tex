\chapter{TEMPORARY: Implementation Plan}
\section{Cast Slicing}
\subsection{Theoretical Foundations and Context}
\subsubsection{Context: Why are casts elaborated}
See The Gradualizer \cite{Gradualizer} for details on this terminology. It doesn't perfectly fit Hazel but is close.
A cast is inserted at a point of either, for a subexpression $e : \tau$ if either:
\begin{itemize}
\item Pattern Matching $\tau \blacktriangleright \tau'$ -- Wrap $e$ in a cast $\scast{\tau}{\tau''}$ where $\tau''$ is the \textit{cast destination of $\tau'$}
\item  Flows $\tau \rightsquigarrow \tau'$ -- Wrap $e$ in a cast $\scast{\tau}{\tau'}$ -- These correspond to uses of type consistency.
\end{itemize}
A \textit{cast destination} for $\tau$ is the result of applying flows recursively on all positive type positions and reversed flows on negative type positions in $\tau$.\par
Flows relate to invocations of consistency and joins, with direction dictated by type polarity and modality:
\begin{itemize}
\item Producers flow to their final type 
\item Final types flow to the consumers
\item Input variables are replaced with the final type.
\end{itemize}
Where the final type is an annotated \textit{type}, output consumers, or the join of all producers.\footnote{Therefore types generally have output mode.}
\par 

\subsubsection{Type slicing a term}
\textbf{A logical interpretation of this would be useful (see the relation of contexts slices and type slice decomposition in implications/functions). The below formulation feels quite ad-hoc.}
\par
A \textit{type slice} of an $e$ in a context is an expression that has sub-expressions of $e$ replaced with holes or type annotations replaced with the dynamic type but still type synthesises or analyses to the same type.\footnote{Additionally they contain other info for use decomposing slices and the encompassing. Discussed throughout.}\par 
\textit{An alternative formalisation could be: Instead of type synthesises or analyses the same type, it could contain all code that could be changed in order to NO LONGER synthesise/analyse to the type.}\par
However, as typing often depends on the context, for which code involved may be outside the scope of the current term, an additional notion of a context slice will be attached. A context slice will contain all the variable bindings (lambdas or let bindings) and, importantly, their annotations. It is important to notice that this only works because Hazel is explicitly typed at variable bindings. \textit{An implicitly type language, like ML, could be translated to an explicitly typed internal language, for example CoreXML \cite{CoreXML}. Then, `constraint slices' could track code which is required for the constraint system to derive each annotation; this idea is similar to that used in type error slicing for higher-order functional languages \cite{ErrSlice, HaackErrSlice}.}
\par 
Slices for compound types will need to be split into slices for component types (as casts are decomposed), a suitable extension of this needs to be produced that allows slices to be decomposed into the slices of component types.\par
Compound types generally involve type checking sub-terms are of the component types, so these component slices can be obtained during this. But,
for types involving variable binding, e.g. function types $\tau = \tau_1 \to \tau_2$, the slice of the argument $\tau_1$ should simply be the binding annotation, in a sense this is the extension of the context slices between $\tau$ and $\tau_2$\footnote{A logical interpretation or analogy of this would be useful}.\\ \par 
Importantly, a hole analyses against any type, so any expressions analysed against can be replaced by holes\footnote{\textbf{FIX}: if using analysis on a non-subsumable type then maybe all non-subsumption rules should also construct slices.}.\par 
On the other hand, synthesis rules may require expanding a slice. Mostly, by composing component slices as discussed above.
\par 
A formal definition of this is given below. Treating a slice $\varsigma$ of $e,\tau$ as a pair of a context slice and expression slice $(\gamma, \varepsilon)$ indexed by $\tau$. The context slice $\gamma$ is simply a subset of the typing context $\Gamma$ and the slice $\varepsilon$ is a term $e'$ which is less precise\footnote{Ideally, least precise} than $e$ and synthesises $\tau$ under the context slice. For simplicity adding the notion of unsubstitutable, unnamed holes $\hole$: 

\begin{figure}[H]
\small
Syntax:
\[\varsigma ::= \tau[\gamma, \varepsilon] \mid (\varsigma \to \varsigma) [\gamma,\varepsilon]\]
\fbox{$\synthesisSlice{e}{\tau}{\varsigma}$}\ \ \ $e$ synthesises type $\tau$ under context $\Gamma$ and produces slice $\varsigma)$
\tiny
\[\inference[\tiny SSConst]{}{\synthesisSlice{c}{b}{b[\cdot, b]}} \quad
\inference[\tiny SSVar]{x : \tau \in \Gamma}{\synthesisSlice{x}{\tau}{\tau[x:\tau,x]}}\]
\[ 
\inference[\tiny SSFun]{\synthesisSlice[\Gamma,x:\tau_1]{e}{\tau_2}{s[\gamma, \varepsilon]}}{\synthesisSlice{\lambda x:\tau_1.e}{\tau_1 \to \tau_2}{(\tau_1[x:\tau_1,\hole] \to s[\gamma, \varepsilon])[\gamma\backslash\{x:\tau_1\},\lambda x:\tau_1. \varepsilon]}}
\]
\[
\inference[\tiny SSApp]{\synthesisSlice{e_1}{\tau_1}{\varsigma_1} & \tau_1 \funmatch \tau_2 \to \tau \\
\varsigma_1 \funmatch (\varsigma_2 \to s[\gamma,\varepsilon])[\gamma_1, \varepsilon_1] & \analysis{e_2}{\tau_2}}{\synthesisSlice{e_1(e_2)}{\tau}{s[\gamma_1, \varepsilon_1(\hole)]}} \quad 
\inference[\tiny SSEHole]{}{\synthesisSlice{\hole^u}{?}{?[\cdot, \hole]}} 
\]
\[
\inference[\tiny SSNEHole]{\synthesis{e}{\tau}}{\synthesisSlice{\hole[e]^u}{?}{?[\cdot, \hole]}}\quad 
\inference[\tiny SAsc]{\analysis{e}{\tau}}{\synthesisSlice{e : \tau}{\tau}{\tau[\cdot, \hole : \tau]}}
\]
\caption{Bidirectional typing judgements for \textit{sliced} external expressions}
\label{fig:typingsliced}
\end{figure}
Defining a slice matching relation analogously to type matching $\funmatch$.\par
Notice that for types involved with the typing context, like function arguments, where no such term with type $\tau_1$ actually exists in code; the context slice gives required all info and the expression slice is filled with a dummy hole. \textbf{Formalise this better: could consider a term of type $\tau_1 \to ?$. Alternatively this may be better expressed as a transformation of DERIVATIONS themselves.}\par 

For implementation and user purposes, the assumptions in the context slice $\gamma$ derive from annotations, whose location in code would be tracked. This could be formalised in the above rules by annotating type assumptions with locations, or by considering a context slice could as a term with holes at every subterm except for the relevant bindings.\par 
 

\subsubsection{Slicing of Casts}
Casts always go from expression types (for which a type slice exists) to either final types or consumers.\par 
Producer types correspond to synthesised types. Consumer types correspond to analysed types. Final types are one of the above or additionally, joins of (producer) types.\par 
\par 
Hence, as producer and consumers originate from type checking, their type slices can be obtained. The only remaining possibility is joins of types, so joins of type slices must be created.\par 
First, I show how slices can be applied to elaboration rules, with casts now being between type slices $\scast{\varsigma_1}{\varsigma_2}$. Notably, additional slicing is required in analysis mode, but not in synthesis mode. When in analysis mode, the slice for $\tau'$, that is the actual internal type of the elaborated expression, is required. When in synthesis mode, the slice for $\tau$ can simply be obtained via the external type slicing. 

\par Formal definition below. Omitting the trivial synthesis cases not involving casts.
\begin{figure}[H]
\small
Redefining elaboration synthesis:
\tiny
\[\inference[\tiny ESApp]{\synthesisSlice{e_1}{\tau_1}{\varsigma_1} & \tau_1 \funmatch \tau_2 \to \tau & \varsigma_1 \funmatch (\varsigma_2 \to \varsigma)[\gamma_1, \varepsilon_1]\\
\elaborationAnalysisSlice{e_1}{\tau_2 \to \tau}{d_1}{\tau_1'}{\Delta_1}{\varsigma_1'} & \elaborationAnalysisSlice{e_1}{\tau_2}{d_2}{\tau_2'}{\Delta_2}{\varsigma_2'}}{\elaborationSynthesis{e_1(e_2)}{\tau}{(d_1\scast{\varsigma_1'}{\varsigma_1})(d_2\scast{\varsigma_2'}{\varsigma_2})}{\Delta_1 \cup \Delta_2}}
\]
\[\inference[\tiny ESAsc]{\synthesisSlice{e}{\tau}{\varsigma} & \elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}}{\elaborationSynthesis{e:\tau}{\tau}{d\scast{\varsigma'}{\varsigma}}{\Delta}}\]
\small
\fbox{$\elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}$}\ \ \ $e$ analyses against type $\tau$ and elaborates to $d$ of consistent type $\tau'$ with slice $\varsigma'$
\tiny
\[
\inference[\tiny SEAFun]{\tau \funmatch \tau_1 \to \tau_2 \\ \elaborationAnalysisSlice[\Gamma,x:\tau_1]{e}{\tau_2}{d}{\tau_2'}{\Delta}{s[\gamma, \epsilon]}}{\elaborationAnalysisSlice{\lambda x. e}{\tau}{\lambda x:\tau_1. d}{\tau_1 \to \tau_2'}{\Delta}(\tau_1[x:\tau_1,\hole] \to s[\gamma, \varepsilon])[\gamma\backslash\{x:\tau_1\},\lambda x. \varepsilon]}
\]
\[
\inference[\tiny SEASubsume]{e \neq \hole^u & e \neq \hole[e']^u & \synthesisSlice{e}{\tau'}{\varsigma'}\\ \elaborationSynthesis{e}{\tau'}{d}{\Delta} & \tau \sim \tau'}{\elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}}
\]
\center
SEAEHole, SEANEHole are simply analogues of the synthesis slicing rules.
\caption{Elaboration judgements with slicing}
\label{fig:elaborationsliced}
\end{figure}

Second, I describe how to calculate joins of slices. This depends on the formalisation of type slicing. If only a minimal code slice to produce the same type is required, then some branches with matching type may be replaced by holes. If we include any code that can cause type checking to change, a join of slices is just the union of the code they point at\footnote{Prove.}.\par Semantically, in the above formulation it is a bit tedious to define and doesn't quite fit the pattern that the type slice actually corresponds to a term, here it corresponds to a union of branches. \textbf{The idea of a type slice should be refined to accept function argument types and joined types more intuitively}\footnote{Consider having join types be explicit. The lack of a link between these types and an expression is the issue.}.
\par 
Implementation-wise, slices will probably be code locations, and will be more granular than expressions.\footnote{Hence, the issues above are irrelevant to implementation.} Granularity at the level of annotations, and could potentially have special cases to distinguish cases where the slicing semantics inserts holes more clearly: for example, highlighting the brackets/hole being applied to a function in SSFun to make it clear that the type derives from \textit{application} rather than just the function which has a very similar slice. \textit{The hole based approach does have the benefit of automatically closing away irrelevant branches of code that is not highlighted, this could be a nice interaction design feature to implement.}

\subsubsection{Context: How are casts evaluated}
The Hazel cast calculus has two primary types of casts:
\begin{itemize}
\item Injections -- Casts to the dynamic type from a ground type
\item Projections -- Casts from the dynamic type to the dynamic type
\end{itemize}
These types of casts can be eliminated upon meeting or transformed into a cast error if the two involved ground types are inconsistent.\par 
Inserted casts don't necessarily fall into either of these two classes. If the cast is a non-ground injection/projection then they can be transformed by matching to a least specific ground type, e.g. $\tau_1 \to \tau_2 \groundmatch ? \to ?$. This ground type can be inserted in the middle of such a cast (ITGround, ITExpand). If the cast is between compound types and reducible by a suitable elimination rule (if casts were ignored), then these casts will be decomposed. \par 
See \cite{GradualizerDynamic} for in depth intuition.

\subsubsection{Cast Splitting}
There are only 3 rules that non-trivially modify casts: ITAppCast, ITGround, ITExpand. For ITGround and  ITExpand, it makes sense to keep the cast slices for $\tau'$ the same as $\tau$.\footnote{Justify this. Maybe casting to ground type was not in code so the cast should not highlight anything, but instead record it's \textit{dependencies}?} ITAppCast requires decomposing the function cast, luckily slices recursively include type slices of their  type components.
\[\inference[ITAppCast]{\tau_1 \to \tau_2 \neq \tau_1' \to \tau_2'}{d_1\scast{\varsigma_1 \to \varsigma_2}{\varsigma_1' \to \varsigma_2'}(d_2) \to (d_1(d_2\scast{\varsigma_1'}{\varsigma_1})\scast{\varsigma_2}{\varsigma_2'})}\]

\par 

Recording dependencies between casts from this might be useful. This allows a user to see how a cast was formed.


\subsubsection{Random notes and refs}
Consider options of: annotating typing derivations, annotating casts (and splitting in evaluation), or reverse unevaluating casts \cite{FunctionalProgExplain}.\par
See constraint free error slicing \cite{ConstraintFreeErrSlice}.\par
\textbf{See the gradualizer to understand how casts work }\cite{Gradualizer}.\par
See Blame tracking to see how common type location transfer is handled \cite{Blame}.\\ \par
Casting:\par 
Casting goes from the terms actual type to coerce it into a new type. Casts are inserted by elaboration at appropriate points (where the type system uses consistency or pattern matching\footnote{See the gradualizer for intuition and direction of consistency casts}). Casts between compound types may be decomposed by the cast calculus. See gradualizer dynamic semantics \cite{GradualizerDynamic} for intuition and \cite{Blame} for intuition on polarity affecting blame. \par 
Consider deeply type constructor polarities \cite[pg.~473]{TAPL}. Covariant types give positive blame (blame value) and contravariant give negative (blame context).


\subsection{Implementation Structure}
Research Into Hazel.

\subsection{Implementation Details and Optimisations}
\subsubsection{Slices -- Location based}
Location based slices would could be represented by intervals.\par 
Due to the recursive nature of slices (component type slices must be available), then there will be a significant number of interval trees. Therefore, a persistent structure \cite[chapter~2]{PurelyFunctionalDataStructures} MUST be used for space efficiency.\par 
Efficiently displaying overlapping intervals is ideal. A sorted list achieves this, so a sorted  tree structure of some sort could be used to maintain efficient insertion.\par 
Hash consing may be similarly useful.\par 
Displaying a slice here does need to be more efficient as there may be many intervals that aren't actually merged vs just walking a tree with holes.


\subsubsection{Slices -- Hole Based}
Slices can be stored immutably as ASTs and splitting of slices will be handled in a space efficient way by default by OCaml's persistent nature.\par 
Overlapping slices can appear, most notably from joining slices. Therefore, joins should be implemented to preserve space. Speed of merging and splitting is also relatively important.\par 
Displaying a slice is a relatively uncommon procedure compared to joining and splitting. Hence, retrieving and aggregating locations from the slice does not need to be fast, nor space efficient. \par 
A regular persistent tree is likely sufficient, but lazy joining and/or hash consing \cite{HashCons} might be useful.


\section{Search Procedure}
\textit{Look into Zippers \cite{Zipper}, and other functional data structures that may be useful here.
}
\subsection{Hole Refinement}
A suitable notion of \textit{type hole} should be used for this. These will differ from regular holes in that they will maintain runtime type information\footnote{Or the evaluation environment will track these.}. In particular they will be annotated by a dynamically inferred type variable, which would be progressively refined.\par
In particular the type given to a hole will be \textit{entirely} determined by the dynamics, and not the static type checking, which may not be well-typed or may be unspecific (using dynamic types).
\par
\textit{Look into Gradual type inference \cite{GradualTI}, Gradual dynamic type inference \cite{DTI}, Seidel et al. \cite{SearchProc}, OLEG \cite[chapter~2]{OLEG} \& Idris for ideas.  
Look into how Hazel may support type-driven code completion.}
\subsection{Hole Instantiation}
Substitution semantics via CMTT contextual substitution.\par 
Generating of values to substitute will match the required type as annotated on the hole. These should be the most general partial values to meet the type, see OLEG refinement \cite[chapter~2]{OLEG}.\par
Generation of suitable concrete values of any type should be informed by ideas like the `small scope hypothesis' \cite{SmallScope}\footnote{This is evaluated for bugs in general in OOP, not necessarily type bugs in FP.} or just could be random as in QuickCheck \cite{QuickCheck}, SmallCheck \cite{SmallCheck}. Allowing some meta-programming of the value generation algorithms would be an interesting idea, especially for user-defined types.\par
The idea of instantiating in order to effectively explore branches is a good idea. This is essentially symbolic execution, and exploring branches space-time-efficiently without overlap is the goal.\footnote{This is the main idea to think about, probably more important than user control of value generation.} \par 
Some heuristics may help with this search. But this project is unlikely to consider any in detail.\par
\textit{Note, both hole refinement and instantiation can be handled as a search procedure wrapper around the stepper.}
\subsection{Witness Generality}
A witness is a hole instantiation that evaluates to a final form containing a cast error(s). \textit{Witness generality} should ensure that generating a witness via the search procedure implies that for any type, a witness exists of this type.\par 
To achieve this, holes should only be instantiated when absolutely required, i.e. lazily as in SmallCheck \cite{SmallCheck}. For example, even if a function has a cast requiring integer arguments, the hole instantiation should be delayed, as passing a hole is still safe. When the hole passes into the function it will accumulate a cast to int, but the inferred type should remain ambiguous until evaluation gets stuck (such as at a case analysis). If this inferred type conflicts with the casts, hole instantiation to the inferred type may still be useful, and Hazel still supports evaluation.\par 
Alternatively, annotations could be treated as correct, meaning casts would refine the inferred type, and thereby treat any value instantiation at this point as a witness. Note that the use of dynamic types still allows the procedure to find general witnesses.

\subsection{Backtracking \& Non-determinism}
If a particular instantiation does not result in a witness, then some form of backtracking is required. This is the primary addition to the small-step evaluator that needs to be added.\par 
A simple way to do this would be wrapping the evaluator in a choice-pointing system -- like in Prolog. \par 
Doing this space-efficiently (saving choice points) would be the goal.\par 
\textbf{Look into non-deterministic evaluation from a logic programming perspective. Also backtracking in general.} See the Warren Abstract Machine \cite{WAM}.\par 
A \textit{semi-persistent} data structure \cite{SemiPersistent} will be needed for this.\par 
In order to find witnesses that are shorter in trace length (and hence easier for the user to understand), it may be useful to perform iterative deepening. Otherwise, trace compression and input simplification (as in QuickCheck \cite{QuickCheck})
\par 
For efficiency reasons, it may be desirable to use some amount of immutability here.\footnote{Hazel code-base rules seem to not allow this though!}

\subsection{Cast Dependence}
A cast error for associated with a witness should depend upon the value instantiated. Unrelated cast errors are not proof that the witness is valid.\par 
This notion of dependence can be modelled by counting only casts that are attached to sub-terms of an instantiated value. \textit{If an unrelated cast error causes evaluation to get stuck, this could be reported in some way to the user.}\par 

\subsection{Evaluation Order, Curried Functions \& Attached Search Holes}
\textit{Making this work for multiple search holes in arbitrary position might be too difficult. The other option is to only support this attached to functions.}\par
It is inefficient to evaluate the whole program in order to perform the search procedure. Instead evaluation should be directed first around search procedure holes\footnote{Of course, only possible because Hazel is pure.}. A key challenge with this is to decide which possible evaluation steps are related to the search hole and worth exploring.\par 
For functions, it is useful to get a witness for multiple arguments. But, for curried functions, each application has extra syntax and is not instantiable by a single hole. In this case, if a general witness has not already been found and the final form is a function, then more search holes could be created. See saturaion in Seidel et al.\ \cite{SearchProc}.\par 
A nice way to represent this would be to thinking of this would be \textit{attaching the function to a search hole}\footnote{Alternatively thought of as applying the function to the hole and instantiating accordingly}, which instantiates to the function with $n$ applications (then $n$ may change during backtracking). Besides using application onto search holes, attaching functions could just use the existing non-empty hole machanism. Further, in a sense a regular search hole is the above but for $n=0$; however, operationally it does not make sense to combine the two\footnote{Fundamentally, there is a difference between asking for a witness of the 1st arg of a function vs.\ the whole function.}. \textbf{Think about user interaction for this to guide this design.}\footnote{e.g. make clear the distinction between saturated witnesses and regular witnesses. Maybe directly annotate a term with a special application: \{\texttt{hole}\} as syntactic sugar for applying $n$ remaining arguments for witness}

\subsection{Witness Result}
A witness will be an initial instantiation leading to a  final form containing a cast error(s). Or potentially a trace containing a cast error (if annotations are treated as correct), for which stopping evaluation before the cast error is removed is ideal. 
\par
A manner in which to get the execution of the actually found witness result will be needed. So, whole traces would need to be stored/accumulated. Consider the space considerations of this. Look into Hazel's stepper and history functionality.

\subsection{Coverage \& Time Limit}
Due to this problem being undecidable and with potential infinite loops, a trace length limit will be imposed. Further, by using coverage metrics via symbolic evaluation, possible values to generate could be fully exhausted, showing that the code is actually safe.

