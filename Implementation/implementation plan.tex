\chapter{TEMPORARY: Implementation Plan}
\section{Cast Slicing}
\subsection{Theoretical Foundations and Context}
\subsubsection{Context: Why are casts elaborated}
See The Gradualizer \cite{Gradualizer} for details on this terminology. It doesn't perfectly fit Hazel but is close.
A cast is inserted at a point of either, for a subexpression $e : \tau$ if either:
\begin{itemize}
\item Pattern Matching $\tau \blacktriangleright \tau'$ -- Wrap $e$ in a cast $\scast{\tau}{\tau''}$ where $\tau''$ is the \textit{cast destination of $\tau'$}
\item  Flows $\tau \rightsquigarrow \tau'$ -- Wrap $e$ in a cast $\scast{\tau}{\tau'}$ -- These correspond to uses of type consistency.
\end{itemize}
A \textit{cast destination} for $\tau$ is the result of applying flows recursively on all positive type positions and reversed flows on negative type positions in $\tau$.\par
Flows relate to invocations of consistency and joins, with direction dictated by type polarity and modality:
\begin{itemize}
\item Producers flow to their final type 
\item Final types flow to the consumers
\item Input variables are replaced with the final type.
\end{itemize}
Where the final type is an annotated \textit{type}, output consumers, or the join of all producers.\footnote{Therefore types generally have output mode.}
\par 

\subsubsection{Type slicing a term}
\textbf{A logical interpretation of this would be useful (see the relation of contexts slices and type slice decomposition in implications/functions). The below formulation feels quite ad-hoc.}
\par
A \textit{type slice} of an $e$ in a context is an expression that has sub-expressions of $e$ replaced with holes or type annotations replaced with the dynamic type but still type synthesises or analyses to the same type.\footnote{Additionally they contain other info for use decomposing slices and the encompassing. Discussed throughout.}\par 
\textit{An alternative formalisation could be: Instead of type synthesises or analyses the same type, it could contain all code that could be changed in order to NO LONGER synthesise/analyse to the type.}\par
However, as typing often depends on the context, for which code involved may be outside the scope of the current term, an additional notion of a context slice will be attached. A context slice will contain all the variable bindings (lambdas or let bindings) and, importantly, their annotations. It is important to notice that this only works because Hazel is explicitly typed at variable bindings. \textit{An implicitly type language, like ML, could be translated to an explicitly typed internal language, for example CoreXML \cite{CoreXML}. Then, `constraint slices' could track code which is required for the constraint system to derive each annotation; this idea is similar to that used in type error slicing for higher-order functional languages \cite{ErrSlice, HaackErrSlice}.}
\par 
Slices for compound types will need to be split into slices for component types (as casts are decomposed), a suitable extension of this needs to be produced that allows slices to be decomposed into the slices of component types.\par
Compound types generally involve type checking sub-terms are of the component types, so these component slices can be obtained during this. But,
for types involving variable binding, e.g. function types $\tau = \tau_1 \to \tau_2$, the slice of the argument $\tau_1$ should simply be the binding annotation, in a sense this is the extension of the context slices between $\tau$ and $\tau_2$\footnote{A logical interpretation or analogy of this would be useful}.\\ \par 
Importantly, a hole analyses against any type, so any expressions analysed against can be replaced by holes\footnote{\textbf{FIX}: if using analysis on a non-subsumable type then maybe all non-subsumption rules should also construct slices.}.\par 
On the other hand, synthesis rules may require expanding a slice. Mostly, by composing component slices as discussed above.
\par 
A formal definition of this is given below. Treating a slice $\varsigma$ of $e,\tau$ as a pair of a context slice and expression slice $(\gamma, \varepsilon)$ indexed by $\tau$. The context slice $\gamma$ is simply a subset of the typing context $\Gamma$ and the slice $\varepsilon$ is a term $e'$ which is less precise\footnote{Ideally, least precise} than $e$ and synthesises $\tau$ under the context slice. For simplicity adding the notion of unsubstitutable, unnamed holes $\hole$: 

\begin{figure}[H]
\small
Syntax:
\[\varsigma ::= \tau[\gamma, \varepsilon] \mid (\varsigma \to \varsigma) [\gamma,\varepsilon]\]
\fbox{$\synthesisSlice{e}{\tau}{\varsigma}$}\ \ \ $e$ synthesises type $\tau$ under context $\Gamma$ and produces slice $\varsigma)$
\tiny
\[\inference[\tiny SSConst]{}{\synthesisSlice{c}{b}{b[\cdot, b]}} \quad
\inference[\tiny SSVar]{x : \tau \in \Gamma}{\synthesisSlice{x}{\tau}{\tau[x:\tau,x]}}\]
\[ 
\inference[\tiny SSFun]{\synthesisSlice[\Gamma,x:\tau_1]{e}{\tau_2}{s[\gamma, \varepsilon]}}{\synthesisSlice{\lambda x:\tau_1.e}{\tau_1 \to \tau_2}{(\tau_1[x:\tau_1,\hole] \to s[\gamma, \varepsilon])[\gamma\backslash\{x:\tau_1\},\lambda x:\tau_1. \varepsilon]}}
\]
\[
\inference[\tiny SSApp]{\synthesisSlice{e_1}{\tau_1}{\varsigma_1} & \tau_1 \funmatch \tau_2 \to \tau \\
\varsigma_1 \funmatch (\varsigma_2 \to s[\gamma,\varepsilon])[\gamma_1, \varepsilon_1] & \analysis{e_2}{\tau_2}}{\synthesisSlice{e_1(e_2)}{\tau}{s[\gamma_1, \varepsilon_1(\hole)]}} \quad 
\inference[\tiny SSEHole]{}{\synthesisSlice{\hole^u}{?}{?[\cdot, \hole]}} 
\]
\[
\inference[\tiny SSNEHole]{\synthesis{e}{\tau}}{\synthesisSlice{\hole[e]^u}{?}{?[\cdot, \hole]}}\quad 
\inference[\tiny SAsc]{\analysis{e}{\tau}}{\synthesisSlice{e : \tau}{\tau}{\tau[\cdot, \hole : \tau]}}
\]
\caption{Bidirectional typing judgements for \textit{sliced} external expressions}
\label{fig:typingsliced}
\end{figure}
Defining a slice matching relation analogously to type matching $\funmatch$.\par
Notice that for types involved with the typing context, like function arguments, where no such term with type $\tau_1$ actually exists in code; the context slice gives required all info and the expression slice is filled with a dummy hole. \textbf{Formalise this better: could consider a term of type $\tau_1 \to ?$. Alternatively this may be better expressed as a transformation of DERIVATIONS themselves.}\par 

For implementation and user purposes, the assumptions in the context slice $\gamma$ derive from annotations, whose location in code would be tracked. This could be formalised in the above rules by annotating type assumptions with locations, or by considering a context slice could as a term with holes at every subterm except for the relevant bindings.\par 

\subsubsection{Analysis Contexts}
Every type that is analysed must have been synthesised by some expression. This expression will be used as an `analysis context' to allow proving formal properties and for use in determining the `scopes of types' and in the search procedure. \textbf{Consider the weird cases like a synthesised function type; what is the expression of the argument??}

\subsubsection{Slicing of Casts}
Casts always go from expression types (for which a type slice exists) to either final types or consumers.\par 
Producer types correspond to synthesised types. Consumer types correspond to analysed types. Final types are one of the above or additionally, joins of (producer) types.\par 
\par 
Hence, as producer and consumers originate from type checking, their type slices can be obtained. The only remaining possibility is joins of types, so joins of type slices must be created.\par 
First, I show how slices can be applied to elaboration rules, with casts now being between type slices $\scast{\varsigma_1}{\varsigma_2}$. Notably, additional slicing is required in analysis mode, but not in synthesis mode. When in analysis mode, the slice for $\tau'$, that is the actual internal type of the elaborated expression, is required. When in synthesis mode, the slice for $\tau$ can simply be obtained via the external type slicing. 

\par Formal definition below. Omitting the trivial synthesis cases not involving casts.
\begin{figure}[H]
\small
Redefining elaboration synthesis:
\tiny
\[\inference[\tiny ESApp]{\synthesisSlice{e_1}{\tau_1}{\varsigma_1} & \tau_1 \funmatch \tau_2 \to \tau & \varsigma_1 \funmatch (\varsigma_2 \to \varsigma)[\gamma_1, \varepsilon_1]\\
\elaborationAnalysisSlice{e_1}{\tau_2 \to \tau}{d_1}{\tau_1'}{\Delta_1}{\varsigma_1'} & \elaborationAnalysisSlice{e_1}{\tau_2}{d_2}{\tau_2'}{\Delta_2}{\varsigma_2'}}{\elaborationSynthesis{e_1(e_2)}{\tau}{(d_1\scast{\varsigma_1'}{\varsigma_1})(d_2\scast{\varsigma_2'}{\varsigma_2})}{\Delta_1 \cup \Delta_2}}
\]
\[\inference[\tiny ESAsc]{\synthesisSlice{e}{\tau}{\varsigma} & \elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}}{\elaborationSynthesis{e:\tau}{\tau}{d\scast{\varsigma'}{\varsigma}}{\Delta}}\]
\small
\fbox{$\elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}$}\ \ \ $e$ analyses against type $\tau$ and elaborates to $d$ of consistent type $\tau'$ with slice $\varsigma'$
\tiny
\[
\inference[\tiny SEAFun]{\tau \funmatch \tau_1 \to \tau_2 \\ \elaborationAnalysisSlice[\Gamma,x:\tau_1]{e}{\tau_2}{d}{\tau_2'}{\Delta}{s[\gamma, \epsilon]}}{\elaborationAnalysisSlice{\lambda x. e}{\tau}{\lambda x:\tau_1. d}{\tau_1 \to \tau_2'}{\Delta}(\tau_1[x:\tau_1,\hole] \to s[\gamma, \varepsilon])[\gamma\backslash\{x:\tau_1\},\lambda x. \varepsilon]}
\]
\[
\inference[\tiny SEASubsume]{e \neq \hole^u & e \neq \hole[e']^u & \synthesisSlice{e}{\tau'}{\varsigma'}\\ \elaborationSynthesis{e}{\tau'}{d}{\Delta} & \tau \sim \tau'}{\elaborationAnalysisSlice{e}{\tau}{d}{\tau'}{\Delta}{\varsigma'}}
\]
\center
SEAEHole, SEANEHole are simply analogues of the synthesis slicing rules.
\caption{Elaboration judgements with slicing}
\label{fig:elaborationsliced}
\end{figure}

Second, I describe how to calculate joins of slices. This depends on the formalisation of type slicing. If only a minimal code slice to produce the same type is required, then some branches with matching type may be replaced by holes. If we include any code that can cause type checking to change, a join of slices is just the union of the code they point at\footnote{Prove.}.\par Semantically, in the above formulation it is a bit tedious to define and doesn't quite fit the pattern that the type slice actually corresponds to a term, here it corresponds to a union of branches. \textbf{The idea of a type slice should be refined to accept function argument types and joined types more intuitively}\footnote{Consider having join types be explicit. The lack of a link between these types and an expression is the issue.}.
\par 
Implementation-wise, slices will probably be code locations, and will be more granular than expressions.\footnote{Hence, the issues above are irrelevant to implementation.} Granularity at the level of annotations, and could potentially have special cases to distinguish cases where the slicing semantics inserts holes more clearly: for example, highlighting the brackets/hole being applied to a function in SSFun to make it clear that the type derives from \textit{application} rather than just the function which has a very similar slice. \textit{The hole based approach does have the benefit of automatically closing away irrelevant branches of code that is not highlighted, this could be a nice interaction design feature to implement.}

\subsubsection{Context: How are casts evaluated}
The Hazel cast calculus has two primary types of casts:
\begin{itemize}
\item Injections -- Casts to the dynamic type from a ground type
\item Projections -- Casts from the dynamic type to the dynamic type
\end{itemize}
These types of casts can be eliminated upon meeting or transformed into a cast error if the two involved ground types are inconsistent.\par 
Inserted casts don't necessarily fall into either of these two classes. If the cast is a non-ground injection/projection then they can be transformed by matching to a least specific ground type, e.g. $\tau_1 \to \tau_2 \groundmatch ? \to ?$. This ground type can be inserted in the middle of such a cast (ITGround, ITExpand). If the cast is between compound types and reducible by a suitable elimination rule (if casts were ignored), then these casts will be decomposed. \par 
See \cite{GradualizerDynamic} for in depth intuition.

\subsubsection{Cast Splitting}
There are only 3 rules that non-trivially modify casts: ITAppCast, ITGround, ITExpand. For ITGround and  ITExpand, it makes sense to keep the cast slices for $\tau'$ the same as $\tau$.\footnote{Justify this. Maybe casting to ground type was not in code so the cast should not highlight anything, but instead record it's \textit{dependencies}?} ITAppCast requires decomposing the function cast, luckily slices recursively include type slices of their  type components.
\[\inference[ITAppCast]{\tau_1 \to \tau_2 \neq \tau_1' \to \tau_2'}{d_1\scast{\varsigma_1 \to \varsigma_2}{\varsigma_1' \to \varsigma_2'}(d_2) \to (d_1(d_2\scast{\varsigma_1'}{\varsigma_1})\scast{\varsigma_2}{\varsigma_2'})}\]

\par 

Recording dependencies between casts from this might be useful. This allows a user to see how a cast was formed.


\subsubsection{Random notes and refs}
Consider options of: annotating typing derivations, annotating casts (and splitting in evaluation), or reverse unevaluating casts \cite{FunctionalProgExplain}.\par
See constraint free error slicing \cite{ConstraintFreeErrSlice}.\par
\textbf{See the gradualizer to understand how casts work }\cite{Gradualizer}.\par
See Blame tracking to see how common type location transfer is handled \cite{Blame}.\\ \par
Casting:\par 
Casting goes from the terms actual type to coerce it into a new type. Casts are inserted by elaboration at appropriate points (where the type system uses consistency or pattern matching\footnote{See the gradualizer for intuition and direction of consistency casts}). Casts between compound types may be decomposed by the cast calculus. See gradualizer dynamic semantics \cite{GradualizerDynamic} for intuition and \cite{Blame} for intuition on polarity affecting blame. \par 
Consider deeply type constructor polarities \cite[pg.~473]{TAPL}. Covariant types give positive blame (blame value) and contravariant give negative (blame context).

\subsection{Implementation Details and Optimisations}
\subsubsection{Slices -- Location based}
Location based slices would could be represented by intervals.\par 
Due to the recursive nature of slices (component type slices must be available), then there will be a significant number of interval trees. Therefore, a persistent structure \cite[chapter~2]{PurelyFunctionalDataStructures} MUST be used for space efficiency.\par 
Efficiently displaying overlapping intervals is ideal. A sorted list achieves this, so a sorted  tree structure of some sort could be used to maintain efficient insertion.\par 
Hash consing may be similarly useful.\par 
Displaying a slice here does need to be more efficient as there may be many intervals that aren't actually merged vs just walking a tree with holes.


\subsubsection{Slices -- Hole Based}
Slices can be stored immutably as ASTs and splitting of slices will be handled in a space efficient way by default by OCaml's persistent nature.\par 
Overlapping slices can appear, most notably from joining slices. Therefore, joins should be implemented to preserve space. Speed of merging and splitting is also relatively important.\par 
Displaying a slice is a relatively uncommon procedure compared to joining and splitting. Hence, retrieving and aggregating locations from the slice does not need to be fast, nor space efficient. \par 
A regular persistent tree is likely sufficient, but lazy joining and/or hash consing \cite{HashCons} might be useful.


\section{Search Procedure}
\textit{Look into Zippers \cite{Zipper}, and other functional data structures that may be useful here.
}
\subsection{Hole Refinement}
A suitable notion of \textit{type hole} should be used for this. These will differ from regular holes in that they will maintain runtime type information\footnote{Or the evaluation environment will track these.}. In particular they will be annotated by a dynamically inferred type variable, which would be progressively refined.\par
In particular the type given to a hole will be \textit{entirely} determined by the dynamics, and not the static type checking, which may not be well-typed or may be unspecific (using dynamic types).
\par
\textit{Look into Gradual type inference \cite{GradualTI}, Gradual dynamic type inference \cite{DTI}, Seidel et al. \cite{SearchProc}, OLEG \cite[chapter~2]{OLEG} \& Idris for ideas.  
Look into how Hazel may support type-driven code completion.}
\subsection{Hole Instantiation}
Substitution semantics via CMTT contextual substitution.\par 
Generating of values to substitute will match the required type as annotated on the hole. These should be the most general partial values to meet the type, see OLEG refinement \cite[chapter~2]{OLEG}.\par
Generation of suitable concrete values of any type should be informed by ideas like the `small scope hypothesis' \cite{SmallScope}\footnote{This is evaluated for bugs in general in OOP, not necessarily type bugs in FP.} or just could be random as in QuickCheck \cite{QuickCheck}, SmallCheck \cite{SmallCheck}. Allowing some meta-programming of the value generation algorithms would be an interesting idea, especially for user-defined types.\par
The idea of instantiating in order to effectively explore branches is a good idea. This is essentially symbolic execution, and exploring branches space-time-efficiently without overlap is the goal.\footnote{This is the main idea to think about, probably more important than user control of value generation.} \par 
Some heuristics may help with this search. But this project is unlikely to consider any in detail.\par
\textit{Note, both hole refinement and instantiation can be handled as a search procedure wrapper around the stepper.}
\subsection{Witness Generality}
A witness is a hole instantiation that evaluates to a final form containing a cast error(s). \textit{Witness generality} should ensure that generating a witness via the search procedure implies that for any type, a witness exists of this type.\par 
To achieve this, holes should only be instantiated when absolutely required, i.e. lazily as in SmallCheck \cite{SmallCheck}. For example, even if a function has a cast requiring integer arguments, the hole instantiation should be delayed, as passing a hole is still safe. When the hole passes into the function it will accumulate a cast to int, but the inferred type should remain ambiguous until evaluation gets stuck (such as at a case analysis). If this inferred type conflicts with the casts, hole instantiation to the inferred type may still be useful, and Hazel still supports evaluation.\par 
Alternatively, annotations could be treated as correct, meaning casts would refine the inferred type, and thereby treat any value instantiation at this point as a witness. Note that the use of dynamic types still allows the procedure to find general witnesses.

\subsection{Instruction Transitions}
Lazy hole instantiation and generation can be formalised with a \textit{non-deterministic} step semantics in Fig. \ref{fig:searchinstructions}.\par 
The below semantics treat casts as correct, and use them to refine types.
\begin{figure}[h]
\[d ::= \dots \mid \hole[\tau]^u_\sigma\]
\tiny
\[
\inference[\tiny GenConst]{\text{primitive op $p$}}{p(\hole[b]^u_\sigma) \longrightarrow p(c)} \quad 
\inference[\tiny GenFun]{\text{fresh $x$}}{\hole[\tau_1 \to \tau_2]^u_\sigma(d) \longrightarrow (\lambda x:\tau_1. \hole[\tau_2]^u_{\sigma, x/x})(d)}
\]
\[
\inference[\tiny Refine]{}{\hole[\tau]^u_\sigma\scast{\tau}{\tau'} \longrightarrow \hole[\tau']^u_\sigma\scast{\tau}{\tau'}}
\]

\caption{Search Procedure Instruction Transitions}
\label{fig:searchevaluationorder}
\end{figure}

Upcasting causes the holes to \textit{lose} type information. This shouldn't be a problem because the only time values are actually generated, the required type can be inferred from the primitive operation, or by generating a dynamic typed function.

\subsection{Backtracking \& Non-determinism}
If a particular instantiation does not result in a witness, then some form of backtracking is required. This is the primary addition to the small-step evaluator that needs to be added.\par 
A simple way to do this would be wrapping the evaluator in a choice-pointing system -- like in Prolog. \par 
Doing this space-efficiently (saving choice points) would be the goal.\par 
\textbf{Look into non-deterministic evaluation from a logic programming perspective. Also backtracking in general.} See the Warren Abstract Machine \cite{WAM}.\par 
A \textit{semi-persistent} data structure \cite{SemiPersistent} will be needed for this.\par 
In order to find witnesses that are shorter in trace length (and hence easier for the user to understand), it may be useful to perform iterative deepening. Otherwise, trace compression and input simplification (as in QuickCheck \cite{QuickCheck})
\par 
For efficiency reasons, it may be desirable to use some amount of immutability here.\footnote{Hazel code-base rules seem to not allow this though!}\par
Also, \texttt{call/cc} or \texttt{shift/reset} could be useful (or maybe even effect handlers \cite{EffectsHaskell}) for managing the desired control flow.

\subsection{Cast Dependence}
A cast error for associated with a witness should depend upon the value instantiated. Unrelated cast errors are not proof that the witness is valid.\par 
This notion of dependence can be modelled by counting only casts that are attached to sub-terms of an instantiated value. \textit{If an unrelated cast error causes evaluation to get stuck, this could be reported in some way to the user.}\par 

\subsection{Evaluation Order}
\textit{Making this work for multiple search holes in arbitrary position might be too difficult. The other option is to only support this attached to functions.}\par
It is inefficient to evaluate the whole program in order to perform the search procedure. Instead evaluation should be directed first around search procedure holes\footnote{Of course, only possible because Hazel is pure.}.\par
The evaluation order could be formalised by adapting the evaluation context to have \textit{multiple} markers, and define inserting several terms into those marks with \textit{strong}\footnote{allowing reductions inside functions} non-deterministic semantics, Fig. \ref{fig:searchevaluationorder}. Then constructing the context would put markers for each redex involving a search hole and evaluation would expand around them\footnote{This ordering seems hard to display in maths, but the implementation should be straightforward}.

\begin{figure}[h]
\tiny
\begin{align*}
&E_0 ::= d\\
&E_1 ::= E\\
&E_n ::= E_k(E_{n-k}) \mid \hole[E_n]^u_\sigma \mid E_n\scast{\tau}{\tau} \mid E_n\scasterror{\tau}{\tau} && \text{for $n>1$ and $k\leq n$}
\end{align*} 

\[\inference[\tiny Step]{d = E_n(d_1, \cdots, d_i, \cdots, d_n) & d_i \longrightarrow d_i' & d' = E_n(d_1, \cdots,  d_i',\cdots, d_n)}{d \mapsto d'}\]

\caption{Search Procedure Evaluation Context}
\label{fig:searchevaluationorder}
\end{figure}
\par
Some form of \textit{concurrent zipper} via delimited continuations \cite{ConcurrentZipper, MultiZipper} for constant (or fast) access to the term context around each search hole would be ideal.\par 
Finally, two search holes may meet. What to do in this case?

\subsection{Curried Functions \& Attached Search Holes}
For functions, it is useful to get a witness for multiple arguments. But, for curried functions, each application has extra syntax and is not instantiable by a single hole. In this case, if a general witness has not already been found and the final form is a function, then more search holes could be created. See saturaion in Seidel et al.\ \cite{SearchProc}.\par 
A nice way to represent this would be to thinking of this would be \textit{attaching the function to a search hole}\footnote{Alternatively thought of as applying the function to the hole and instantiating accordingly}, which instantiates to the function with $n$ applications (then $n$ may change during backtracking). Besides using application onto search holes, attaching functions could just use the existing non-empty hole machanism. Further, in a sense a regular search hole is the above but for $n=0$; however, operationally it does not make sense to combine the two\footnote{Fundamentally, there is a difference between asking for a witness of the 1st arg of a function vs.\ the whole function.}. \textbf{Think about user interaction for this to guide this design.}\footnote{e.g. make clear the distinction between saturated witnesses and regular witnesses. Maybe directly annotate a term with a special application: \{\texttt{hole}\} as syntactic sugar for applying $n$ remaining arguments for witness}

\subsection{Witness Result}
A witness will be an initial instantiation leading to a  final form containing a cast error(s). Or potentially a trace containing a cast error (if annotations are treated as correct), for which stopping evaluation before the cast error is removed is ideal. 
\par
A manner in which to get the execution of the actually found witness result will be needed. So, whole traces would need to be stored/accumulated. Consider the space considerations of this. Look into Hazel's stepper and history functionality.\par 
The trace should ideally be presented in the standard call-by-value evaluation order, rather than the search procedure evaluation order.\par 
Consider trace reduction and/or witness reduction (see quickcheck).

\subsection{How do search holes react upon meeting?}
Multiple holes can meet, and this is where this method can find witnesses that were NOT possible in Seidel. But, a way of choosing values to instantiate in these cases must be devised AND the backtracking must be devised to recalculate too much information.

\subsection{Coverage \& Time Limit}
Due to this problem being undecidable and with potential infinite loops, a trace length limit will be imposed. Further, by using coverage metrics via symbolic evaluation, possible values to generate could be fully exhausted, showing that the code is actually safe.

\subsection{Hazel Implementation}
It appears the main branch of Hazel has not actually implemented fill-and-resume. However, an earlier branch has -- haz3l-fill-and-resume.\par 
This old branch does NOT include all features required by my core goal -- missing \textit{sum types} and \textit{type aliases}. And advanced features like polymorphism are not all implemented with fill-and-resume. To reach my extension goal I must implement some form of fill-and-resume for the dev branch. And the core goal requires adding \textit{at least} sum types and type aliases to the old branch. \textbf{This is an unforseen extra task.}\par 
Maybe a hacky fill-and-resume that works just well enough for the search procedure could be implemented.

\section{Type Error Witnesses Interaction in General}
There are various classes of type errors, and witnesses will be evaluation traces to cast errors directly witnessing the type error.\footnote{Come up with a formal way of saying this.}:

\subsection{Static Error Witnesses \& Type Witnesses}
Each expression synthesises a type. A witness (evaluation to a value) can be provided for (some) of these. \textbf{Do this.}\par 
Some expressions analyse against a type. The witness of the origin of the analysed type will have a type witness derived from synthesis type witnesses of it's components?\par
Then for each class of type errors:
\begin{itemize}
\item Inconsistent against analysis type -- Use the type witness of the expression which imposed the analysis.
\item Inconsistent branches -- Compare synthesised type witnesses of the inconsistent branches.
\item Bad application -- 
\end{itemize}

\subsubsection{Type Synthesis Witnesses}
Consider:
\begin{minted}{reason}
let sqsum = fun xs -> case xs 
  | [] => 0 
  | h :: t => sqsum(t) @ (h * h) end 
in ?
\end{minted}
and also with \texttt{sqsum} annotated with \texttt{[?] -> ?} or \texttt{[String] -> ?}.\\ \par
How to treat vars? \par 
Treat them as instances to use the search procedure on. There are various instances for free vars: 
\begin{itemize}
\item Let binding -- These always have a direct definition \textit{(even recursive bindings, via fix)}, and hence can be substituted directly.
\item Fun binding -- Use of such a binding implies the current expression is a component part of the function itself. This function can be searched using the search procedure. Essentially, this is taking the variable as an input to the procedure.
\item Pattern binding -- These can be desugared into a one of the above bindings followed by deconstructing the bound value. i.e. in a fun binding it is a restriction on possible instantiations.
\end{itemize}
Note that the values by the above may NOT be possible in practice. i.e. if they do not result from a witness of the variable. Highlight these?

\subsubsection{Static Error Witnesses}
Essentially when putting errors into holes at compile time, tag the holes with their analysed type and original expression and context.\par 
Then a search procedure stage can be added which performs the search on each hole treating free vars as above. For each type error we do:
\begin{itemize}
\item Inconsistent type between synthesised and analysed:\par  We can get a type witness of it's synthesised type, instantiating any holes within appropriately (make this very clear in the visualisation\footnote{e.g. an error hole containing an int 1 could be instantiated to a string "str" if required}).\par 
The witness of this hole is then the analysis context using this witness (and any other required witnesses).
\item Inconsistent branches: Get a type witness of each branch and compare.
\item Bad function position (inconsistent with arrow): Get witness of the expected arrow. The result will be a something applied to a non-function (also make sure the cast slice error produced here works).
\item No type: e.g. bad application, bad token, free constructor -- Treat as holes.
\end{itemize}

\subsubsection{Treatment of holes:}
Treat as search procedure inputs and instantiate accordingly if progress is not being made.

\subsubsection{On the interaction of multiple errors}
\textit{Think about cases where error holes may be dependent? Or where recursion may result in a hole depending on itself?}

\subsection{Treatment of dynamic typed regions:}
There may still be traces leading to a cast error that are not statically caught due to presence of dynamically typed regions.\par 
Selecting functions and searching for type errors (not linked to static errors) could be conducted as in Seidel. This could be generalised to selection any binding, with nullary functions (constants) just being evaluated directly with no inputs.

\subsection{Interaction}
There are 3 modes of use that I can envisage:
\begin{enumerate}
\item Searching in a selected expression: \begin{itemize}
\item Select an expression (via cursor)
\item Start search procedure on this expression, instantiating lazily all holes in the expression.
\item Evaluate as far as possible/up until a limit to collect all cast errors. Or alternatively stop at the first.
\end{itemize}
\item Searching for witnesses of a specific, statically caught, type error.
\begin{itemize}
\item Click on static errors to obtain witness.
\item Allow checking any dynamic/partially dynamic function for potential type errors via search procedure. This could be a button when selecting such a function.
\item Allow getting a type witness of any expression. Again, could be a button. This is essentially just evaluating at the cursor, but with lazy function input instantiation.
\end{itemize}
\item Searching in a selected expression, around holes:\par  Same interaction as (1), but evaluates around holes using a multi-zipper structure. This approach has the following benefits:
\begin{itemize}
\item Avoids evaluating regions without holes (unless necessary).
\item Finds errors in (some) dead code.
\item Is a more principled way to attempt to find instantiations of \textit{every} hole involved. Each trace around each hole is evaluated to similar depths.
\end{itemize}
And downsides:
\begin{itemize}
\item Implementation difficulty.
\item Higher constant factor in evaluation.
\item Unusual evaluation order, making reasoning more difficult. This could be remedied by creating a system to replay the evaluation in a more standard ordering than the search procedure used.
\end{itemize}

\end{enumerate}

\subsection{Proofs}
The ideal proofs here would be, alongside the expected ones for evaluation of search procedure holes (generality, progress):\par 
That cast errors should relate directly to the source static type error. The witness of a static type error will contain a cast attached directly to this term (or one of it's evaluated derivatives).

\subsection{Actual Plan of Action}
\begin{itemize}
\item Implement hole substitution.
\item Implement lazy hole instantiation: First when inside casts. Then also upon reaching operation when no wrapped in a cast. \textbf{This is nondeterministic.}
\item Implement a evaluation viewer for this evaluation semantics, similar to the stepper.
\item Explaining holes: Clicking each hole instantiation gives a reason why it was instantiated. Importantly, it will also link back to the ids of the original hole in the code AND make it clear why this term is a hole (i.e. due to being an error)\footnote{Error holes which involve instantiated free variables may be somewhat confusing (they will be instantiated to a different type to the value within the hole)}.
\item Search for witnesses of statically caught type errors. For the three cases: Inconsistent types using an analysis context; Inconsistent branches, which will produce a list of evaluation traces \textit{(note: no cast errors here)\footnote{Consider how to link these errors back to code}}; Inconsistent with arrow type.
\item UI: Click on static type errors to obtain this.
\item Implement a multi-zipper data structure. \cite{MultiZipper}
\item Add a `composition' function that given a cursor on a hole in the zipper, can compose the hole with it's context to find the smallest reduction involving it.
\item Implement the multi-zipper search procedure evaluation using this.
\end{itemize}