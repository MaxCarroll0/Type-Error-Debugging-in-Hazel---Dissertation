\chapter{Preparation}
\label{chap:Preparation}
In this chapter I present the technical background knowledge for this project: an introduction to the type theory for understanding Hazel's core semantics, an overview of Hazel implementation, and notes on non-determinism (as the type witness search procedure is non-deterministic). Following this, I present my software engineering methodology.

\section{Background Knowledge}\label{sec:BackgroundKnowledge}
\subsection{Static Type Systems}\label{sec:TypeSystems}
A \textit{type system} is a lightweight formal mathematical method which categorises values into \textit{types} and expressions into types that evaluate to values of the same type. It is effectively a static \textit{approximation} to the runtime behaviour of a language. The following sections expect basic knowledge formal methods of type systems in terms of judgements (\cref{sec:Judgements} reviews this). Note that I will use \textit{partial functions} to represent typing assumption contexts. 


\subsubsection{Dynamic Type Systems}\label{sec:DynamicTypeSystem}
\textit{Dynamic typing} has purported strengths allowing rapid development and flexibility, evidenced by their popularity \cite{DynamicLangShift, TIOBE}. Of particular relevance to this project, execution traces are known to help provide insight to errors \cite{TraceVisualisation}, yet statically typed languages remove the ability to execute programs with type errors, whereas dynamically typed languages do not.\par 

A \textit{dynamically typed system} can be implemented and represented semantically by use of dynamic \textit{type tags} and a \textit{dynamic type}\footnote{Not necessarily needed for implementation, but is useful when reasoning about dynamic types within a formal type system or when considering types within a \textit{static} context.} \cite{DynamicTyping}. Then, runtime values can have their type checked at runtime and \textit{cast} between types. This suggests a way to encode dynamic typing via \textit{first-class}\footnote{Directly represented in the language syntax as expressions.} cast expressions which maintain and enforce runtime type constraints alongside a dynamic type written \dyn.

Cast expressions can be represented in the syntax of expression by $e\scast{\tau_1}{\tau_2}$ for expression $e$ and types $\tau_1, \tau_2$, encoding that $e$ has type $\tau_1$ and is cast to new type $\tau_2$. An intuitive way to think about these is to consider two classes of casts:
\begin{itemize}
\item \textit{Injections} -- Casts \textit{to} the dynamic type $e\scast{\tau}{?}$. These are effectively equivalent to type tags, they say that $e$ has type $\tau$ but that it should be treated dynamically.
\item \textit{Projections} -- Casts \textit{from} the dynamic type $e\scast{?}{\tau}$. These are type requirements, for example the add operator could require inputs to be of type \code{int}, and such a projection would force any dynamic value input to be cast to \code{int}. 
\end{itemize}
Then when \textit{injections} meet \textit{projections}, $v\scastcast{\tau_1}{\dyn}{\tau_2}$, representing an attempt to perform a cast $\scast{\tau_1}{\tau_2}$ on $v$. We check the cast is valid and perform if so:
\[\inference{\tau_1 \text{ is castable to } \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v'} \qquad \inference{\tau_1  \text{ is {\color{red} not }castable to }  \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v\scasterror{\tau_1}{\tau_2}}\]


Compound type casts can be broken down during evaluation upon usage of such constructs. For example, applying $v$ to a \textit{wrapped}\footnote{Wrapped in a cast between function types.} functions decomposes the cast into casting the applied argument and then the result:
\[(f\scast{\tau_1 \to \tau_2}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\tau_1})\scast{\tau_2}{\tau_2'})\]
Or if $f$ has the dynamic type:
\[(f\scast{\dyn}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\dyn})\scast{\dyn}{\tau_2'})\]
The direction of the casts reflects the \textit{contravariance} \cite[ch. 2]{BasicCatTheory} of functions\footnote{A bifunctor.} in their argument.
See that the cast $\scast{\tau_1'}{\tau_1}$ on the argument is \textit{reversed} with respect to the original cast on $f$. This makes sense as we must first cast the applied input to match the actual input type of the function $f$.

Hence, casts around functions (type information) will be moved to the actual arguments at runtime, meeting with casts casts on the argument, resulting in a cast error or a successful casts.

\subsubsection{Gradual Type Systems}\label{sec:GradualTypeSystem}

A \textit{gradual type system} \cite{GradualRefined, GradualFunctional} combines static and dynamic typing. Terms may be annotated as dynamic, marking regions of code omitted from type-checking but still \textit{interoperable} with static code. For example, the following type checks:
\begin{minted}[escapeinside=||]{reason}
let x : |\dyn| = 10;  // Dynamically typed
x ++ "str"       // Statically typed
\end{minted}
Where \code{++} is string concatenation expecting inputs to be \code{string}. But would then cause a runtime \textit{cast error} when attempting to calculate \code{10 ++ "str"}.

It does this by representing casts as expressed previously. The language is split into two parts:
\begin{itemize}
\item The \textit{external language} -- where static type checking is performed which allows annotating expressions with the dynamic type.
\item The \textit{internal language} -- where evaluation and runtime type checking is performed via cast expressions.\footnote{i.e. the proposed \textit{dynamic type system} above.} The example above would reduce to a \textit{cast error}\footnote{Cast errors now represented with a strike-through and in red. From here-on they are considered as first-class constructs.}: \[10\scasterror{\code{int}}{\code{string}}\code{ ++ "str"}\]
\end{itemize}
For type checking, a \textit{consistency} relation $\tau_1 \sim \tau_2$ is introduced meaning \textit{types $\tau_1, \tau_2$ are consistent}. This is a weakening of the type equality requirements in normal static type checking, allowing consistent types to be used additionally.

Consistency must satisfy a few properties: that the dynamic type is consistent with every type, that $\sim$ is reflexive and symmetric, and two concrete types (having no  dynamic sub-parts) are consistent iff they are equal\footnote{e.g. when $\tau_1, \tau_1', \tau_2, \tau_2'$ don't contain $\dyn$, then $\tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'$ iff $\tau_1 \to \tau_2 = \tau_1' \to \tau_2$}. Finally, differing this from type equality, that every type $\tau$ is consistent with the dynamic type $\dyn$:
\[\inference{}{\tau \sim \dyn} \quad \inference{}{\tau \sim \tau} \quad \inference{\tau_1 \sim \tau_2}{\tau_2 \sim \tau_1} \quad \inference{\tau_1 \sim \tau_1' & \tau_2 \sim \tau_2}{\tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}\]
This is very similar to the notion of \textit{subtyping} \cite[ch. 15]{TAPL} with a \textit{top} type $\top$, but with symmetry instead of transitivity.

Then typing rules can be written to use consistency instead of equality. For example, application typing:
\[\inference{\Gamma \vdash e_1 : \tau_1 & \Gamma \vdash e_2 : \tau_2'\\ \tau_1 \funmatch \tau_2 \to \tau & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau_2'}\]
Where $\funmatch$ is a pattern matching function to extract the argument and return types from a function type, used to account for if $\Gamma \vdash e_1 : \dyn$, where we treat $\dyn$ then as a dynamic function $\dyn \funmatch \dyn \to \dyn$.
Intuitively, $e_1(e_2)$ has type $\tau_2'$ if $e_1$ has type $\tau_1' \to \tau_2'$ or $\dyn$ (then treated as $\dyn \to \dyn$), and $e_2$ has type $\tau_1$ which is consistent with $\tau_1'$ and hence is assumed that it can be passed into the function.

But, for evaluation to work the static type information needs to be encoded into casts to be used in the dynamic internal language, for which the evaluation semantics are defined. This is done via \textit{elaboration}, similarly to Harper and Stone's approach to defining (globally inferred) Standard ML \cite{StandardMLTypeTheory} by elaboration to an explicitly typed internal language XML \cite{CoreXML}. The \textit{elaboration judgement} $\Gamma \vdash e \leadsto d : \tau$ read as: external expression $e$ is elaborated to internal expression $d$ with type $\tau$ under typing context $\Gamma$. For example to insert casts around function applications:
\[\inference{\Gamma \vdash e_1 \leadsto d_1 : \tau_1 & \Gamma \vdash e_2 \leadsto d_2 : \tau_2' \\ \tau_1 \funmatch \tau_2 \to \tau  & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau \leadsto (d_1\scast{\tau_1}{\tau_2 \to \tau})(d_2\scast{\tau_2'}{\tau_2}) : \tau}\]
If $e_1$ elaborates to $d_1$ with type $\tau_1 \sim \tau_2 \to \tau$ and $e_2$ elaborates to $\tau_2'$ with $\tau_2 \sim \tau_2$ then we place a cast\footnote{This cast is required, as if $\tau_1 = \dyn$ then we need a cast to realise that it is even a function. Otherwise $\tau_1 = \tau_2 \to \tau$ and the cast is redundant.} on the function $d_1$ to $\tau_2 \to \tau$ and on the argument $d_2$ to the function's expected argument type $\tau_2$ to perform runtime type checking of arguments.
Intuitively, casts must be inserted whenever type consistency is used, but which casts to insert are non-trivial \cite{Gradualizer}.

The runtime semantics of the internal expression is that of the \textit{dynamic type system} discussed above (\ref{sec:DynamicTypeSystem}). A cast is determined to succeed iff the types are \textit{consistent}.

\subsubsection{Bidirectional Type Systems}\label{sec:BidirectionalTypeSystem}
A \textit{bidirectional type system} \cite{BidirectionalTypes} takes on a more algorithmic definition of typing judgements, being more intuitive to implement. They also allow some amount of local type inference \cite{LocalInference}, allowing programmers to omit \textit{type annotations}, with some being inferred directly from code. Global type inference systems \cite[ch. 22]{TAPL} can be \textit{difficult to implement}, often via constraint solving \cite[ch. 10]{ATTAPL}, and difficult or impossible to \textit{balance} with complex language features, for example global inference in System F (\ref{sec:System F}) is undecidable \cite{SystemFUndecidable}.

This is done in a similar way to annotating logic programs \cite[123]{LogicProg}, by specifying the \textit{mode} of the type parameter in a typing judgement, distinguishing when it is an \textit{input} (type checking) and when it is an \textit{output} (type synthesis).

We express this with two judgements:
\[\synthesis{e}{\tau}\]
Read as: \textit{$e$ synthesises a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{output}.
\[\analysis{e}{\tau}\]
Read as: \textit{$e$ analyses against a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{input}

When designing such a system care must be taken to ensure \textit{mode correctness} \cite{ModeCorrectness}. Mode correctness ensures that input-output dataflow is consistent such that an input never needs to be \textit{guessed}. For example the following function application rule is \textit{not} mode correct:
\[\inference{\analysis{e_1}{{\color{red}\tau_1} \to \tau_2} & \analysis{e_2}{{\color{red}\tau_1}}}{\analysis{e_1(e_2)}{\tau_2}}\]
We try to \textit{check} $e_2$ with input ${\color{red}\tau_1}$\footnote{Highlighted in red as an error.} which is \textit{not known} from either an \textit{output} of any premise nor from the \textit{input} to the conclusion, $\tau_2$. On the other hand, the following \textit{is} mode correct:
\[\inference{\synthesis{e_1}{\tau_1 \to \tau_2} & \analysis{e_2}{\tau_1}}{\analysis{e_1(e_2)}{\tau_2}}\]
Where $\tau_1$ is now known, being \textit{synthesised} from the premise $\synthesis{e_1}{\tau_1 \to \tau_2}$. As before, $\tau_2$ is known as it is an input in the conclusion $\analysis{e_1(e_2)}{\tau_2}$.

Such languages will have three obvious rules. That variables can synthesise their type, being accessible from the typing assumptions. Annotated terms synthesise their type from the annotation (after checking the validity). Subsumption: a synthesising term successfully checks against that same type.

\subsection{The Hazel Calculus}\label{sec:CoreHazel}
\index{Hazel}
Hazel is a language that allows the writing of incomplete programs, evaluating them, and evaluating around static and dynamic errors.\footnote{Among other features, like an structure editor with syntactically meaningless states, and various learning aids.}

It does this via adding \textit{holes}, which can both be typed and have evaluation proceed around them seamlessly. Errors can be placed in holes allowing continued evaluation.

The core calculus \cite{HazelLivePaper} is a gradually and bidirectionally typed lambda calculus. Therefore it has a locally inferred bidirectional \textit{external language} with the dynamic type $\dyn$ elaborated to an explicitly typed \textit{internal language} including cast expressions. 

The full semantics are documented in the Hazel Formal Semantics appendix \ref{sec:HazelSemantics}, with only rules relevant \textit{holes} discussed in this section. The combination of gradual and bidirectional typing system is itself non-trivial, but only particularly notable consequences are mentioned here. The intuition should be clear from the previous gradual and bidirectional typing sections.\footnote{The difficulties combining gradual and bidirectional typing are largely orthogonal to adding holes.}

\subsubsection{Syntax}\label{sec:HazelSyntax}
\index{\textbf{Core Hazel} syntax}
\par The syntax, in Fig. \ref{fig:syntax}, consists of \textit{types} $\tau$ including the dynamic type $\dyn$, \textit{external expressions} $e$ including (optional) annotations, \textit{internal expressions} $d$ including cast expressions. The external language is \textit{bidirectionally typed}, and therefore is a locally inferred surface syntax for the language, and is statically elaborated to (explicitly typed) \textit{internal expressions}.

Notating $\hole{}^u$ or $\hole[e]^u$ for empty and non-empty holes respectively, where $u$ is the \textit{metavariable} or name for a hole. Internal expression holes, $\hole{}^u_\sigma$ or $\hole[e]^u_\sigma$, also maintain an environment $\sigma$ mapping variables $x$ to internal expressions $d$. These internal holes act as \textit{closures}, recording which variables have been substituted during evaluation.\footnote{This is required, as holes may later be substituted, with variables then receiving their values from the closure environment.}
\begin{figure}[h]
\begin{align*}
\tau &::= b \mid \tau \to \tau \mid\  ?\\
e &::= c \mid x \mid \lambda x : \tau.e \mid \lambda x. e \mid e(e) \mid \hole^u \mid \hole[e]^u \mid e : \tau\\
d &::= c \mid x \mid \lambda x : \tau d \mid d(d) \mid \hole^u_\sigma \mid \hole[d]^u_\sigma \mid d\scast{\tau}{\tau} \mid d\scasterror{\tau}{\tau}
\end{align*}
\caption{Syntax: \textit{types} $\tau$, \textit{external expressions} $e$, \textit{internal expressions} $d$. With $x$ ranging over variables, $u$ over hole names, $\sigma$ over $x \to d$ \textit{internal language} substitutions/environments, $b$ over base types and $c$ over constants.}
\label{fig:syntax}
\end{figure}

\subsubsection{External Language}\label{sec:HazelExternalLang}
\index{External language type system}
We have a bidirectionally static semantics for the \textit{external language}, giving the bidirectional typing judgements: $\synthesis{e}{\tau}$ and $\analysis{e}{\tau}$. Holes synthesise the \textit{dynamic type}, a natural choice made possible by the use of gradual types:
\[\inference[\tiny SNEHole]{\synthesis{e}{\tau}}{\synthesis{\hole[e]^u}{\dyn}} \qquad \inference[\tiny SEHole]{}{\synthesis{\hole^u}{\dyn}}\] 
One notable consequence of combining gradual and bidirectional typing is that the \textit{subsumption rule} in bidirectional typing is naturally extended to allow subsuming any terms of \textit{consistent} types:
\[\inference[\tiny ASubsume]{\synthesis{e}{\tau'}& \tau \sim \tau'}{\analysis{e}{\tau}}\]
Of course $e$ should type check against $\tau$ if it can synthesise a consistent type as the goal of type consistency is that we may type check terms as if they were of the consistent type.

\subsubsection{Internal Language}\label{sec:HazelInternalLang}
The internal language is non-bidirectionally typed and requires an extra \textit{hole context} $\Delta$ mapping each hole \textit{metavariable} $u$ to it's \textit{checked type} $\tau$ \footnote{Originally required when typing the external language expression. See Elaboration section.} and the type context $\Gamma$ under which the hole was typed. Each metavariable context notated as $u :: \tau[\Gamma]$, notation borrowed from contextual modal type theory (CMTT) \cite{CMTT}.\footnote{Hole contexts corresponding to \textit{modal contexts}, hole names with \textit{metavariables}, and holes with \textit{metavariable closures} (on environments $\sigma$).} 

The type assignment judgement $\typeassignment{d}{\tau}$ means that $d$ has type $\tau$ under typing and hole contexts $\Gamma, \Delta$. The rules for holes take their types from the hole context and ensure that the hole environment substitutions $\sigma$ are well-typed\footnote{With respect to the original typing context captured by the hole.}:
\[\inference[\tiny TAEHole]{u :: \tau[\Gamma'] \in \Delta & \typeassignment{\sigma}{\Gamma'}}{\typeassignment{\hole^u_\sigma}{\tau}}\]

A well-typed external expression will elaborate to a well-typed internal expression consistent with the external type. Hence, there is no need for an algorithmic definition of the internal language typing.

\subsubsection{Elaboration}\label{sec:HazelElaboration}
Cast insertion requires an elaboration to the \textit{internal language}, so must output an additional context for holes $\Delta$. The judgements are notated: \[\elaborationSynthesis{e}{\tau}{d}{\Delta}\]
\textit{
External expression $e$ which synthesises type $\tau$ under type context $\Gamma$ is elaborated to internal expression $d$ producing hole context $\Delta$. }
\[\elaborationAnalysis{e}{\tau}{d}{\tau'}{\Delta}\]
\textit{
External expression $e$ which type checks against type $\tau$ under type context $\Gamma$ is elaborated to internal expression $d$ of consistent type $\tau'$ producing hole context $\Delta$.}

The elaboration judgements for holes must add the hole to the output hole context. And they will elaborate to holes with the default empty environment $\sigma = \mathrm{id}(\Gamma)$, i.e. no substitutions.
\[\inference[\tiny ESEHole]{}{\elaborationSynthesis{\hole^u}{\dyn}{\hole^u_{\mathrm{id}(\Gamma)}}{u :: \hole[] [\Gamma]}}\]
\[\inference[\tiny EAEHole]{}{\elaborationAnalysis{\hole^u}{\tau}{\hole^u_{\mathrm{id}(\Gamma)}}{\tau}{u::\tau[\Gamma]}}\]
When elaborating a \textit{type checked} hole, this checked type is used. Typing them instead as {\dyn} would lose type information.\footnote{Potentially leading to incorrect cast insertion.}

\subsubsection{Final Forms}\label{sec:HazelFinalForms}
The primary addition of Hazel is the addition of a new kind of \textit{final forms} and \textit{values}. This is what allows evaluation to proceed around holes and errors. There are three types of final forms:
\begin{itemize}
\item \textit{Values} -- Constants and functions.
\item \textit{Boxed Values} -- Values wrapped in \textit{injection} casts, or \textit{function}\footnote{Between function types} casts.
\item \textit{Indeterminate Final Forms} -- Terms containing holes that cannot be directly evaluated, e.g. holes or function applications where the function is indeterminate, e.g. $\hole^u(1)$.
\end{itemize}
 Importantly, \textit{any} final form can be treated as a value (in a \textit{call-by-value} context). For example, they can be passed inside a (determinate) function: $(\lambda x. x)(\hole^u)$ can evaluate to $\hole^u$.

\subsubsection{Dynamics}\label{sec:HazelDynamics}
A small-step contextual dynamics \cite[ch. 5]{PracticalFoundations} is defined on the internal expressions to define a \textit{call-by-value} evaluation order, values in this sense are \textit{final forms}. 

Like the \textit{refined criteria}  \cite{GradualRefined}, Hazel presents a rather different cast semantics designed around \textit{ground types}, that is, base types (\code{Int}, \code{Bool}) and least precise\footnote{In the sense that $\dyn$ is more general than any concrete type.} compound types associated via a ground matching relation mapping compound types to their corresponding ground type, e.g. $\code{int} \to \code{int} \groundmatch \dyn \to \dyn$. This formalisation more closely represents common dynamically typed language implementations which only use generic type tags like \textit{fun}, corresponding to the ground type $\dyn \to \dyn$. However, the idea of type consistency checking when \textit{injections} meet \textit{projections} remains the same.\footnote{With projections/injections now being to/fro \textit{ground types}.}


The cast calculus is therefore more complex. However, the fundamental logic is the same as the dynamic type system described previously in \cref{sec:DynamicTypeSystem}.


Evaluation proceeds by \textit{capture avoiding variable substitution} $[d'/x]d$ (substitute $d'$ for $x$ in $d$). Substitutions are recorded in each hole's environment $\sigma$ by substituting all occurences of $x$ for $d$ in each $\sigma$. 

\subsubsection{Hole Substitutions}\label{sec:HoleSubstitution}
Holes are indexed by \textit{metavariables} $u$, and can hence also be substituted. Hole substitution is a \textit{meta} action $\contextualsub d'$ meaning substituting each hole named $u$ for expression $d$ in some term $d'$ with the holes environment. Importantly, the substitutions $d$ can contain variables, whose values are found by checking the holes \textit{environment}, effectively making a \textit{delayed substitution}. See the following rule:
 \[\contextualsub \hole^u_\sigma = [\contextualsub \sigma]d\]
When substituting a matching hole $u$, we replace it with $d$ and apply substitutions from the environment $\sigma$ of $u$ to $d$, after first substituting any occurrences of $u$ in the hole's environment $\sigma$. This corresponds to \textit{contextual substitution} in CMTT \cite{CMTT}.
 
 This can be thought of as a \textit{fill-and-resume} functionality, allowing incomplete program parts to be filled during evaluation rather than only before evaluation.

As Hazel is a \textit{pure language}\footnote{Having no side effects.} and as holes act as closures, then performing hole substitution is \textit{commutative} with respect to evaluation. That is, filling incomplete parts of a program \textit{before} evaluation gives the same result as filling \textit{after} evaluation then resuming evaluation.

\subsection{The Hazel Implementation}\label{sec:HazelImplementation}
The Hazel implementation \cite{HazelCode} is written primarily in ReasonML and OCaml with approximately 65,000 lines of code. It implements the Hazel core calculus along with many additional features. 

\subsubsection{Language Features}\label{sec:HazelAdditionalFeatures}
The relevant additional language features not already discussed are:\footnote{Additionally, Hazel supports other features, which do not concern this project.}
\begin{itemize}
\item \textbf{Lists} -- Linked lists, in the style of ML. By use of the dynamic type, Hazel can represent \textit{heterogeneous} lists, which may have elements of differing types.  
\item \textbf{Tuples \& Labelled Tuples}\footnote{Merged towards the end of the project's development.} -- Allowing compound terms to be constructed and typed \cite[ch. 11.7-8]{TAPL}.
\item \textbf{Sum Types} -- Representing a value as one of many labelled variants, each of possibly different types \cite[ch. 11.10]{TAPL}.
\item \textbf{Type Aliases} -- Binding a name to a type, used to improve code readability or simplify complex type definitions redefining types.
\item \textbf{Pattern Matching} -- Checks a value against a pattern and destructures it accordingly, binding it's sub-structures to variable names.
\item \textbf{Explicit Polymorphism} -- System F style parametric polymorphism \cite[ch. 23]{TAPL}. Where explicit type functions bind arbitrary types to names, which may then be used in annotations. Polymorphic functions are then applied to a type, uniformly giving the corresponding monomorphic function. Implicit and ad-hoc polymorphic functions can still be written as dynamic code, without use of type functions.\footnote{In which case, they are untyped.}
\item \textbf{Iso-Recursive Types} -- Types defined in terms of themselves, allowing the representation of data with potentially infinite or \textit{self-referential} shape \cite[ch. 22-23]{TAPL}, for example linked lists or trees.
\end{itemize}

\subsubsection{Evaluator}\label{sec:HazelEvaluator}
The Hazel implementation has a complex evaluator abstraction (module type \code{EV_MODE}) which is used extensively by the search procedure implementation. The evaluator implementations `evaluate' parametric values, not necessarily having to be terms, for example:
\begin{itemize}
\item \textbf{Final Form Checker} -- Returns whether a term is either: a evaluable expression, a value, or an indeterminate term. Using the evaluator abstraction with this means there is no need to maintain a separate syntactic value checker, instead it is derived directly from the evaluation transitions. Yet, it is still \textit{syntactic} as the implementation does not actually \textit{perform} any evaluation steps which the abstraction presents it with.
\item \textbf{Evaluator} -- Maintains a stack machine to actually perform the reduction steps and fully evaluate terms.
\item \textbf{Stepper} -- Returns a list of \textit{possible}\footnote{Of any evaluation order.} evaluation steps. This allows the evaluation order to be user-controlled in a stepper environment.
\end{itemize}
Each evaluation method is transformed into an evaluator by being applied to an OCaml \textit{functor} \cite[ch. 10]{RealWorldOCaml}. The resulting module produces a \code{transition} method that takes terms to evaluation results in an environment.\footnote{Mapping variable bindings.}


\subsection{Bounded Non-Determinism}\label{sec:Nondeterminism}
The search procedure \cite{SearchProc} is effectively a \textit{dynamic type-directed} test generator, attempting to find dynamic type errors. In Hazel, dynamic type errors will manifest themselves as \textit{cast errors}.

Type-directed generation of inputs and searching for one which manifests an error is a \textit{non-deterministic} algorithm \cite{NondeterministicAlgorithms}. 

\subsubsection{High Level}
At a high level, non-determinism can be represented declaratively by two ideas:
\begin{itemize}
\item \textit{Choice} (\texttt{<||>}): Determines the search space, flipping a coin will return heads \textit{or} tails.
\item \textit{Failure} (\texttt{fail}): The \textit{empty} result, no solutions to the algorithm.
\end{itemize}
Suppose the non-deterministic result of the algorithm has type $\tau$. These can be represented by operations:
\[\texttt{<||>} : \tau \to \tau \to \tau\]
\[\texttt{fail} : \tau\]
Where \texttt{<||>} should be \textit{associative} and \texttt{fail} should be a \textit{zero element}, forming a \textit{monoid}:
\[x \texttt{ <||> } (y \texttt{ <||> } z) = (x \texttt{ <||> } y) \texttt{ <||> } z \qquad \texttt{fail} \texttt{ <||> } x = x = x \texttt{ <||> } \texttt{fail}\]
That is, the order of making binary choices does not matter, and there is no reason to choose failure.

There are many proposed ways to represent and manage non-deterministic programs which I considered, of which a monadic representation over a tree based state-space model was chosen as a good balance of flexibility, simplicity, and familiarity to other Hazel developers. \Cref{sec:NonDeterminismAppendix} reasons and details other options considered: continuations, effect handlers, tagless final DSLs, direct implementation.  

\paragraph{Monadic Non-determinism:} Some monads (\cref{sec:Monads} explains monads) can be extended to represent non-determinism by adding the \texttt{choice} and \texttt{fail} operators satisfying the usual laws.
These operations interact by \texttt{bind} distributing over \texttt{choice}, and \texttt{fail} being a left-identity for \texttt{bind}.
\[\texttt{bind}(m_1 \texttt{ <||> } m_2)(f) = \texttt{bind}(m_1)(f) \texttt{ <||> } \texttt{bind}(m_2)(f)\]
\[\texttt{bind}(\texttt{fail})(f) = \texttt{fail}\]
In this context, bind can be thought of as \textit{conjunction}: if we can map each guess to another set of choices, \texttt{bind} will conjoin all the choices from every guess. \Cref{fig:Conjunction} demonstrates how flipping a coin followed by rolling a dice can be conjoined, yielding the choice of all pairs of coin flip and dice roll.

Distributivity represents this interpretation:  guessing over a combined choice is the same as guessing over each individual choice and then combining the results. \texttt{fail} being the left identity of \texttt{bind} states that you cannot make any guesses from the no choice (\texttt{fail}). 
\begin{figure}[h]\centering
\begin{minted}{reason}
let coin = return(Heads) <||> return(Tails);
let dice = return(1) <||> ... <||> return(6);
let m = coin 
    >>= flip => // Flip a coin
        dice
    >>= roll => // Roll a dice
        return((flip, roll)) // Return conjunction
\end{minted}
\caption{Examples: Bind as Conjunction}
\label{fig:Conjunction}
\end{figure}

While the \texttt{bind} and \texttt{return} operators are \textit{not} required to represent non-determinism, they are a \textit{familiar}\footnote{For OCaml developers and functional programmers.} way to represent a large class of effects in general way. 

\subsubsection{Low Level}
Computers are deterministic, therefore, the declarative specification abstracts over a low level implementation which will concretely try each choice, searching for solutions from the many choices. One way to resolve this non-determinism is by modelling choices as trees and searching the trees by a variety of either uninformed, or informed strategies.

\section{Starting Point}\label{sec:StartingPoint}
\subsubsection{Concepts}
The foundations of most concepts in understanding Hazel from Part IB Semantics of Programming (and Part II Types later). The concept of gradual typing briefly appeared in Part IB Concepts of Programming Languages, but was not formalised. Dynamic typing, gradual typing, holes, and contextual modal type theory were not covered in Part IB, so were partially researched leading up to the project, then researched further in greater depth during the early stages. Similarly, Part IB Artificial Integlligence provided some context for search procedures. Primarily, the OCaml search procedure for ill-typed witnesses Seidel et al. \cite{SearchProc} and the Hazel core language \cite{HazelLivePaper} were researched over the preceding summer.

\subsubsection{Tools and Source Code}
My only experience in OCaml was from the Part IA Foundations of Computer Science course. The Hazel source code had not been inspected in any detail until after starting the project.

\section{Requirement Analysis}\label{sec:RequirementAnalysis}

\section{Software Engineering Methodology}\label{sec:EngineeringMethodology}
Do the theory first. 

Git. Merging from dev frequently (many of which were very difficult, as my code touches all of type checking and much of dynamics; and some massive changes, i.e. UI update). 

Talking with devs over slack.

Using existing unit tests \& tests in web documentation to ensure typing is not broken.

\section{Legality}\label{sec:Legality}
MIT licence for Hazel.
