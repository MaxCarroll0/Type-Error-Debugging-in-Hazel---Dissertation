\chapter{Preparation}
\label{chap:Preparation}
In this chapter I present the technical background knowledge for this project: an introduction to the type theory for understanding Hazel's core semantics, an overview of Hazel implementation, and notes on non-determinism. Following this, I present my software engineering methodology.

\section{Background Knowledge}\label{sec:BackgroundKnowledge}
\subsection{Type Systems}\label{sec:TypeSystems}
A \textit{type system} is a lightweight formal mathematical method which categorises values into \textit{types} and expressions into types that evaluate to values of the same type. It is effectively a static \textit{approximation} to the runtime behaviour of a language. The following sections expect basic knowledge formal methods of type systems in terms of judgements (\cref{sec:Judgements} reviews this). Note that I will use \textit{partial functions} to represent typing assumption contexts. 


\subsubsection{Dynamic Type Systems}\label{sec:DynamicTypeSystem}
\textit{Dynamic typing} has purported strengths allowing rapid development and flexibility, evidenced by their popularity \cite{DynamicLangShift, TIOBE}. Of particular relevance to this project, execution traces are known to help provide insight to errors \cite{TraceVisualisation}, yet statically typed languages remove the ability to execute programs with type errors, whereas dynamically typed languages do not.\par 

A \textit{dynamically typed system} can be implemented and represented semantically by use of dynamic \textit{type tags} and a \textit{dynamic type} \cite{DynamicTyping}. Then, runtime values can have their type checked at runtime and \textit{cast} between types. This suggests a way to encode dynamic typing via \textit{first-class}\footnote{Directly represented in the language syntax as expressions.} cast expressions which maintain and enforce runtime type constraints alongside a dynamic type written \dyn.

Cast expressions can be represented in the syntax of expression by $e\scast{\tau_1}{\tau_2}$ for expression $e$ and types $\tau_1, \tau_2$, encoding that $e$ has type $\tau_1$ and is cast to new type $\tau_2$. An intuitive way to think about these is to consider two classes of casts:
\begin{itemize}
\item \textit{Injections} -- Casts \textit{to} the dynamic type $e\scast{\tau}{?}$. These are effectively equivalent to type tags, they say that $e$ has type $\tau$ but that it should be treated dynamically.
\item \textit{Projections} -- Casts \textit{from} the dynamic type $e\scast{?}{\tau}$. These are type requirements, for example the add operator could require inputs to be of type \code{int}, and such a projection would force any dynamic value input to be cast to \code{int}. 
\end{itemize}
Then when \textit{injections} meet \textit{projections}, $v\scastcast{\tau_1}{\dyn}{\tau_2}$, representing an attempt to perform a cast $\scast{\tau_1}{\tau_2}$ on $v$. We check the cast is valid and perform if so:
\[\inference{\tau_1 \text{ is castable to } \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v'} \qquad \inference{\tau_1  \text{ is {\color{red} not }castable to }  \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v\scasterror{\tau_1}{\tau_2}}\]


Compound type casts will be decomposed during evaluation. For example, applying $v$ to a function wrapped in a cast decomposes the cast into casting the applied argument and then the result:
\[(f\scast{\tau_1 \to \tau_2}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\tau_1})\scast{\tau_2}{\tau_2'})\]
Or if $f$ has the dynamic type:
\[(f\scast{\dyn}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\dyn})\scast{\dyn}{\tau_2'})\]

Hence, casts around functions (type information) will be moved to the actual arguments at runtime, meeting with casts casts on the argument, resulting in a cast error or a successful cast.

\subsubsection{Gradual Type Systems}\label{sec:GradualTypeSystem}

A \textit{gradual type system} \cite{GradualRefined, GradualFunctional} combines static and dynamic typing. Terms may be annotated as dynamic, marking regions of code omitted from type-checking but still \textit{interoperable} with static code. For example, the following type checks:
\begin{minted}[escapeinside=||]{reason}
let x : |\dyn| = 10;  // Dynamically typed
x ++ "str"       // Statically typed
\end{minted}
Where \code{++} is string concatenation expecting inputs to be \code{string}. But would then cause a runtime \textit{cast error} when attempting to calculate \code{10 ++ "str"}.

It does this by representing casts as expressed previously. The language is split into two parts:
\begin{itemize}
\item The \textit{external language} -- where static type checking is performed which allows annotating expressions with the dynamic type.
\item The \textit{internal language} -- where evaluation and runtime type checking is performed via cast expressions.\footnote{i.e. the proposed \textit{dynamic type system} above.} The example above would reduce to a \textit{cast error}: \[10\scasterror{\code{Int}}{\code{String}}\code{ ++ "str"}\]
\end{itemize}
For type checking, a \textit{consistency} relation $\tau_1 \sim \tau_2$ is introduced. This is a weakening of the type equality requirements in normal static type checking, allowing \textit{consistent} types to be used additionally. Where every type $\tau$ is consistent with the dynamic type $\dyn$.
\[\inference{}{\tau \sim \dyn} \quad \inference{}{\tau \sim \tau} \quad \inference{\tau_1 \sim \tau_2}{\tau_2 \sim \tau_1} \quad \inference{\tau_1 \sim \tau_1' & \tau_2 \sim \tau_2}{\tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}\]

Then typing rules can be written to use consistency instead of equality. For example, application typing:
\[\inference{\Gamma \vdash e_1 : \tau_1 & \Gamma \vdash e_2 : \tau_2'\\ \tau_1 \funmatch \tau_2 \to \tau & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau_2'}\]
Where $\funmatch$ extracts the argument and return types from a function type, used to account for if $\Gamma \vdash e_1 : \dyn$, where we treat $\dyn$ then as a dynamic function $\dyn \funmatch \dyn \to \dyn$.
Intuitively, $e_1(e_2)$ has type $\tau_2'$ if $e_1$ has type $\tau_1' \to \tau_2'$ or $\dyn$ (then treated as $\dyn \to \dyn$), and $e_2$ has type $\tau_1$ which is consistent with $\tau_1'$ and hence is assumed that it can be passed into the function.

But, for evaluation to work the static type information needs to be encoded into casts to be used in the dynamic internal language, for which the evaluation semantics are defined. This is done via \textit{elaboration}, similarly to Harper and Stone's approach to defining (globally inferred) Standard ML \cite{StandardMLTypeTheory} by elaboration to an explicitly typed internal language XML \cite{CoreXML}. The \textit{elaboration judgement} $\Gamma \vdash e \leadsto d : \tau$ read as: external expression $e$ is elaborated to internal expression $d$ with type $\tau$ under typing context $\Gamma$. For example to insert casts around function applications:
\[\inference{\Gamma \vdash e_1 \leadsto d_1 : \tau_1 & \Gamma \vdash e_2 \leadsto d_2 : \tau_2' \\ \tau_1 \funmatch \tau_2 \to \tau  & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau \leadsto (d_1\scast{\tau_1}{\tau_2 \to \tau})(d_2\scast{\tau_2'}{\tau_2}) : \tau}\]
If $e_1$ elaborates to $d_1$ with type $\tau_1 \sim \tau_2 \to \tau$ and $e_2$ elaborates to $\tau_2'$ with $\tau_2 \sim \tau_2$ then we place a cast\footnote{This cast is required, as if $\tau_1 = \dyn$ then we need a cast to realise that it is even a function. Otherwise $\tau_1 = \tau_2 \to \tau$ and the cast is redundant.} on the function $d_1$ to $\tau_2 \to \tau$ and on the argument $d_2$ to the function's expected argument type $\tau_2$ to perform runtime type checking of arguments.
Intuitively, casts must be inserted whenever type consistency is used, but which casts to insert are non-trivial \cite{Gradualizer}.

The runtime semantics of the internal expression is that of the \textit{dynamic type system} discussed above (\ref{sec:DynamicTypeSystem}). A cast is determined to succeed iff the types are \textit{consistent}.

\subsubsection{Bidirectional Type Systems}\label{sec:BidirectionalTypeSystem}
A \textit{bidirectional type system} \cite{BidirectionalTypes} takes on a more algorithmic definition of typing judgements, being more intuitive to implement. They also allow some amount of local type inference \cite{LocalInference}.

This is done in a similar way to annotating logic programs \cite[123]{LogicProg}, by specifying the \textit{mode} of the type parameter in a typing judgement, distinguishing when it is an \textit{input} (type checking) and when it is an \textit{output} (type synthesis).

We express this with two judgements:
\[\synthesis{e}{\tau}\]
Read as: \textit{$e$ synthesises a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{output}.
\[\analysis{e}{\tau}\]
Read as: \textit{$e$ analyses against a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{input}

When designing such a system care must be taken to ensure \textit{mode correctness} \cite{ModeCorrectness}. Mode correctness ensures that input-output dataflow is consistent such that an input never needs to be \textit{guessed}. For example the following function application rule is \textit{not} mode correct:
\[\inference{\analysis{e_1}{{\color{red}\tau_1} \to \tau_2} & \analysis{e_2}{{\color{red}\tau_1}}}{\analysis{e_1(e_2)}{\tau_2}}\]
We try to \textit{check} $e_2$ with input ${\color{red}\tau_1}$ which is \textit{not known} from either an \textit{output} of any premise nor from the \textit{input} to the conclusion, $\tau_2$. On the other hand, the following \textit{is} mode correct:
\[\inference{\synthesis{e_1}{\tau_1 \to \tau_2} & \analysis{e_2}{\tau_1}}{\analysis{e_1(e_2)}{\tau_2}}\]
Where $\tau_1$ is now known, being \textit{synthesised} from the premise $\synthesis{e_1}{\tau_1 \to \tau_2}$. As before, $\tau_2$ is known as it is an input in the conclusion $\analysis{e_1(e_2)}{\tau_2}$.

Such languages will have three obvious rules. That variables can synthesise their type, being accessible from the typing assumptions. Annotated terms synthesise their type from the annotation (after checking the validity). Subsumption: a synthesising term successfully checks against that same type.

\subsection{The Hazel Calculus}\label{sec:CoreHazel}
\index{Hazel}
Hazel is a language that allows the writing of incomplete programs, evaluating them, and evaluating around static and dynamic errors.

It does this via adding \textit{holes}, which can both be typed and have evaluation proceed around them seamlessly. Errors can be placed in holes allowing continued evaluation.

The core calculus \cite{HazelLivePaper} is a gradually and bidirectionally typed lambda calculus. Therefore it has a locally inferred bidirectional \textit{external language} with the dynamic type $\dyn$ elaborated to an explicitly typed \textit{internal language} including cast expressions. 

The full semantics are documented in the Hazel Formal Semantics appendix \ref{sec:HazelSemantics}, with only rules relevant \textit{holes} discussed in this section. The combination of gradual and bidirectional typing system is itself non-trivial, but only particularly notable consequences are mentioned here. The intuition should be clear from the previous gradual and bidirectional typing sections.\footnote{The difficulties combining gradual and bidirectional typing are largely orthogonal to adding holes.}

\subsubsection{Syntax}\label{sec:HazelSyntax}
\index{\textbf{Core Hazel} syntax}
\par The syntax, in Fig. \ref{fig:syntax}, consists of \textit{types} $\tau$ including the dynamic type $\dyn$, \textit{external expressions} $e$ including (optional) annotations, \textit{internal expressions} $d$ including cast expressions.

Notating $\hole{}^u$ or $\hole[e]^u$ for empty and non-empty holes respectively, where $u$ is the \textit{metavariable} or name for a hole. Internal expression holes, $\hole{}^u_\sigma$ or $\hole[e]^u_\sigma$, also maintain an environment $\sigma$ mapping variables $x$ to internal expressions $d$. These internal holes act as \textit{closures}, recording which variables have been substituted during evaluation.\footnote{This is required, as holes may later be substituted with terms containing variables, receiving their values from the closure environment.}
\begin{figure}[h]
\begin{align*}
\tau &::= b \mid \tau \to \tau \mid\  ?\\
e &::= c \mid x \mid \lambda x : \tau.e \mid \lambda x. e \mid e(e) \mid \hole^u \mid \hole[e]^u \mid e : \tau\\
d &::= c \mid x \mid \lambda x : \tau d \mid d(d) \mid \hole^u_\sigma \mid \hole[d]^u_\sigma \mid d\scast{\tau}{\tau} \mid d\scasterror{\tau}{\tau}
\end{align*}
\caption{Syntax: \textit{types} $\tau$, \textit{external expressions} $e$, \textit{internal expressions} $d$. With $x$ ranging over variables, $u$ over hole names, $\sigma$ over $x \to d$ \textit{internal language} substitutions/environments, $b$ over base types and $c$ over constants.}
\label{fig:syntax}
\end{figure}

\subsubsection{External Language}\label{sec:HazelExternalLang}
\index{External language type system}
Holes synthesise the \textit{dynamic type}, a natural choice made possible by the use of gradual types:
\[\inference[\tiny SNEHole]{\synthesis{e}{\tau}}{\synthesis{\hole[e]^u}{\dyn}} \qquad \inference[\tiny SEHole]{}{\synthesis{\hole^u}{\dyn}}\] 
One notable consequence of combining gradual and bidirectional typing is that the \textit{subsumption rule} in bidirectional typing is naturally extended to allow subsuming any terms of \textit{consistent} types:
\[\inference[\tiny ASubsume]{\synthesis{e}{\tau'}& \tau \sim \tau'}{\analysis{e}{\tau}}\]
Of course $e$ should type check against $\tau$ if it can synthesise a consistent type as the goal of type consistency is that we may type check terms as if they were of the consistent type.

\subsubsection{Internal Language}\label{sec:HazelInternalLang}
The internal language is explicitly typed with typing judgement, $\typeassignment{d}{\tau}$. Where $\Delta$ is a \textit{hole context}, mapping each hole \textit{metavariable} $u$ to it's \textit{checked type} $\tau$\footnote{As originally required when typing the external language expression.} and it's type context $\Gamma$ under which the hole was typed.

\subsubsection{Elaboration}\label{sec:HazelElaboration}
Cast insertion is performed by elaborating to the \textit{internal language}, and must also output an additional context for holes: $\elaborationAnalysis{e}{\tau}{d}{\tau'}{\Delta}$ and $\elaborationSynthesis{e}{\tau}{d}{\Delta}$.

The resulting hole context will record each hole's original \textit{analysing} type along with the typing assumptions for its hole closure. Recording them instead as {\dyn} would lose type information.

A well-typed external expression elaborates to a well-typed internal expression \textit{consistent} with the external type.

\subsubsection{Final Forms}\label{sec:HazelFinalForms}
The primary addition of Hazel is the addition of a new kind of \textit{final forms} and \textit{values}. This is what allows evaluation to proceed around holes and errors. There are three types of final forms:
\begin{itemize}
\item \textit{Values} -- Constants and functions.
\item \textit{Boxed Values} -- Values wrapped in casts which cannot be further reduced.
\item \textit{Indeterminate Final Forms} -- Terms containing holes that cannot be directly evaluated, e.g. holes or function applications where the function is indeterminate, e.g. $\hole^u(1)$.
\end{itemize}
 Importantly, \textit{any} final form can be treated as a value (in a \textit{call-by-value} context). For example, they can be passed inside a (determinate) function: $(\lambda x. x)(\hole^u)$ can evaluate to $\hole^u$.

\subsubsection{Dynamics}\label{sec:HazelDynamics}
A small-step contextual dynamics \cite[ch. 5]{PracticalFoundations} is defined on the internal expressions to define a \textit{call-by-value} evaluation order, values in this sense are \textit{final forms}. 

Like the \textit{refined criteria}  \cite{GradualRefined}, Hazel presents a rather different cast semantics designed around \textit{ground types}, that is, base types (\code{Bool} etc.) and least precise compound types, e.g. $\code{Bool} \to \code{Bool} \groundmatch \dyn \to \dyn$. This formalisation more closely represents common dynamically typed language implementations which only use generic type tags like \textit{fun}, corresponding to the ground type $\dyn \to \dyn$. However, the idea of type consistency checking when \textit{injections} meet \textit{projections} remains the same, with projections/injections now being to/fro \textit{ground types}.

\subsubsection{Hole Substitutions}\label{sec:HoleSubstitution}
Holes are indexed by \textit{metavariables} $u$, and can hence also be substituted. Hole substitution is a \textit{meta} action $\contextualsub d'$ meaning substituting each hole named $u$ for expression $d$ in some term $d'$ with the holes environment. Importantly, the substitutions $d$ can contain variables, whose values are found by checking the holes \textit{environment}, effectively making a \textit{delayed substitution}. See the following rule:
 \[\contextualsub \hole^u_\sigma = [\contextualsub \sigma]d\]
When substituting a matching hole $u$, we replace it with $d$ and apply substitutions from the environment $\sigma$ of $u$ to $d$, after first substituting any occurrences of $u$ in the hole's environment $\sigma$. This corresponds to \textit{contextual substitution} in contextual modal type theory \cite{CMTT}.
 
 This can be thought of as a \textit{fill-and-resume} functionality, allowing incomplete program parts to be filled during evaluation rather than only before evaluation.

As Hazel is a \textit{pure language} and holes record variable substitutions, then performing hole substitution is \textit{commutative} with respect to evaluation. That is, filling incomplete parts of a program \textit{before} evaluation gives the same result as filling \textit{after} evaluation then resuming evaluation.

\subsection{The Hazel Implementation}\label{sec:HazelImplementation}
The Hazel implementation \cite{HazelCode} is written primarily in ReasonML and OCaml with approximately 65,000 lines of code. It is under very active development, with much of the code being undocumented; this dissertation summarises the implementation as of April 2025. It implements the Hazel core calculus along with many additional features below. 

\subsubsection{Language Features}\label{sec:HazelAdditionalFeatures}
The relevant additional language features not already discussed are:\footnote{Additionally, Hazel supports other features, which do not concern this project.}
\begin{itemize}
\item \textbf{Lists} -- Linked lists, in the style of ML. By use of the dynamic type, Hazel can represent \textit{heterogeneous} lists, which may have elements of differing types.  
\item \textbf{Tuples \& Labelled Tuples}\footnote{Merged towards the end of the project's development.} -- Allowing compound terms to be constructed and typed \cite[ch. 11.7-8]{TAPL}.
\item \textbf{Sum Types} -- Representing a value as one of many labelled variants, each of possibly different types \cite[ch. 11.10]{TAPL}.
\item \textbf{Type Aliases} -- Binding a name to a type, used to improve code readability or simplify complex type definitions redefining types.
\item \textbf{Pattern Matching} -- Checks a value against a pattern and destructures it accordingly, binding it's sub-structures to variable names.
\item \textbf{Explicit Polymorphism} -- System F style parametric polymorphism \cite[ch. 23]{TAPL}. Where explicit type functions bind arbitrary types to names, which may then be used in annotations. Polymorphic functions are then applied to a type, uniformly giving the corresponding monomorphic function. Implicit and ad-hoc polymorphic functions can still be written as dynamic code, without use of type functions, but are untyped.
\item \textbf{Iso-Recursive Types} -- Types defined in terms of themselves, allowing the representation of data with potentially infinite or \textit{self-referential} shape \cite[ch. 22-23]{TAPL}, for example linked lists or trees.
\end{itemize}

\subsubsection{Evaluator}\label{sec:HazelEvaluator}
The Hazel implementation has a complex evaluator abstraction (module type \code{EV_MODE}) which is used extensively by the search procedure implementation. The evaluator implementations `evaluate' parametric values, not necessarily having to be terms, for example:
\begin{itemize}
\item \textbf{Final Form Checker} -- Returns whether a term is either: a evaluable expression, a value, or an indeterminate term. Using the evaluator abstraction with this means there is no need to maintain a separate syntactic value checker, instead it is derived directly from the evaluation transitions. Yet, it is still \textit{syntactic} as the implementation does not actually \textit{perform} any evaluation steps which the abstraction presents it with.
\item \textbf{Evaluator} -- Maintains a stack machine to actually perform the reduction steps and fully evaluate terms.
\item \textbf{Stepper} -- Returns a list of \textit{possible} evaluation steps (in many evaluation orderings). This allows the evaluation order to be user-controlled in a stepper environment.
\end{itemize}

\subsection{Bounded Non-Determinism}\label{sec:Nondeterminism}
Input generation for a witness search procedure \cite{SearchProc} a \textit{non-deterministic} algorithm \cite{NondeterministicAlgorithms}. At a high level, non-determinism can be represented declaratively by two ideas:
\begin{itemize}
\item \textit{Choice} (\texttt{<||>}): Determines the search space, flipping a coin will return heads \textit{or} tails.
\item \textit{Failure} (\texttt{fail}): The \textit{empty} result, no solutions to the algorithm.
\end{itemize}
Suppose the non-deterministic result of the algorithm has type $\tau$. These can be represented by operations:
\[\texttt{<||>} : \tau \to \tau \to \tau\]
\[\texttt{fail} : \tau\]
Where \texttt{<||>} should be \textit{associative} and \texttt{fail} should be a \textit{zero element}, forming a \textit{monoid}:
\[x \texttt{ <||> } (y \texttt{ <||> } z) = (x \texttt{ <||> } y) \texttt{ <||> } z \qquad \texttt{fail} \texttt{ <||> } x = x = x \texttt{ <||> } \texttt{fail}\]
That is, the order of making binary choices does not matter, and there is no reason to choose failure.

There are many proposed ways to represent and manage non-deterministic programs which I considered, of which a monadic representation over a tree based state-space model was chosen as a good balance of flexibility, simplicity, and familiarity to other Hazel developers. \Cref{sec:NonDeterminismAppendix} reasons and details other options considered: continuations, effect handlers, tagless final DSLs, direct implementation.  

\paragraph{Monadic Non-determinism} Some monads can be extended to represent non-determinism by adding the \texttt{choice} and \texttt{fail} operators satisfying the usual laws.
These operations interact by \texttt{bind} distributing over \texttt{choice}, and \texttt{fail} being a left-identity for \texttt{bind}.
\[\texttt{bind}(m_1 \texttt{ <||> } m_2)(f) = \texttt{bind}(m_1)(f) \texttt{ <||> } \texttt{bind}(m_2)(f)\]
\[\texttt{bind}(\texttt{fail})(f) = \texttt{fail}\]
In this context, bind can be thought of as \textit{conjunction}: if we can map each guess to another set of choices, \texttt{bind} will conjoin all the choices from every guess. \Cref{fig:Conjunction} demonstrates how flipping a coin followed by rolling a dice can be conjoined, yielding the choice of all pairs of coin flip and dice roll.

Distributivity represents this interpretation:  guessing over a combined choice is the same as guessing over each individual choice and then combining the results. \texttt{fail} being the left identity of \texttt{bind} states that you cannot make any guesses from the no choice (\texttt{fail}). 
\begin{figure}[h]\centering
\begin{minted}{reason}
let coin = return(Heads) <||> return(Tails);
let dice = return(1) <||> ... <||> return(6);
let m = coin 
    >>= flip => // Flip a coin
        dice
    >>= roll => // Roll a dice
        return((flip, roll)) // Return conjunction
\end{minted}
\caption{Examples: Bind (\code{>>=}) as Conjunction}
\label{fig:Conjunction}
\end{figure}

\section{Starting Point}\label{sec:StartingPoint}
\subsubsection{Concepts}
Only the basic foundations of most concepts in understanding Hazel were covered in Part IB Semantics of Programming (and Part II Types later). The concept of gradual typing briefly appeared in Part IB Concepts of Programming Languages, but was not formalised. Monads and non-determinism were also present in this course, but not their intersection.

\subsubsection{Tools and Source Code}
My only experience in OCaml was from the Part IA Foundations of Computer Science course. This project builds directly upon the open-source Hazel language codebase \cite{HazelCode}. The type witness search procedure is inspired by Seidel et al. \cite{SearchProc}, however my implementation differs significantly, being applied to Hazel rather than OCaml. Three (DFS, BFS, BDFS) searching methods for monadic non-determinism are based on Spivey \cite{Bunches} with minor changes, due to OCaml being a strict language.

\section{Requirements Analysis}\label{sec:RequirementAnalysis}
The motivation for this project was a desire to assist programmers by improving type error debugging. Deficiencies in highlighting systems for static and dynamic errors, and potential in combining dynamic traces for static errors, were identified. Hazel, being gradually typed, is then a natural choice to explore both static and dynamic aspects together. At a high level, three aims were then established:
\begin{enumerate}
\item Create a more complete highlighting system for static type errors in Hazel.
\item Create a (complete) source code highlighting system for dynamic errors in Hazel.
\item Provide automated discovery of dynamic witnesses for static errors in Hazel.
\end{enumerate}
At the time of writing the project proposal (\cref{sec:ProjectProposal}), the first aim was not present, but naturally followed from the direction that the theorising of cast slicing took.

The first two aims were addressed by devising a novel theory. Therefore, only after this, a set of core goals and extensions were specified. Aim 3 is based on existing research, so it's goals are as in the project proposal.

Some extensions were added upon critical analysis marked (*) and (**) after the 1st or 2nd evaluations (see software methodology) and are detailed in \cref{sec:CriticalAnalysis}. Most extensions relate to \textit{maintainability} of code, \textit{conciseness} of slices, witness \textit{coverage} improvements, and \textit{usability} (UI improvements), the latter two with lower priority. The focus was to \textit{devise} and \textit{implement} the features, to an extent that proves their potential, but not necessarily to create a complete integrated debugging aid.

\paragraph{{\color{red} Core Goals}} Create/translate a corpus of ill-typed Hazel programs for evaluation usage. The witness search procedure must have reasonable coverage ($>75\%$) in a time suitable for interactive debugging (30s) over the corpus. Implement synthesis and analysis type slice theories (\cref{sec:SynthesisSlices}, \ref{sec:AnalysisSlices}), and cast slice theory (\cref{sec:CastSlicingTheory}) including basic UI highlighting of the source code.

\paragraph{{\color{orange} Extensions}} Implement contribution slice theory (\cref{sec:ContributionSlices}). Allow customisable witness instantiation ordering. \textit{(Maintainability)} Segregate type slicing logic from type checking semantics. Segregate cast slicing logic from transition semantics. \textit{(Conciseness)} Error slices (*). Minimised error slices (**). 

\paragraph{{\color{blue} Low-Priority Extensions}} \textit{(Coverage)} Extended pattern instantiation (**). \textit{(Usability)} Trace visualisation \& compression. Graphs for cast dependence. UI to select sub-parts of slices (*). 

Each goal and extension must be achieved for a sufficient subset of Hazel, with features classified according to the MoSCoW method \cite{Moscow} in \cref{fig:Classes}.

\begin{figure}[h]
\begin{tabular}{lp{12cm}}
Class & Features\\
\hline
{\color{red}Must Implement} & {Base types: their constants \& operations, lists, functions, bindings, type aliases, tuples, sum types \& constructors, holes, casts.}\\
{\color{orange}Should Implement} & Pattern matching\\
{\color{blue}Could Implement} & labelled tuples (*), type functions, recursive types.\\
{\color{Green}Won't Implement} & Tests, deferrals, probes (*), filters, live literals.\\
\end{tabular}
\ \\

(*) New features merged from main branch (in February) not present in Hazel when proposal was written. 
\caption{Hazel Subsets to Implement for}
\label{fig:Classes}
\end{figure}


Extensions with (*) were added post-evaluation and (**) after re-evaluation.
\section{Software Engineering Tools and Techniques}\label{sec:EngineeringMethodology}
\paragraph{Methodology} Due to the high level of uncertainty and risk in devising new theories, a spiral development model was followed. Each iteration refined the implementation through defined milestones (see project proposal) and repeated evaluation (\cref{fig:Phases}). Essential improvements and new extensions were added after each evaluation, being prioritised based on their risk to not be fully implemented by a deadline (coverage improvements and usability extensions assessed as having higher risk).

\begin{figure}
\centering
\tikzstyle{phase} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!20]
\tikzstyle{evaluation} = [phase, fill=orange!30]
\tikzstyle{implementation} = [phase, fill=green!30]
\tikzstyle{arrow} = [thick, ->, >=Stealth]
\begin{tikzpicture}[node distance=1.2cm and 0.8cm]

\node (f1) [phase, draw=red] {\parbox{3cm}{\small\raggedright Formalise slicing theories. Define evaluation criteria.}};
\node (f2) [implementation, below=of f1] {\parbox{3cm}{\small\raggedright Implement core goals}};
\node (f3a) [evaluation, left=of f2] {\parbox{3cm}{\small\raggedright Evaluate success. Core goals not met ($<75\%$ coverage).}};
\node (f3b) [phase, left=of f1] {\parbox{3.1cm}{\small\raggedright Plan new extensions, prioritised on risk. Refine evaluation criteria.}};
\node (f4) [implementation, above=of f3b] {\parbox{3cm}{\small\raggedright Implement improvements}};
\node (f5a) [evaluation, right=of f4] {\parbox{3cm}{\small\raggedright Re-evaluate with refined criteria. Core goals met.}};
\node (f5b) [phase, right=of f5a] {\parbox{3cm}{\small\raggedright Plan improvements and further extensions, priotise on risk.}};
\node (f6) [implementation, right=of f1] {\parbox{3cm}{\small\raggedright Implement highest priority extensions}};
\node (f7) [xshift=-0.12cm,evaluation, right=of f2] {\parbox{3.3cm}{\small\raggedright Evaluate completed extensions}};

\draw [arrow] (f1) -- (f2);
\draw [arrow] (f2) -- (f3a);
\draw [arrow] (f3a) -- (f3b);
\draw [arrow] (f3b) -- (f4);
\draw [arrow] (f4) -- (f5a);
\draw [arrow] (f5a) -- (f5b);
\draw [arrow] (f5b) -- (f6);
\draw [arrow] (f6) -- (f7);

\end{tikzpicture}
\caption{Phases of development}
\label{fig:Phases}
\end{figure}
\paragraph{Hazel Codebase \& Interaction} The Hazel codebase is extensive (65k lines), with much of it being undocumented. As such interaction with the Hazel development team was required to clarify workings. Equally, I found and raised issues on bugs throughout, some being fixed by myself and later merged into the main development branch. I re-use their existing build, formatting, and deployment systems and conform with their coding standards. 
\paragraph{Version Control \& Merges} Git and GitHub were used for version control and backups. My project had various extensions and alternate implementations and improvements, for which multiple branches were created. Hazel is a very active research project, so many bugfixes (and bugs introduced) and new features were added over the course of developing this project. These updates were regularly merged into my project, often requiring extensive conflict resolution (Slicing touches almost the entire codebase). Included in this were three major merges for a UI architecture rewrite, labelled tuples, and probes. Over a week of work was spent entirely on merges, see \cref{sec:Merges} for a list.
\paragraph{Continuous Integration \& Deployment} The main branch of this project is integrated with Hazel's continuous integration and deployment system (using GitHub actions). As such, unit tests and coverage are performed automatically, and the main branch of this project can be accessed at \url{https://hazel.org/build/witnesses-type-slicing/}.\footnote{This is continuously deployed. New functionality implemented \textit{after} the deadline may also be present.}
\paragraph{Testing} Hazel performs it's testing by listing code samples with errors labelled by comments. Or, more recently, shifting towards unit regression tests using the \textit{Alcotest} package \cite{AlcoTest}. I reuse these to ensure type checking and evaluation is not broken by my additions. However, as slicing involves random term IDs, unit testing is more difficult. Therefore, I use the earlier method of testing directly within the editor, querying the slicing UI to test for calculation errors.
\paragraph{Tools} No new dependencies were introduced into Hazel, instead existing dependencies were used, e.g. \code{Js_of_ocaml} \cite{JSOO} for regex matching, Jane Street \code{Base.Sequence} \cite{Base} for streams. However, micro benchmarking of the search procedure was performed using \textit{Bechamel} \cite{Bechamel}.

\paragraph{Licences} Hazel is open-source available under an MIT licence. The source OCaml corpus of ill-typed programs \cite{OCamlCorpus} translated into Hazel is freely available under a Creative Commons Zero (CC0) licence. I again license the project with an MIT licence.
