\chapter{Preparation}
\label{chap:Preparation}
In this chapter I present the technical background knowledge for this project: an introduction to the type theory for understanding Hazel's core semantics, an overview of Hazel implementation, and notes on non-determinism. Following this, I present my software engineering methodology.

\section{Background Knowledge}\label{sec:BackgroundKnowledge}
\subsection{Static Type Systems}\label{sec:TypeSystems}
A \textit{type system} is a lightweight formal mathematical method which categorises values into \textit{types} and expressions into types that evaluate to values of the same type. It is effectively a static \textit{approximation} to the runtime behaviour of a language. The following sections expect basic knowledge formal methods of type systems in terms of judgements (\cref{sec:Judgements} reviews this). Note that I will use \textit{partial functions} to represent typing assumption contexts. 


\subsubsection{Dynamic Type Systems}\label{sec:DynamicTypeSystem}
\textit{Dynamic typing} has purported strengths allowing rapid development and flexibility, evidenced by their popularity \cite{DynamicLangShift, TIOBE}. Of particular relevance to this project, execution traces are known to help provide insight to errors \cite{TraceVisualisation}, yet statically typed languages remove the ability to execute programs with type errors, whereas dynamically typed languages do not.\par 

A \textit{dynamically typed system} can be implemented and represented semantically by use of dynamic \textit{type tags} and a \textit{dynamic type}\footnote{Not necessarily needed for implementation, but is useful when reasoning about dynamic types within a formal type system or when considering types within a \textit{static} context.} \cite{DynamicTyping}. Then, runtime values can have their type checked at runtime and \textit{cast} between types. This suggests a way to encode dynamic typing via \textit{first-class}\footnote{Directly represented in the language syntax as expressions.} cast expressions which maintain and enforce runtime type constraints alongside a dynamic type written \dyn.

Cast expressions can be represented in the syntax of expression by $e\scast{\tau_1}{\tau_2}$ for expression $e$ and types $\tau_1, \tau_2$, encoding that $e$ has type $\tau_1$ and is cast to new type $\tau_2$. An intuitive way to think about these is to consider two classes of casts:
\begin{itemize}
\item \textit{Injections} -- Casts \textit{to} the dynamic type $e\scast{\tau}{?}$. These are effectively equivalent to type tags, they say that $e$ has type $\tau$ but that it should be treated dynamically.
\item \textit{Projections} -- Casts \textit{from} the dynamic type $e\scast{?}{\tau}$. These are type requirements, for example the add operator could require inputs to be of type \code{int}, and such a projection would force any dynamic value input to be cast to \code{int}. 
\end{itemize}
Then when \textit{injections} meet \textit{projections}, $v\scastcast{\tau_1}{\dyn}{\tau_2}$, representing an attempt to perform a cast $\scast{\tau_1}{\tau_2}$ on $v$. We check the cast is valid and perform if so:
\[\inference{\tau_1 \text{ is castable to } \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v'} \qquad \inference{\tau_1  \text{ is {\color{red} not }castable to }  \tau_2}{v\scastcast{\tau_1}{\dyn}{\tau_2} \mapsto v\scasterror{\tau_1}{\tau_2}}\]


Compound type casts can be broken down during evaluation upon usage of such constructs. For example, applying $v$ to a \textit{wrapped}\footnote{Wrapped in a cast between function types.} functions decomposes the cast into casting the applied argument and then the result:
\[(f\scast{\tau_1 \to \tau_2}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\tau_1})\scast{\tau_2}{\tau_2'})\]
Or if $f$ has the dynamic type:
\[(f\scast{\dyn}{\tau_1' \to \tau_2'})(v) \mapsto (f(v \scast{\tau_1'}{\dyn})\scast{\dyn}{\tau_2'})\]

Hence, casts around functions (type information) will be moved to the actual arguments at runtime, meeting with casts casts on the argument, resulting in a cast error or a successful casts.

\subsubsection{Gradual Type Systems}\label{sec:GradualTypeSystem}

A \textit{gradual type system} \cite{GradualRefined, GradualFunctional} combines static and dynamic typing. Terms may be annotated as dynamic, marking regions of code omitted from type-checking but still \textit{interoperable} with static code. For example, the following type checks:
\begin{minted}[escapeinside=||]{reason}
let x : |\dyn| = 10;  // Dynamically typed
x ++ "str"       // Statically typed
\end{minted}
Where \code{++} is string concatenation expecting inputs to be \code{string}. But would then cause a runtime \textit{cast error} when attempting to calculate \code{10 ++ "str"}.

It does this by representing casts as expressed previously. The language is split into two parts:
\begin{itemize}
\item The \textit{external language} -- where static type checking is performed which allows annotating expressions with the dynamic type.
\item The \textit{internal language} -- where evaluation and runtime type checking is performed via cast expressions.\footnote{i.e. the proposed \textit{dynamic type system} above.} The example above would reduce to a \textit{cast error}\footnote{Cast errors now represented with a strike-through and in red. From here-on they are considered as first-class constructs.}: \[10\scasterror{\code{int}}{\code{string}}\code{ ++ "str"}\]
\end{itemize}
For type checking, a \textit{consistency} relation $\tau_1 \sim \tau_2$ is introduced. This is a weakening of the type equality requirements in normal static type checking, allowing \textit{consistent} types to be used additionally. Where every type $\tau$ is consistent with the dynamic type $\dyn$.
\[\inference{}{\tau \sim \dyn} \quad \inference{}{\tau \sim \tau} \quad \inference{\tau_1 \sim \tau_2}{\tau_2 \sim \tau_1} \quad \inference{\tau_1 \sim \tau_1' & \tau_2 \sim \tau_2}{\tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}\]

Then typing rules can be written to use consistency instead of equality. For example, application typing:
\[\inference{\Gamma \vdash e_1 : \tau_1 & \Gamma \vdash e_2 : \tau_2'\\ \tau_1 \funmatch \tau_2 \to \tau & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau_2'}\]
Where $\funmatch$ extracts the argument and return types from a function type, used to account for if $\Gamma \vdash e_1 : \dyn$, where we treat $\dyn$ then as a dynamic function $\dyn \funmatch \dyn \to \dyn$.
Intuitively, $e_1(e_2)$ has type $\tau_2'$ if $e_1$ has type $\tau_1' \to \tau_2'$ or $\dyn$ (then treated as $\dyn \to \dyn$), and $e_2$ has type $\tau_1$ which is consistent with $\tau_1'$ and hence is assumed that it can be passed into the function.

But, for evaluation to work the static type information needs to be encoded into casts to be used in the dynamic internal language, for which the evaluation semantics are defined. This is done via \textit{elaboration}, similarly to Harper and Stone's approach to defining (globally inferred) Standard ML \cite{StandardMLTypeTheory} by elaboration to an explicitly typed internal language XML \cite{CoreXML}. The \textit{elaboration judgement} $\Gamma \vdash e \leadsto d : \tau$ read as: external expression $e$ is elaborated to internal expression $d$ with type $\tau$ under typing context $\Gamma$. For example to insert casts around function applications:
\[\inference{\Gamma \vdash e_1 \leadsto d_1 : \tau_1 & \Gamma \vdash e_2 \leadsto d_2 : \tau_2' \\ \tau_1 \funmatch \tau_2 \to \tau  & \tau_2 \sim \tau_2'}{\Gamma \vdash e_1(e_2) : \tau \leadsto (d_1\scast{\tau_1}{\tau_2 \to \tau})(d_2\scast{\tau_2'}{\tau_2}) : \tau}\]
If $e_1$ elaborates to $d_1$ with type $\tau_1 \sim \tau_2 \to \tau$ and $e_2$ elaborates to $\tau_2'$ with $\tau_2 \sim \tau_2$ then we place a cast\footnote{This cast is required, as if $\tau_1 = \dyn$ then we need a cast to realise that it is even a function. Otherwise $\tau_1 = \tau_2 \to \tau$ and the cast is redundant.} on the function $d_1$ to $\tau_2 \to \tau$ and on the argument $d_2$ to the function's expected argument type $\tau_2$ to perform runtime type checking of arguments.
Intuitively, casts must be inserted whenever type consistency is used, but which casts to insert are non-trivial \cite{Gradualizer}.

The runtime semantics of the internal expression is that of the \textit{dynamic type system} discussed above (\ref{sec:DynamicTypeSystem}). A cast is determined to succeed iff the types are \textit{consistent}.

\subsubsection{Bidirectional Type Systems}\label{sec:BidirectionalTypeSystem}
A \textit{bidirectional type system} \cite{BidirectionalTypes} takes on a more algorithmic definition of typing judgements, being more intuitive to implement. They also allow some amount of local type inference \cite{LocalInference}.

This is done in a similar way to annotating logic programs \cite[123]{LogicProg}, by specifying the \textit{mode} of the type parameter in a typing judgement, distinguishing when it is an \textit{input} (type checking) and when it is an \textit{output} (type synthesis).

We express this with two judgements:
\[\synthesis{e}{\tau}\]
Read as: \textit{$e$ synthesises a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{output}.
\[\analysis{e}{\tau}\]
Read as: \textit{$e$ analyses against a type $\tau$ under typing context $\Gamma$}. Type $\tau$ is an \textit{input}

When designing such a system care must be taken to ensure \textit{mode correctness} \cite{ModeCorrectness}. Mode correctness ensures that input-output dataflow is consistent such that an input never needs to be \textit{guessed}. For example the following function application rule is \textit{not} mode correct:
\[\inference{\analysis{e_1}{{\color{red}\tau_1} \to \tau_2} & \analysis{e_2}{{\color{red}\tau_1}}}{\analysis{e_1(e_2)}{\tau_2}}\]
We try to \textit{check} $e_2$ with input ${\color{red}\tau_1}$\footnote{Highlighted in red as an error.} which is \textit{not known} from either an \textit{output} of any premise nor from the \textit{input} to the conclusion, $\tau_2$. On the other hand, the following \textit{is} mode correct:
\[\inference{\synthesis{e_1}{\tau_1 \to \tau_2} & \analysis{e_2}{\tau_1}}{\analysis{e_1(e_2)}{\tau_2}}\]
Where $\tau_1$ is now known, being \textit{synthesised} from the premise $\synthesis{e_1}{\tau_1 \to \tau_2}$. As before, $\tau_2$ is known as it is an input in the conclusion $\analysis{e_1(e_2)}{\tau_2}$.

Such languages will have three obvious rules. That variables can synthesise their type, being accessible from the typing assumptions. Annotated terms synthesise their type from the annotation (after checking the validity). Subsumption: a synthesising term successfully checks against that same type.

\subsection{The Hazel Calculus}\label{sec:CoreHazel}
\index{Hazel}
Hazel is a language that allows the writing of incomplete programs, evaluating them, and evaluating around static and dynamic errors.\footnote{Among other features, like an structure editor with syntactically meaningless states, and various learning aids.}

It does this via adding \textit{holes}, which can both be typed and have evaluation proceed around them seamlessly. Errors can be placed in holes allowing continued evaluation.

The core calculus \cite{HazelLivePaper} is a gradually and bidirectionally typed lambda calculus. Therefore it has a locally inferred bidirectional \textit{external language} with the dynamic type $\dyn$ elaborated to an explicitly typed \textit{internal language} including cast expressions. 

The full semantics are documented in the Hazel Formal Semantics appendix \ref{sec:HazelSemantics}, with only rules relevant \textit{holes} discussed in this section. The combination of gradual and bidirectional typing system is itself non-trivial, but only particularly notable consequences are mentioned here. The intuition should be clear from the previous gradual and bidirectional typing sections.\footnote{The difficulties combining gradual and bidirectional typing are largely orthogonal to adding holes.}

\subsubsection{Syntax}\label{sec:HazelSyntax}
\index{\textbf{Core Hazel} syntax}
\par The syntax, in Fig. \ref{fig:syntax}, consists of \textit{types} $\tau$ including the dynamic type $\dyn$, \textit{external expressions} $e$ including (optional) annotations, \textit{internal expressions} $d$ including cast expressions.

Notating $\hole{}^u$ or $\hole[e]^u$ for empty and non-empty holes respectively, where $u$ is the \textit{metavariable} or name for a hole. Internal expression holes, $\hole{}^u_\sigma$ or $\hole[e]^u_\sigma$, also maintain an environment $\sigma$ mapping variables $x$ to internal expressions $d$. These internal holes act as \textit{closures}, recording which variables have been substituted during evaluation.\footnote{This is required, as holes may later be substituted, with variables then receiving their values from the closure environment.}
\begin{figure}[h]
\begin{align*}
\tau &::= b \mid \tau \to \tau \mid\  ?\\
e &::= c \mid x \mid \lambda x : \tau.e \mid \lambda x. e \mid e(e) \mid \hole^u \mid \hole[e]^u \mid e : \tau\\
d &::= c \mid x \mid \lambda x : \tau d \mid d(d) \mid \hole^u_\sigma \mid \hole[d]^u_\sigma \mid d\scast{\tau}{\tau} \mid d\scasterror{\tau}{\tau}
\end{align*}
\caption{Syntax: \textit{types} $\tau$, \textit{external expressions} $e$, \textit{internal expressions} $d$. With $x$ ranging over variables, $u$ over hole names, $\sigma$ over $x \to d$ \textit{internal language} substitutions/environments, $b$ over base types and $c$ over constants.}
\label{fig:syntax}
\end{figure}

\subsubsection{External Language}\label{sec:HazelExternalLang}
\index{External language type system}
Holes synthesise the \textit{dynamic type}, a natural choice made possible by the use of gradual types:
\[\inference[\tiny SNEHole]{\synthesis{e}{\tau}}{\synthesis{\hole[e]^u}{\dyn}} \qquad \inference[\tiny SEHole]{}{\synthesis{\hole^u}{\dyn}}\] 
One notable consequence of combining gradual and bidirectional typing is that the \textit{subsumption rule} in bidirectional typing is naturally extended to allow subsuming any terms of \textit{consistent} types:
\[\inference[\tiny ASubsume]{\synthesis{e}{\tau'}& \tau \sim \tau'}{\analysis{e}{\tau}}\]
Of course $e$ should type check against $\tau$ if it can synthesise a consistent type as the goal of type consistency is that we may type check terms as if they were of the consistent type.

\subsubsection{Internal Language}\label{sec:HazelInternalLang}
The internal language requires an extra \textit{hole context} $\Delta$ mapping each hole \textit{metavariable} $u$ to it's \textit{checked type} $\tau$\footnote{As originally required when typing the external language expression.} and it's type context $\Gamma$ under which the hole was typed.

There is also a type assignment judgement, $\typeassignment{d}{\tau}$, for the internal language using hole contexts. Where a well-typed external expression elaborates to a well-typed internal expression consistent with the external type. Hence, there is no need for an algorithmic (bidirectional) definition of the internal language typing.

\subsubsection{Elaboration}\label{sec:HazelElaboration}
Cast insertion is performed by elaborating to the \textit{internal language}, and must also output an additional context for holes: $\elaborationAnalysis{e}{\tau}{d}{\tau'}$ and $\elaborationSynthesis{e}{\tau}{d}$. Where $\tau'$ is the elaborated type of $d$ and must be consistent with $\tau$. 

The resulting hole context will record each hole's original \textit{analysing} type along with the typing assumptions for its hole closure. Recording them instead as {\dyn} would lose type information.

\subsubsection{Final Forms}\label{sec:HazelFinalForms}
The primary addition of Hazel is the addition of a new kind of \textit{final forms} and \textit{values}. This is what allows evaluation to proceed around holes and errors. There are three types of final forms:
\begin{itemize}
\item \textit{Values} -- Constants and functions.
\item \textit{Boxed Values} -- Values wrapped in \textit{injection} casts, or \textit{function}\footnote{Between function types} casts.
\item \textit{Indeterminate Final Forms} -- Terms containing holes that cannot be directly evaluated, e.g. holes or function applications where the function is indeterminate, e.g. $\hole^u(1)$.
\end{itemize}
 Importantly, \textit{any} final form can be treated as a value (in a \textit{call-by-value} context). For example, they can be passed inside a (determinate) function: $(\lambda x. x)(\hole^u)$ can evaluate to $\hole^u$.

\subsubsection{Dynamics}\label{sec:HazelDynamics}
A small-step contextual dynamics \cite[ch. 5]{PracticalFoundations} is defined on the internal expressions to define a \textit{call-by-value} evaluation order, values in this sense are \textit{final forms}. 

Like the \textit{refined criteria}  \cite{GradualRefined}, Hazel presents a rather different cast semantics designed around \textit{ground types}, that is, base types (\code{Int}, \code{Bool}) and least precise\footnote{In the sense that $\dyn$ is more general than any concrete type.} compound types associated via a ground matching relation mapping compound types to their corresponding ground type, e.g. $\code{int} \to \code{int} \groundmatch \dyn \to \dyn$. This formalisation more closely represents common dynamically typed language implementations which only use generic type tags like \textit{fun}, corresponding to the ground type $\dyn \to \dyn$. However, the idea of type consistency checking when \textit{injections} meet \textit{projections} remains the same.\footnote{With projections/injections now being to/fro \textit{ground types}.} 

\subsubsection{Hole Substitutions}\label{sec:HoleSubstitution}
Holes are indexed by \textit{metavariables} $u$, and can hence also be substituted. Hole substitution is a \textit{meta} action $\contextualsub d'$ meaning substituting each hole named $u$ for expression $d$ in some term $d'$ with the holes environment. Importantly, the substitutions $d$ can contain variables, whose values are found by checking the holes \textit{environment}, effectively making a \textit{delayed substitution}. See the following rule:
 \[\contextualsub \hole^u_\sigma = [\contextualsub \sigma]d\]
When substituting a matching hole $u$, we replace it with $d$ and apply substitutions from the environment $\sigma$ of $u$ to $d$, after first substituting any occurrences of $u$ in the hole's environment $\sigma$. This corresponds to \textit{contextual substitution} in contextual modal type theory \cite{CMTT}.
 
 This can be thought of as a \textit{fill-and-resume} functionality, allowing incomplete program parts to be filled during evaluation rather than only before evaluation.

As Hazel is a \textit{pure language}\footnote{Having no side effects.} and as holes act as closures, then performing hole substitution is \textit{commutative} with respect to evaluation. That is, filling incomplete parts of a program \textit{before} evaluation gives the same result as filling \textit{after} evaluation then resuming evaluation.

\subsection{The Hazel Implementation}\label{sec:HazelImplementation}
The Hazel implementation \cite{HazelCode} is written primarily in ReasonML and OCaml with approximately 65,000 lines of code. It is under very active development, with much of the code being undocumented; this dissertation summarises the implementation as of April 2025. It implements the Hazel core calculus along with many additional features below. 

\subsubsection{Language Features}\label{sec:HazelAdditionalFeatures}
The relevant additional language features not already discussed are:\footnote{Additionally, Hazel supports other features, which do not concern this project.}
\begin{itemize}
\item \textbf{Lists} -- Linked lists, in the style of ML. By use of the dynamic type, Hazel can represent \textit{heterogeneous} lists, which may have elements of differing types.  
\item \textbf{Tuples \& Labelled Tuples}\footnote{Merged towards the end of the project's development.} -- Allowing compound terms to be constructed and typed \cite[ch. 11.7-8]{TAPL}.
\item \textbf{Sum Types} -- Representing a value as one of many labelled variants, each of possibly different types \cite[ch. 11.10]{TAPL}.
\item \textbf{Type Aliases} -- Binding a name to a type, used to improve code readability or simplify complex type definitions redefining types.
\item \textbf{Pattern Matching} -- Checks a value against a pattern and destructures it accordingly, binding it's sub-structures to variable names.
\item \textbf{Explicit Polymorphism} -- System F style parametric polymorphism \cite[ch. 23]{TAPL}. Where explicit type functions bind arbitrary types to names, which may then be used in annotations. Polymorphic functions are then applied to a type, uniformly giving the corresponding monomorphic function. Implicit and ad-hoc polymorphic functions can still be written as dynamic code, without use of type functions.\footnote{In which case, they are untyped.}
\item \textbf{Iso-Recursive Types} -- Types defined in terms of themselves, allowing the representation of data with potentially infinite or \textit{self-referential} shape \cite[ch. 22-23]{TAPL}, for example linked lists or trees.
\end{itemize}

\subsubsection{Evaluator}\label{sec:HazelEvaluator}
The Hazel implementation has a complex evaluator abstraction (module type \code{EV_MODE}) which is used extensively by the search procedure implementation. The evaluator implementations `evaluate' parametric values, not necessarily having to be terms, for example:
\begin{itemize}
\item \textbf{Final Form Checker} -- Returns whether a term is either: a evaluable expression, a value, or an indeterminate term. Using the evaluator abstraction with this means there is no need to maintain a separate syntactic value checker, instead it is derived directly from the evaluation transitions. Yet, it is still \textit{syntactic} as the implementation does not actually \textit{perform} any evaluation steps which the abstraction presents it with.
\item \textbf{Evaluator} -- Maintains a stack machine to actually perform the reduction steps and fully evaluate terms.
\item \textbf{Stepper} -- Returns a list of \textit{possible}\footnote{Of any evaluation order.} evaluation steps. This allows the evaluation order to be user-controlled in a stepper environment.
\end{itemize}

\subsection{Bounded Non-Determinism}\label{sec:Nondeterminism}
Input generation for a witness search procedure \cite{SearchProc} a \textit{non-deterministic} algorithm \cite{NondeterministicAlgorithms}. 

At a high level, non-determinism can be represented declaratively by two ideas:
\begin{itemize}
\item \textit{Choice} (\texttt{<||>}): Determines the search space, flipping a coin will return heads \textit{or} tails.
\item \textit{Failure} (\texttt{fail}): The \textit{empty} result, no solutions to the algorithm.
\end{itemize}
Suppose the non-deterministic result of the algorithm has type $\tau$. These can be represented by operations:
\[\texttt{<||>} : \tau \to \tau \to \tau\]
\[\texttt{fail} : \tau\]
Where \texttt{<||>} should be \textit{associative} and \texttt{fail} should be a \textit{zero element}, forming a \textit{monoid}:
\[x \texttt{ <||> } (y \texttt{ <||> } z) = (x \texttt{ <||> } y) \texttt{ <||> } z \qquad \texttt{fail} \texttt{ <||> } x = x = x \texttt{ <||> } \texttt{fail}\]
That is, the order of making binary choices does not matter, and there is no reason to choose failure.

There are many proposed ways to represent and manage non-deterministic programs which I considered, of which a monadic representation over a tree based state-space model was chosen as a good balance of flexibility, simplicity, and familiarity to other Hazel developers. \Cref{sec:NonDeterminismAppendix} reasons and details other options considered: continuations, effect handlers, tagless final DSLs, direct implementation.  

\paragraph{Monadic Non-determinism} Some monads can be extended to represent non-determinism by adding the \texttt{choice} and \texttt{fail} operators satisfying the usual laws.
These operations interact by \texttt{bind} distributing over \texttt{choice}, and \texttt{fail} being a left-identity for \texttt{bind}.
\[\texttt{bind}(m_1 \texttt{ <||> } m_2)(f) = \texttt{bind}(m_1)(f) \texttt{ <||> } \texttt{bind}(m_2)(f)\]
\[\texttt{bind}(\texttt{fail})(f) = \texttt{fail}\]
In this context, bind can be thought of as \textit{conjunction}: if we can map each guess to another set of choices, \texttt{bind} will conjoin all the choices from every guess. \Cref{fig:Conjunction} demonstrates how flipping a coin followed by rolling a dice can be conjoined, yielding the choice of all pairs of coin flip and dice roll.

Distributivity represents this interpretation:  guessing over a combined choice is the same as guessing over each individual choice and then combining the results. \texttt{fail} being the left identity of \texttt{bind} states that you cannot make any guesses from the no choice (\texttt{fail}). 
\begin{figure}[h]\centering
\begin{minted}{reason}
let coin = return(Heads) <||> return(Tails);
let dice = return(1) <||> ... <||> return(6);
let m = coin 
    >>= flip => // Flip a coin
        dice
    >>= roll => // Roll a dice
        return((flip, roll)) // Return conjunction
\end{minted}
\caption{Examples: Bind as Conjunction}
\label{fig:Conjunction}
\end{figure}

While the \texttt{bind} and \texttt{return} operators are \textit{not} required to represent non-determinism, they are a \textit{familiar}\footnote{For OCaml developers and functional programmers.} way to represent a large class of effects in general way. 


\section{Starting Point}\label{sec:StartingPoint}
\subsubsection{Concepts}
Only the basic foundations of most concepts in understanding Hazel were covered in Part IB Semantics of Programming (and Part II Types later). The concept of gradual typing briefly appeared in Part IB Concepts of Programming Languages, but was not formalised. Monads and non-determinism were also present in this course, but not their intersection.

\subsubsection{Tools and Source Code}
My only experience in OCaml was from the Part IA Foundations of Computer Science course. This project builds directly upon the open-source Hazel language codebase \cite{HazelCode}. The type witness search procedure is inspired by Seidel et al. \cite{SearchProc}, however my implementation differs significantly, being applied to Hazel rather than OCaml. Three (DFS, BFS, BDFS) searching methods for monadic non-determinism are based on Spivey \cite{Bunches} with minor changes, due to OCaml being a strict language.

\section{Requirements Analysis}\label{sec:RequirementAnalysis}
At a high level, this project aimed to achieve three goals:
\begin{enumerate}
\item Create a more complete highlighting system for static type errors in Hazel.
\item Create a (complete) source code highlighting system for dynamic errors in Hazel.
\item Provide dynamic witnesses for static errors in Hazel. 
\end{enumerate}
At the time of writing the project proposal (\cref{sec:ProjectProposal}), the second aim was unexplored in existing research, so a concrete requirements were not yet known. Similarly, the first goal had not yet been considered, but naturally follows from the direction that the theorising of cast slicing took.

Developing theories was high risk and took longer than expected. But, significant slack time was allocated to mitigate this, and the original goals were left open to simpler formalisations than the one eventually taken.\footnote{For example, a less complete highlighting based only upon traces.}

After formalisation (phase 1), a set of goals and extensions were devised. This includes those for the witness search procedure taken from the project proposal. The goals should be achieved for a subset of Hazel; Hazel features are classified in \cref{fig:Classes} into: must implement, extensions, and won't implement\footnote{Those where the project is less applicable}. Most extensions relate to \textit{maintainability} of code, \textit{conciseness} of slices, witness \textit{coverage} improvements, and \textit{usability} (UI improvements), the latter two considered with lower priority.

\begin{figure}[h]
\begin{tabular}{lp{12cm}}
Class & Features\\
\hline
{\color{red}Must Implement} & {Base types: their constants \& operations, lists, functions, bindings, type aliases, tuples, sum types \& constructors, holes, casts.}\\
{\color{orange}Extensions} & Pattern matching, labelled tuples (*), type functions, recursive types.\\
{\color{blue}Won't Implement} & Tests, deferrals, probes (*), filters, live literals.\\
\end{tabular}
\ \\

(*) New features merged from main branch not present in Hazel when proposal was written. 
\caption{Hazel Subsets to Implement for}
\label{fig:Classes}
\end{figure}

\paragraph{{\color{red} Core Goals}} Create/translate a corpus of ill-typed Hazel programs for evaluation usage. The witness search procedure must have reasonable coverage ($>75\%$) in a time suitable for interactive debugging (30s) over the corpus. Implement synthesis and analysis type slice theory, and cast slice theory with a basic UI highlighting the source code.

\paragraph{{\color{orange} Extensions}} Implement contribution slice theory. Customisable search procedure instantiation ordering. \textit{(Maintainability)} Segregate type slicing logic from type checking semantics. Segregate cast slicing logic from transition semantics. \textit{(Conciseness)} Error slices (*). Minimised error slices (**). 

\paragraph{{\color{blue} Low-Priority Extensions}} \textit{(Coverage)} Extended pattern instantiation (**). \textit{(Usability)} Trace visualisation \& compression. Graphs for cast dependence. UI to select sub-parts of slices (*). 


Extensions with (*) were added post-evaluation and (**) after re-evaluation.
\section{Software Engineering Tools and Techniques}\label{sec:EngineeringMethodology}
\paragraph{Methodology} A spiral development model was followed, with each iteration refining the implementation through defined milestones (see project proposal) and repeated evaluation (\cref{fig:Phases}). New extensions were added after each evaluation and prioritised based on their risk to not be fully implemented by a deadline (coverage improvements and usability extensions having higher risk).

\begin{figure}
\centering
\tikzstyle{phase} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!20]
\tikzstyle{evaluation} = [phase, fill=orange!30]
\tikzstyle{implementation} = [phase, fill=green!30]
\tikzstyle{arrow} = [thick, ->, >=Stealth]
\begin{tikzpicture}[node distance=1.2cm and 0.8cm]

\node (f1) [phase] {\parbox{3cm}{\small\raggedright Formalise slicing theories. Define evaluation criteria.}};
\node (f2) [implementation, right=of f1] {\parbox{3cm}{\small\raggedright Implement core goals}};
\node (f3a) [evaluation, right=of f2] {\parbox{3cm}{\small\raggedright Evaluate success. Core goals not met ($<75\%$ coverage).}};
\node (f3b) [phase, right=of f3a] {\parbox{3cm}{\small\raggedright Plan new extensions, prioritised on risk. Refine evaluation criteria.}};
\node (f4) [implementation, below=of f3b] {\parbox{3cm}{\small\raggedright Implement improvements}};
\node (f5a) [evaluation, left=of f4] {\parbox{3cm}{\small\raggedright Re-evaluate with refined criteria. Core goals met.}};
\node (f5b) [phase, left=of f5a] {\parbox{3cm}{\small\raggedright Plan improvements and further extensions, priotise on risk.}};
\node (f6) [implementation, left=of f5b] {\parbox{3cm}{\small\raggedright Implement highest priority extensions}};
\node (f7) [evaluation, below=of f6] {\parbox{3.3cm}{\small\raggedright Evaluate completed extensions}};

\draw [arrow] (f1) -- (f2);
\draw [arrow] (f2) -- (f3a);
\draw [arrow] (f3a) -- (f3b);
\draw [arrow] (f3b) -- (f4);
\draw [arrow] (f4) -- (f5a);
\draw [arrow] (f5a) -- (f5b);
\draw [arrow] (f5b) -- (f6);
\draw [arrow] (f6) -- (f7);

\end{tikzpicture}
\caption{Phases of development}
\label{fig:Phases}
\end{figure}
\paragraph{Hazel Codebase \& Interaction} The Hazel codebase is extensive (65k lines), with much of it being undocumented. As such interaction with the Hazel development team was required to clarify workings. Equally, I found and raised issues on bugs throughout, some being fixed by myself and later merged into the main development branch. 
\paragraph{Version Control \& Merges} Git and GitHub were used for version control and backups. My project had various extensions and alternate implementations and improvements, for which multiple branches were created.

Hazel is a very active research project, so many bugfixes (and bugs introduced) and new features were added over the course of developing this project. These updates were regularly merged into my project, often requiring extensive conflict resolution (Slicing touches almost the entire codebase). Included in this were two major merges for labelled tuples and a UI architecture rewrite. See \cref{sec:Merges} for a list of merges.
\paragraph{Continuous Integration \& Deployment} The main branch of this project is integrated with Hazel's continuous integration and deployment system (using GitHub actions). As such, unit tests and coverage are performed automatically, and the main branch of this project can be accessed at \url{https://hazel.org/build/witnesses-type-slicing/}.\footnote{This is continuously deployed. New functionality implemented \textit{after} the deadline may also be present.}
\paragraph{Testing} Hazel performs it's testing by listing code samples with errors labelled by comments. Or, more recently, shifting towards unit regression tests using the \textit{Alcotest} package \cite{AlcoTest}. I reuse these to ensure type checking and evaluation is not broken by my additions. However, as slicing involves random term IDs, unit testing is more difficult. Therefore, I use the earlier method of testing directly within the editor, querying the slicing UI to test for calculation errors.
\paragraph{Tools} No new dependencies were introduced into Hazel, instead existing dependencies were used, e.g. \code{Js_of_ocaml} \cite{JSOO} for regex matching, Jane Street \code{Base.Sequence} \cite{Base} for streams. However, micro benchmarking of the search procedure was performed using \textit{Bechamel} \cite{Bechamel}

\paragraph{Licences} Hazel is open-source available under an MIT licence. The source OCaml corpus of ill-typed programs \cite{OCamlCorpus} translated into Hazel is freely available under a Creative Commons Zero (CC0) licence. We again license the project with an MIT licence and the translated Hazel corpus with a CC0 licence.
